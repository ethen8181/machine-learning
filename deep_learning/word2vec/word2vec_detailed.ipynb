{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "@import url('http://fonts.googleapis.com/css?family=Source+Code+Pro');\n",
       "@import url('http://fonts.googleapis.com/css?family=Vollkorn');\n",
       "@import url('http://fonts.googleapis.com/css?family=Arimo');\n",
       "@import url('http://fonts.googleapis.com/css?family=Fira_sans');\n",
       "    \n",
       "    div.cell {\n",
       "        width: 1000px;\n",
       "        margin-left: 0% !important;\n",
       "        margin-right: auto;\n",
       "    }\n",
       "    div.text_cell code {\n",
       "        background: transparent;\n",
       "        color: #000000;\n",
       "        font-weight: 600;\n",
       "        font-size: 12pt;\n",
       "        font-style: bold;\n",
       "        font-family:  'Source Code Pro', Consolas, monocco, monospace;\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "\t}\n",
       "\t\n",
       "    div.input_area {\n",
       "        background: #F6F6F9;\n",
       "        border: 1px solid #586e75;\n",
       "    }\n",
       "\n",
       "    .text_cell_render h1 {\n",
       "        font-weight: 200;\n",
       "        font-size: 30pt;\n",
       "        line-height: 100%;\n",
       "        color:#c76c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 1em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    } \n",
       "    h2 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "        text-align: left;\n",
       "    }\n",
       "    .text_cell_render h2 {\n",
       "        font-weight: 200;\n",
       "        font-size: 16pt;\n",
       "        font-style: italic;\n",
       "        line-height: 100%;\n",
       "        color:#c76c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 1.5em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    } \n",
       "    h3 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h3 {\n",
       "        font-weight: 200;\n",
       "        font-size: 14pt;\n",
       "        line-height: 100%;\n",
       "        color:#d77c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 2em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    }\n",
       "    h4 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h4 {\n",
       "        font-weight: 100;\n",
       "        font-size: 14pt;\n",
       "        color:#d77c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "        white-space: nowrap;\n",
       "    }\n",
       "    h5 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 200;\n",
       "        font-style: normal;\n",
       "        color: #1d3b84;\n",
       "        font-size: 16pt;\n",
       "        margin-bottom: 0em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "        white-space: nowrap;\n",
       "    }\n",
       "    div.text_cell_render{\n",
       "        font-family: 'Fira sans', verdana,arial,sans-serif;\n",
       "        line-height: 125%;\n",
       "        font-size: 115%;\n",
       "        text-align:justify;\n",
       "        text-justify:inter-word;\n",
       "    }\n",
       "    div.output_wrapper{\n",
       "        margin-top:0.2em;\n",
       "        margin-bottom:0.2em;\n",
       "    }\n",
       "\n",
       "    code{\n",
       "      font-size: 70%;\n",
       "    }\n",
       "    .rendered_html code{\n",
       "    background-color: transparent;\n",
       "    }\n",
       "    ul{\n",
       "        margin: 2em;\n",
       "    }\n",
       "    ul li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.5em; \n",
       "    }\n",
       "    ul li li{\n",
       "        padding-left: 0.2em; \n",
       "        margin-bottom: 0.2em; \n",
       "        margin-top: 0.2em; \n",
       "    }\n",
       "    ol{\n",
       "        margin: 2em;\n",
       "    }\n",
       "    ol li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.5em; \n",
       "    }\n",
       "    ul li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.2em; \n",
       "    }\n",
       "    a:link{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    a:visited{\n",
       "       font-weight: bold;\n",
       "       color: #1d3b84;\n",
       "    }\n",
       "    a:hover{\n",
       "       font-weight: bold;\n",
       "       color: #1d3b84;\n",
       "    }\n",
       "    a:focus{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    a:active{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    .rendered_html :link {\n",
       "       text-decoration: underline; \n",
       "    }\n",
       "    .rendered_html :hover {\n",
       "       text-decoration: none; \n",
       "    }\n",
       "    .rendered_html :visited {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .rendered_html :focus {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .rendered_html :active {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "    } \n",
       "    hr {\n",
       "      color: #f3f3f3;\n",
       "      background-color: #f3f3f3;\n",
       "      height: 1px;\n",
       "    }\n",
       "    blockquote{\n",
       "      display:block;\n",
       "      background: #fcfcfc;\n",
       "      border-left: 5px solid #c76c0c;\n",
       "      font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "      width:680px;\n",
       "      padding: 10px 10px 10px 10px;\n",
       "      text-align:justify;\n",
       "      text-justify:inter-word;\n",
       "      }\n",
       "      blockquote p {\n",
       "        margin-bottom: 0;\n",
       "        line-height: 125%;\n",
       "        font-size: 100%;\n",
       "      }\n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    scale:100,\n",
       "                        availableFonts: [],\n",
       "                        preferredFont:null,\n",
       "                        webFont: \"TeX\",\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code for loading the format for the notebook\n",
    "import os\n",
    "\n",
    "# path : store the current path to convert back to it later\n",
    "path = os.getcwd()\n",
    "os.chdir(os.path.join('..', '..', 'notebook_format'))\n",
    "from formats import load_style\n",
    "load_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethen 2017-08-11 17:56:35 \n",
      "\n",
      "CPython 3.5.2\n",
      "IPython 6.1.0\n",
      "\n",
      "numpy 1.13.1\n",
      "scipy 0.19.1\n",
      "pandas 0.19.2\n",
      "sklearn 0.18.1\n",
      "tensorflow 1.2.1\n",
      "gensim 2.3.0\n",
      "spacy 1.9.0\n"
     ]
    }
   ],
   "source": [
    "os.chdir(path)\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "from joblib import cpu_count\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# 1. magic for inline plot\n",
    "# 2. magic to print version\n",
    "# 3. magic so that the notebook will reload external python modules\n",
    "# 4. magic to enable retina (high resolution) plots\n",
    "# https://gist.github.com/minrk/3301035\n",
    "%matplotlib inline\n",
    "%load_ext watermark\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "%watermark -a 'Ethen' -d -t -v -p numpy,scipy,pandas,sklearn,tensorflow,gensim,spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec (Skipgram)\n",
    "\n",
    "At a high level `Word2Vec` is a unsupervised learning algorithm that uses a shallow neural network (with one hidden layer) to learn the vectorial representations of all the unique words/phrases for a given corpus. The advantage that word2vec offers is it tries to preserve the semantic meaning behind those terms. For example, a document may employ the words \"dog\" and \"canine\" to mean the same thing, but never use them together in a sentence. Ideally, the word2vec algorithm would be able to learn the context and place them together in similar vector semantic space.\n",
    "\n",
    "We'll start off by using the Gensim's implementation of the algorithm to provide a high-level intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw data:\n",
      "\n",
      " From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "example input:\n",
      " ['From:', 'lerxst@wam.umd.edu', \"(where's\", 'my', 'thing)', 'Subject:', 'WHAT', 'car', 'is', 'this!?', 'Nntp-Posting-Host:', 'rac3.wam.umd.edu', 'Organization:', 'University', 'of', 'Maryland,', 'College', 'Park', 'Lines:', '15', 'I', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'I', 'saw', 'the', 'other', 'day.', 'It', 'was', 'a', '2-door', 'sports', 'car,', 'looked', 'to', 'be', 'from', 'the', 'late', '60s/', 'early', '70s.', 'It', 'was', 'called', 'a', 'Bricklin.', 'The', 'doors', 'were', 'really', 'small.', 'In', 'addition,', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body.', 'This', 'is', 'all', 'I', 'know.', 'If', 'anyone', 'can', 'tellme', 'a', 'model', 'name,', 'engine', 'specs,', 'years', 'of', 'production,', 'where', 'this', 'car', 'is', 'made,', 'history,', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car,', 'please', 'e-mail.', 'Thanks,', '-', 'IL', '----', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'Lerxst', '----']\n"
     ]
    }
   ],
   "source": [
    "# the .data attribute will access the raw data, where\n",
    "# each element is a document\n",
    "newsgroups_train = fetch_20newsgroups(subset = 'train')\n",
    "\n",
    "# gensim’s Word2vec expects a sequence of sentences as its input,\n",
    "# where each sentence a list of words. We'll be lazy for now\n",
    "# and not perform any sort of text preprocessing\n",
    "sentences = [doc.strip().split() for doc in newsgroups_train.data]\n",
    "\n",
    "# example output of the data\n",
    "print('raw data:\\n\\n', newsgroups_train.data[0])\n",
    "print('example input:\\n', sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse time: 8.487048149108887\n",
      "word vector dimension:  (44593, 100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>0.875965</td>\n",
       "      <td>-1.072996</td>\n",
       "      <td>0.651279</td>\n",
       "      <td>0.969601</td>\n",
       "      <td>-1.239173</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-0.026741</td>\n",
       "      <td>-0.494909</td>\n",
       "      <td>-1.147683</td>\n",
       "      <td>-0.673804</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058065</td>\n",
       "      <td>-2.117051</td>\n",
       "      <td>-1.844739</td>\n",
       "      <td>-3.408572</td>\n",
       "      <td>-0.452634</td>\n",
       "      <td>1.059920</td>\n",
       "      <td>-0.580822</td>\n",
       "      <td>-0.359145</td>\n",
       "      <td>-1.448598</td>\n",
       "      <td>1.770400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.517399</td>\n",
       "      <td>-1.550736</td>\n",
       "      <td>0.561461</td>\n",
       "      <td>-0.410522</td>\n",
       "      <td>-0.834121</td>\n",
       "      <td>-0.747776</td>\n",
       "      <td>-3.933393</td>\n",
       "      <td>2.577003</td>\n",
       "      <td>-0.962767</td>\n",
       "      <td>-1.607448</td>\n",
       "      <td>...</td>\n",
       "      <td>0.324609</td>\n",
       "      <td>-0.802488</td>\n",
       "      <td>0.560674</td>\n",
       "      <td>-0.493972</td>\n",
       "      <td>-0.855662</td>\n",
       "      <td>-4.484555</td>\n",
       "      <td>0.664931</td>\n",
       "      <td>-0.638924</td>\n",
       "      <td>-0.567126</td>\n",
       "      <td>3.082960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>0.739635</td>\n",
       "      <td>-0.995467</td>\n",
       "      <td>-1.177334</td>\n",
       "      <td>0.167292</td>\n",
       "      <td>-0.138621</td>\n",
       "      <td>-0.094872</td>\n",
       "      <td>-0.553020</td>\n",
       "      <td>-0.909594</td>\n",
       "      <td>-1.468312</td>\n",
       "      <td>-1.804325</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.890942</td>\n",
       "      <td>1.142688</td>\n",
       "      <td>1.205020</td>\n",
       "      <td>-2.180235</td>\n",
       "      <td>-3.122204</td>\n",
       "      <td>-1.757854</td>\n",
       "      <td>-2.372113</td>\n",
       "      <td>-3.707302</td>\n",
       "      <td>-1.036076</td>\n",
       "      <td>-0.625225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>-1.265011</td>\n",
       "      <td>-2.267246</td>\n",
       "      <td>0.982152</td>\n",
       "      <td>-2.038259</td>\n",
       "      <td>-3.091936</td>\n",
       "      <td>0.127317</td>\n",
       "      <td>-2.154119</td>\n",
       "      <td>0.969919</td>\n",
       "      <td>-1.305455</td>\n",
       "      <td>1.667451</td>\n",
       "      <td>...</td>\n",
       "      <td>2.273709</td>\n",
       "      <td>-0.930035</td>\n",
       "      <td>-1.947077</td>\n",
       "      <td>-1.215089</td>\n",
       "      <td>0.988526</td>\n",
       "      <td>-0.069218</td>\n",
       "      <td>-0.330214</td>\n",
       "      <td>-0.889438</td>\n",
       "      <td>-0.013739</td>\n",
       "      <td>0.192290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>-0.607925</td>\n",
       "      <td>-2.396938</td>\n",
       "      <td>-0.081813</td>\n",
       "      <td>-0.213720</td>\n",
       "      <td>-0.008861</td>\n",
       "      <td>-0.413868</td>\n",
       "      <td>-0.706492</td>\n",
       "      <td>-0.059030</td>\n",
       "      <td>0.028941</td>\n",
       "      <td>-2.246091</td>\n",
       "      <td>...</td>\n",
       "      <td>1.299553</td>\n",
       "      <td>0.082098</td>\n",
       "      <td>-0.096058</td>\n",
       "      <td>-1.223995</td>\n",
       "      <td>-0.801336</td>\n",
       "      <td>-0.227632</td>\n",
       "      <td>0.429083</td>\n",
       "      <td>-0.770775</td>\n",
       "      <td>-1.180003</td>\n",
       "      <td>0.248578</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "the  0.875965 -1.072996  0.651279  0.969601 -1.239173 -0.000006 -0.026741   \n",
       "to   0.517399 -1.550736  0.561461 -0.410522 -0.834121 -0.747776 -3.933393   \n",
       "of   0.739635 -0.995467 -1.177334  0.167292 -0.138621 -0.094872 -0.553020   \n",
       "a   -1.265011 -2.267246  0.982152 -2.038259 -3.091936  0.127317 -2.154119   \n",
       "and -0.607925 -2.396938 -0.081813 -0.213720 -0.008861 -0.413868 -0.706492   \n",
       "\n",
       "           7         8         9     ...           90        91        92  \\\n",
       "the -0.494909 -1.147683 -0.673804    ...    -0.058065 -2.117051 -1.844739   \n",
       "to   2.577003 -0.962767 -1.607448    ...     0.324609 -0.802488  0.560674   \n",
       "of  -0.909594 -1.468312 -1.804325    ...    -1.890942  1.142688  1.205020   \n",
       "a    0.969919 -1.305455  1.667451    ...     2.273709 -0.930035 -1.947077   \n",
       "and -0.059030  0.028941 -2.246091    ...     1.299553  0.082098 -0.096058   \n",
       "\n",
       "           93        94        95        96        97        98        99  \n",
       "the -3.408572 -0.452634  1.059920 -0.580822 -0.359145 -1.448598  1.770400  \n",
       "to  -0.493972 -0.855662 -4.484555  0.664931 -0.638924 -0.567126  3.082960  \n",
       "of  -2.180235 -3.122204 -1.757854 -2.372113 -3.707302 -1.036076 -0.625225  \n",
       "a   -1.215089  0.988526 -0.069218 -0.330214 -0.889438 -0.013739  0.192290  \n",
       "and -1.223995 -0.801336 -0.227632  0.429083 -0.770775 -1.180003  0.248578  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apart from the input sentence, the only additional paramter\n",
    "# we'll set is to specify use all possible cpu to train the model\n",
    "workers = cpu_count()\n",
    "\n",
    "start = time()\n",
    "word2vec = Word2Vec(sentences, workers = workers)\n",
    "elapse = time() - start\n",
    "print('elapse time:', elapse)\n",
    "\n",
    "# obtain the learned word vectors (.wv.syn0)\n",
    "# and the vocabulary/word that corresponds to each word vector\n",
    "word_vectors = pd.DataFrame(word2vec.wv.syn0, index = word2vec.wv.index2word)\n",
    "print('word vector dimension: ', word_vectors.shape)\n",
    "word_vectors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model has learned the word vectors, we can use them to look up related words and phrases (words that have similar semantic meaning) for a given term of interest by comparing distances between the vectors using distance metric such as cosine distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('machine', 0.8475774526596069),\n",
       " ('keyboard', 0.8243974447250366),\n",
       " ('application', 0.8197864294052124),\n",
       " ('network', 0.813165009021759),\n",
       " ('modem', 0.8092001676559448)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.most_similar(positive = ['computer'], topn = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from finding similar words using distance metric such as cosine distance, word vectors have the remarkable property that analogies between words seem to be encoded in the difference between word vectors. For example there seems to be a constant male-female difference vector:\n",
    "\n",
    "<img src=\"img/word_vectors.png\" width=\"40%\" height=\"40%\">\n",
    "\n",
    "\\begin{align}\n",
    "W(woman) - W(man)\n",
    "&\\tilde{=} W(aunt) - W(uncle) \\\\\n",
    "&\\tilde{=} W(queen) - W(king)\n",
    "\\end{align}\n",
    "\n",
    "This property means we also perform vector manipulation (e.g. addition and subtraction) with our word vectors. For example, you might have heard or saw the famous example of: King - male + female = queen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Details\n",
    "\n",
    "The way the model works underneath the hood is that trains a neural network by do the following: Given a specific word in the middle of a sentence (the input word), look at the words nearby and pick one at random. \n",
    "\n",
    "There is a **window size** hyperparameter to the algorithm that quantifies the word \"nearby\". A typical window size might be 5, meaning 5 words behind and 5 words ahead (10 in total). There're some implementations that something even fancier: Instead of using a fix $k$ window around each word, the window is uniformly distributed from $1, 2, ..., K$, where $K$ is the max window size we specify.\n",
    "\n",
    "The diagram below shows some of the training samples (word pairs) we would take from the sentence \"The quick brown fox jumps over the lazy dog.\" We'll used a small window size of 2 just for the example. The word highlighted in blue is the input word.\n",
    "\n",
    "<img src=\"img/skipgram.png\" width=\"60%\" height=\"60%\">\n",
    "\n",
    "After feeding a bunch of word pairs to the network, it is going to tell us the probability for every word in our vocabulary being \"nearby\" word that we chose. The output probabilities are going to relate to how likely it is find each vocabulary word nearby our input word. For example, if we gave the trained network the input word \"Soviet\", the output probabilities should be much higher for words like \"Union\" and \"Russia\" than for unrelated words like \"watermelon\" and \"kangaroo\".\n",
    "\n",
    "\n",
    "So how is this all represented? First of all, we know we can't feed a word just as a text string to a neural network (or probably any machine learning model), i.e. we need a way to represent the words to the network. To do this, we first build a vocabulary of words from our training documents. We'll assume that our corpus has a vocabulary size of 10,000.\n",
    "\n",
    "We’re going to represent an input word like \"ants\" as a one-hot vector. This vector will have 10,000 components (one for every unqiue word in our vocabulary) and we'll place a \"1\" in the position corresponding to the word \"ants\", and 0s in all of the other positions. The output of the network is a single vector (also with 10,000 components) containing, for every word in our vocabulary, the probability that a randomly selected nearby word is that vocabulary word. Here’s the architecture of our single-layer neural network.\n",
    "\n",
    "<img src=\"img/word2vec_architecture.png\" width=\"70%\" height=\"70%\">\n",
    "\n",
    "There is no activation function on the hidden layer neurons, but the output neurons use softmax. We’ll come back to this later.\n",
    "\n",
    "When training this network on word pairs, the input is a one-hot vector representing the input word and the training output is also a one-hot vector representing the output word. But when we evaluate the trained network on an input word, the output vector will actually be a probability distribution (i.e., a bunch of floating point values, not a one-hot vector).\n",
    "\n",
    "An alternative diagram that depicts that Skip-gram model architecture well:\n",
    "\n",
    "<img src=\"img/skipgram_architecture.png\" width=\"30%\" height=\"30%\">\n",
    "\n",
    "Where we're using the centre word $w_{(t)}$ to predict the surrounding words and the training objective is to learn word vector representations, a.k.a projections that are good at predicting the nearby words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Hidden Layer\n",
    "\n",
    "Let's say that we wish to learn word vectors with 300 features. The number of features is a hyperparameter that we would have to tune to our application to see which one yields the best result. So the hidden layer is going to be represented by a weight matrix with 10,000 rows (one for every word in our vocabulary) and 300 columns (one for every hidden neuron).\n",
    "\n",
    "Now if we look at what would happen when we multiply the 1 x 10,000 one-hot vector representation of the word with a 10,000 x 300 matrix that represents the hidden layer's weight, it will effectively just select the matrix row corresponding to the \"1\". The following figure is a small example that does a matrix multiplication of a 1 x 5 one hot vector with a 5 x 2 hidden layer's weight to give you a visual. \n",
    "\n",
    "<img src=\"img/hidden_layer.png\" width=\"70%\" height=\"70%\">\n",
    "\n",
    "This means that the hidden layer of this model is really just operating as a lookup table. The output of the hidden layer is just the \"word vector\" for the input word.\n",
    "\n",
    "## The Output Layer\n",
    "\n",
    "The 1 x 300 word vector for \"ants\" then gets fed to the output layer. The output layer is a softmax regression classifier. There's another documentation on Softmax Regression [here](http://nbviewer.jupyter.org/github/ethen8181/machine-learning/blob/master/deep_learning/softmax.ipynb), but the gist of it is that each output neuron, one per word in our vocabulary will produce an output probability between 0 and 1 and the sum of all these output values will add up to 1.\n",
    "\n",
    "Specifically, each output neuron has a weight vector which it multiplies against the word vector from the hidden layer, then it applies the function `exp(x)` to the result. Finally, in order to get the outputs to sum up to 1, we divide this result by the sum of the results from all 10,000 output nodes. Here’s an illustration of calculating the output probability for the word \"car\".\n",
    "\n",
    "<img src=\"img/output_layer.png\" width=\"70%\" height=\"70%\">\n",
    "\n",
    "Note that neural network does not know anything about the offset of the output word relative to the input word. In other words, it does not learn a different set of probabilities for the word before the input versus the word after.\n",
    "\n",
    "Recall that in the beginning of the documentation, we mentioned that the goal for word2vec is to represent each word in the corpus as the vector representation while trying to reserve semantic meaning. This means that if two different words have very similar \"contexts\" (that is, what words are likely to appear around them), then our model needs to output very similar results for these two words. And one way for the network to output similar context predictions for these two words is if the word vectors are similar. So to hit the notion home, if two words have similar contexts, then word2vec is motivated to learn similar word vectors for these two words!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Word2vec\n",
    "\n",
    "You may have noticed that the skip-gram neural network contains a huge number of weights ... For our example with 300 features and a vocab of 10,000 words, that's 3M weights in the hidden layer and output layer each! Training this on a large dataset would be slow and prone to overfitting, so the word2vec authors introduced a number of tweaks to make training feasible.\n",
    "\n",
    "- Modifying the optimization objective with a technique they called \"Negative Sampling\", which causes each training sample to update only a small percentage of the model’s weights\n",
    "- Subsampling frequent words to decrease the number of training examples\n",
    "- Treating common word pairs or phrases as single word in the model\n",
    "\n",
    "It’s worth noting that subsampling frequent words and applying Negative Sampling not only reduced the compute burden of the training process, but also improved the quality of their resulting word vectors as well.\n",
    "\n",
    "\n",
    "## Negative Sampling\n",
    "\n",
    "Training a neural network means taking a training example and adjusting all of the neuron weights slightly so that it predicts that training sample more accurately. In other words, each training sample will tweak all of the weights in the neural network. As we discussed above, the size of our word vocabulary means that our skip-gram neural network has a tremendous number of weights, all of which would be updated slightly by every one of our billions of training samples! Negative sampling addresses this by having each training sample only modify a small percentage of the weights, rather than all of them. Here’s how it works:\n",
    "\n",
    "When training the network on the word pair (\"fox\", \"quick\"), we want the \"correct output\" of the network, that is the output neuron corresponding to \"quick\" to output a 1, and for all of the other thousands of output neurons to output a 0.\n",
    "\n",
    "With negative sampling, we are instead going to randomly select just a small number of \"negative\" words (let’s say 5) to update the weights for. (In this context, a \"negative\" word is one for which we want the network to output a 0 for). We will also still update the weights for our “positive” word (which is the word “quick” in our current example).\n",
    "\n",
    "> The paper says that selecting 5-20 words works well for smaller datasets, and we can get away with only 2-5 words for large datasets.\n",
    "\n",
    "Recall that the output layer of our model has a weight matrix that's 300 x 10,000. So we will just be updating the weights for our positive word (\"quick\"), plus the weights for 5 other words that we want to output 0. That’s a total of 6 output neurons, and 1,800 weight values total. That’s only 0.06% of the 3M weights in the output layer!\n",
    "\n",
    "In the hidden layer, only the weights for the input word are updated (this is true whether you’re using Negative Sampling or not)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Notation\n",
    "\n",
    "This section goes back and re-visit the mathematical notation for Word2vec skipgram's objective function. Hopefully, the math won't look so daunting after having an understanding of the model from a non-mathematical standpoint.\n",
    "\n",
    "In this model we are given a corpus of words $w$ and their contexts $c$ (the word pair of the targeted word that we've sampled). We consider the conditional probabilities $p(c | w)$ and given a corpus $Text$, the goal is to set the parameters $\\theta$ of $p(c | w; \\theta)$ to maximize our objective function $J_\\theta$, i.e. our the corpus probability, with regard to our model parameters $\\theta$:\n",
    "\n",
    "\\begin{align}\n",
    "J_{\\theta} &= \n",
    "arg\\underset{\\theta}{max} \\prod_{w \\in Text} \\bigg[ \\prod_{c \\in C(w)} p(c | w; \\theta) \\bigg]\n",
    "\\end{align}\n",
    "\n",
    "Here $C(w)$ is word $w$'s set of contexts.\n",
    "\n",
    "One approach for parameterizing the $p(c | w; \\theta)$ part of the skip-gram model is the classic softmax objective function:\n",
    "\n",
    "\\begin{align}\n",
    "p(c | w; \\theta) &= \\dfrac{exp(v_c \\cdot v_w)}{ \\sum_{c' \\in C} exp(v_c' \\cdot v_w)}\n",
    "\\end{align}\n",
    "\n",
    "Where:\n",
    "\n",
    "- $v_c$ and $v_w$ are the vector representations for word $c$ and $w$ respectively\n",
    "- $C$ is the set of all available contexts\n",
    "\n",
    "While the objective function above can be computed, it is computationally expensive to do due to the summation $\\sum_{c' \\in C} exp(v_c' \\cdot v_w)$ since there can be thousands if not million of them. And this is where negative sampling comes in. \n",
    "\n",
    "Instead of trying to estimate the probability of the word pair directly, we train a model to differentiate the target word from noise (negative sample). We can thus reduce the problem of predicting the correct word to a binary classification task, where the model tries to distinguish positive/genuine data from noise/negative samples.\n",
    "\n",
    "We know for the word pair we generated from the data we want to maximize its probability, i.e.\n",
    "\n",
    "\\begin{align}\n",
    "arg\\underset{\\theta}{max} \\prod_{(w, c) \\in D} p(D = 1 \\big| c, w; \\theta)\n",
    "\\end{align}\n",
    "\n",
    "Here:\n",
    "\n",
    "- $D$ is the set of all word and context pairs we extract from the text\n",
    "- $p(D = 1 \\big| w, c)$ the probability that the word pair $(w, c)$ came from the corpus data\n",
    "\n",
    "We then generate the set $D'$ of random (w, c) pairs, assuming they are all incorrect. The name \"negative sampling\" stems from the set $D'$ of randomly sampled negative examples. Note that when we pick one random word from the vocabulary, there is some tiny chance that the picked word is actually a valid context. If we consider the large number of vobulary we have, we can argue that the probability is really really tiny, but a lot of the packages still do take care of removing these \"accidents\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a binary classification loss, we can use logistic regression to minimize the negative log-likelihood, leading to the objective function:\n",
    "\n",
    "\\begin{align}\n",
    "J_{\\theta} \n",
    "&= arg\\underset{\\theta}{max} \n",
    "\\prod_{(w, c) \\in D} p(D = 1 \\big| c, w; \\theta) \\prod_{(w, c) \\in D'} p(D = 0 \\big| c, w; \\theta) \\\\\n",
    "&= arg\\underset{\\theta}{max} \n",
    "\\prod_{(w, c) \\in D} p(D = 1 \\big| c, w; \\theta) \\prod_{(w, c) \\in D'} \\big(1 - p(D = 1 \\big| c, w; \\theta)\\big) \\\\\n",
    "&= arg\\underset{\\theta}{max} \n",
    "\\sum_{(w, c) \\in D} log \\big(p(D = 1 \\big| c, w; \\theta)\\big) \\sum_{(w, c) \\in D'} log \\big(1 - p(D = 1 \\big| c, w; \\theta)\\big) \\\\\n",
    "&= arg\\underset{\\theta}{max} \n",
    "\\sum_{(w, c) \\in D} log \\big( \\dfrac{1}{1 + \\text{exp}(-v_c \\cdot v_w)} \\big) \\sum_{(w, c) \\in D'} log \\big (1 - \\dfrac{1}{1 + \\text{exp}(-v_c \\cdot v_w)} \\big) \\\\\n",
    "&= arg\\underset{\\theta}{max}\n",
    "\\sum_{(w, c) \\in D} log \\big( \\dfrac{1}{1 + \\text{exp}(-v_c \\cdot v_w)} \\big) \\sum_{(w, c) \\in D'} log \\big (\\dfrac{1}{1 + \\text{exp}(v_c \\cdot v_w)} \\big) \\\\\n",
    "\\end{align}\n",
    "\n",
    "Note that there're various other approaches to approximate the expensive softmax objective function. Negative sampling is soley discussed here because it is fast and works really really well. If you would like to go deeper with this topic, the following link might be a good place to start. [Blog: On word embeddings - Part 2: Approximating the Softmax](http://ruder.io/word-embeddings-softmax/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Negative Samples\n",
    "\n",
    "One little detail that's missing from the description above is how do we select the negative samples.\n",
    "\n",
    "The negative samples are chosen using the unigram distribution. Essentially, the probability of selecting a word as a negative sample is related to its frequency, with more frequenct words being more likely to be selected as negative samples. Instead of using the raw frequency for $w_i$, $\\text{freq}(w_i)$, in the original word2vec paper, each word is given a weight that's equal to it's frequency (word count) raised to the 3/4 power. The probability for selecting a word is just it's weight divided by the sum of weights for all words.\n",
    "\n",
    "\\begin{align}\n",
    "P(w_i) = \\frac{ {\\text{freq}(w_i)}^{3/4} }{\\sum_{j=0}^{n} \\left({\\text{freq}(w_j)}^{3/4} \\right) }\n",
    "\\end{align}\n",
    "\n",
    "This decision to raise the frequency to the 3/4 power appears to be empirical; as the author claims it outperformed other functions (e.g. just using unigram distribution).\n",
    "\n",
    "Side note: The way this selection is implemented in the original word2vec C code is interesting. They have a large array with 100M elements (which they refer to as the unigram table). They fill this table with the index of each word in the vocabulary multiple times, and the number of times a word’s index appears in the table is given by $P(w_i) \\times \\text{table_size}$. Then, to actually select a negative sample, we just generate a random integer between 0 and 100M, and use the word at that index in the table. Since the higher probability words occur more times in the table, we're more likely to pick those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsampling Frequenct Words\n",
    "\n",
    "Word2vec has two additional parameters for discarding some of the input words: words appearing less\n",
    "than `min-count` times are not considered as either words or contexts, and in addition frequent words are down-sampled as defined by the `sample` parameter.\n",
    "\n",
    "There are two potential issues with frequently appeared words like \"the\":\n",
    "\n",
    "- When looking at word pairs that includes \"the\", e.g. (\"fox\", \"the\"), \"the\" doesn’t tell us much about the meaning of \"fox\", since it appears in the context of pretty much every word.\n",
    "- We will have more than enough samples of (\"the\", \"the other word for the word pair\") than we need to learn a good vector for \"the\".\n",
    "\n",
    "Word2Vec implements a \"subsampling\" scheme to address this. For each word we encounter in our training text, there is a chance that we will discard it from the text. The probability that we cut the word is related to the word's frequency.\n",
    "\n",
    "\\begin{align}\n",
    "\\text{probability of keeping the word } w_i \n",
    "&= (\\sqrt{\\frac{z(w_i)}{0.001}} + 1) \\cdot \\frac{0.001}{z(w_i)}\n",
    "\\end{align}\n",
    "\n",
    "Where:\n",
    "\n",
    "- $z(w_i)$ is the fraction of the total words in the corpus that are that word. For example, if the word \"peanut\" occurs 1,000 times in a 1 billion word corpus, then z(\"peanut\") = 1E-6.\n",
    "- There is also a parameter called `sample` which controls how much subsampling occurs, and the default value is 0.001. Smaller values of `sample` mean words are less likely to be kept\n",
    "\n",
    "Here are some interesting observations of this subsampling function (again this is using the default sample value of 0.001).\n",
    "\n",
    "- $\\text{probability of keeping the word } w_i = 1$ (100% chance of being kept) when $z(w_i) <= 0.0026$. This means that only words which represent more than 0.26% of the total words will be subsampled\n",
    "- $\\text{probability of keeping the word } w_i = 0.5$ (50% chance of being kept) when $z(w_i) <= 0.00746$\n",
    "- $\\text{probability of keeping the word } w_i = 0.033$ (3.3% chance of being kept) when $z(w_i) = 1.0$. That is, if the corpus consisted entirely of word $w_i$, which of course is ridiculous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting Phrases\n",
    "\n",
    "Word pair like \"Boston Globe\" (a newspaper) has a much different meaning than the individual words \"Boston\" and \"Globe\". So it makes sense to treat \"Boston Globe\", wherever it occurs in the text, as a single word with its own word vector representation.\n",
    "\n",
    "The formula our phrase models will use to determine whether two tokens $A$ and $B$ constitute a phrase is:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{count(A B) - count_{min}}{count(A) \\cdot count(B)} \\cdot N > threshold\n",
    "\\end{align}\n",
    "\n",
    "Where:\n",
    "\n",
    "- $count(A)$ is the number of times token $A$ appears in the corpus\n",
    "- $count(B)$ is the number of times token $B$ appears in the corpus\n",
    "- $count(A B)$ is the number of times the tokens $A B$ appear in the corpus in this specific order\n",
    "- $N$ is the total size of the corpus vocabulary\n",
    "- $count_{min}$ is a user-defined parameter to ensure that accepted phrases occur a minimum number of times,\n",
    "- $threshold$ is a user-defined parameter to control how strong of a relationship between two tokens the model requires before accepting them as a phrase\n",
    "    \n",
    "As we can infer the formula is designed to make phrases out of words which occur together often relative to the number of individual occurrences. And a higher threshold value will favors phrases made of infrequent words in order to avoid making phrases out of common words like \"and the\" or \"this is\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy\n",
    "\n",
    "After covering a bit of theory about of the Word2vec skipgram model, we'll take a step back and perform some text preprocessing using [**spaCy**](https://spacy.io).\n",
    "\n",
    "<img src=\"img/spaCy.png\" width=\"80%\" height=\"80%\">\n",
    "\n",
    "[**spaCy**](https://spacy.io) is an industrial-strength natural language processing (_NLP_) library for Python. According to the author, spaCy's goal is to take recent advancements in natural language processing out of research papers and put them in the hands of users to build production software.\n",
    "\n",
    "Not only does it handles many tasks commonly associated with building an end-to-end natural language processing pipeline:\n",
    "\n",
    "- Tokenization\n",
    "- Text normalization, such as lowercasing, stemming/lemmatization\n",
    "- Part-of-speech tagging\n",
    "- Syntactic dependency parsing\n",
    "- Sentence boundary detection\n",
    "- Named entity recognition and annotation\n",
    "\n",
    "But it is also written in optimized Cython, which means it's _fast_. According to a few independent sources, it's the fastest syntactic parser available in any language. Key pieces of the spaCy parsing pipeline are written in pure C, enabling efficient multithreading (i.e., spaCy can release the _GIL_).\n",
    "\n",
    "The first step to use `spaCy` is to constructs a language processing pipeline, here we're:\n",
    "\n",
    "- Loading the pre-trained english model\n",
    "- Grabbing a sample text and hand it over to spaCy and be prepared to wait...\n",
    "\n",
    "```python\n",
    "# download the package first\n",
    "pip install spacy\n",
    "\n",
    "# after that download the trained english model\n",
    "python -m spacy download en\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nice place Better than some reviews give it cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what a surprise What a surprise the Sheraton w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Good location       Boston from 17th Floor of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Find an alternative to the Sheraton We stayed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Barely Tolerable If it were possible to give o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Nice place Better than some reviews give it cr...\n",
       "1  what a surprise What a surprise the Sheraton w...\n",
       "2  Good location       Boston from 17th Floor of ...\n",
       "3  Find an alternative to the Sheraton We stayed ...\n",
       "4  Barely Tolerable If it were possible to give o..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we'll be using reviews of a hotel obtained from tripadvisor’s website\n",
    "# to showcase the general process\n",
    "reviews = pd.read_table('hotelreviews.txt', names = ['text'])\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the model/pipeline, once we have\n",
    "# loaded the object, we can call it as\n",
    "# though it were a function\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Nice place Better than some reviews give it credit for. Overall, the rooms were a bit small but nice. Everything was clean, the view was wonderful and it is very well located (the Prudential Center makes shopping and eating easy and the T is nearby for jaunts out and about the city). Overall, it was a good experience and the staff was quite friendly. "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grab a single document, hand it over to spacy\n",
    "doc = reviews.loc[0, 'text']\n",
    "parsed_doc = nlp(doc)\n",
    "parsed_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...1/20th of a second or so. Although the text looks exactly the same as before, a lot has actually happened under the hood. Let's take a look at what we got during that time. From here, we'll start to look at the functionalities/properties that spaCy provided us out of the box.\n",
    "\n",
    "\n",
    "## Tokenization\n",
    "\n",
    "The first one is sentence detection/segmentation (note that all of these features have already been computed, all we're doing now is accessing it via attribute). Every spaCy document is tokenized into sentences and further into tokens which can be accessed by iterating over the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1:\n",
      "Nice place Better than some reviews give it credit for.\n",
      "\n",
      "Sentence 2:\n",
      "Overall, the rooms were a bit small but nice.\n",
      "\n",
      "Sentence 3:\n",
      "Everything was clean, the view was wonderful and it is very well located (the Prudential Center makes shopping and eating easy and the T is nearby for jaunts out and about the city).\n",
      "\n",
      "Sentence 4:\n",
      "Overall, it was a good experience and the staff was quite friendly.\n",
      "\n",
      "tokens:\n",
      "Nice\n"
     ]
    }
   ],
   "source": [
    "# access the sents attribute, which is a\n",
    "# generator that we can loop through\n",
    "for num, sentence in enumerate(parsed_doc.sents):\n",
    "    print('Sentence {}:'.format(num + 1))\n",
    "    print(sentence)\n",
    "    print()\n",
    "\n",
    "# access the first token\n",
    "print('tokens:')\n",
    "print(parsed_doc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part of Speech Tagging\n",
    "\n",
    "Part-of-speech tags (POST) are the properties of the word that are defined by the usage of the word in a grammatically correct sentence. These tags can be used as the text features in information filtering, statistical models and rule based parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_text</th>\n",
       "      <th>part_of_speech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nice</td>\n",
       "      <td>ADJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>place</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Better</td>\n",
       "      <td>PROPN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>than</td>\n",
       "      <td>ADP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>some</td>\n",
       "      <td>DET</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  token_text part_of_speech\n",
       "0       Nice            ADJ\n",
       "1      place           NOUN\n",
       "2     Better          PROPN\n",
       "3       than            ADP\n",
       "4       some            DET"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .orth_ access the raw string and\n",
    "# .pos_ acess the part of speech tags\n",
    "token_text = [token.orth_ for token in parsed_doc]\n",
    "token_pos = [token.pos_ for token in parsed_doc]\n",
    "\n",
    "post = pd.DataFrame(list(zip(token_text, token_pos)),\n",
    "                    columns = ['token_text', 'part_of_speech'])\n",
    "post.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the table above, we can see that the word \"Nice\" is an adjective and so on.\n",
    "\n",
    "\n",
    "## Named Entity Recognition\n",
    "\n",
    "Spacy also consists of a fast entity recognition model which is capable of identifying entitiy phrases from the document. Entities can be of different types, such as – person, location, organization, dates, numericals etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity 1: Better - FAC\n",
      "Entity 2: the Prudential Center - ORG\n"
     ]
    }
   ],
   "source": [
    "# For a given document, the standard way to access entity is\n",
    "# to use the .ents attribute; for each entity, we can then\n",
    "# access the .label_ attribute to check the entity type for\n",
    "# the entity that got flagged;\n",
    "\n",
    "# please check the documentation to see what the label for\n",
    "# entity means\n",
    "# https://spacy.io/docs/usage/entity-recognition#entity-types\n",
    "for num, entity in enumerate(parsed_doc.ents):\n",
    "    print('Entity {}:'.format(num + 1), entity.orth_, '-', entity.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also perform token-level entity analysis. This is basically the name entity recognition that we've already looked at, but at the token by token level. It also provides a inside outside begin indicator. e.g. here \"the Prudential Center\" represents one single entity, so \"the\" is the beginning of the entity (B); \"Prudential Center\" are both inside that entity (I). And the one that does not belong to an entity gets labeled as outside (O)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_text</th>\n",
       "      <th>entity_type</th>\n",
       "      <th>inside_outside_begin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>the</td>\n",
       "      <td>ORG</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>Prudential</td>\n",
       "      <td>ORG</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Center</td>\n",
       "      <td>ORG</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>makes</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    token_text entity_type inside_outside_begin\n",
       "37         the         ORG                    B\n",
       "38  Prudential         ORG                    I\n",
       "39      Center         ORG                    I\n",
       "40       makes                                O"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_entity_type = [token.ent_type_ for token in parsed_doc]\n",
    "token_entity_iob = [token.ent_iob_ for token in parsed_doc]\n",
    "\n",
    "entity = pd.DataFrame(list(zip(token_text, token_entity_type, token_entity_iob)),\n",
    "                      columns = ['token_text', 'entity_type', 'inside_outside_begin'])\n",
    "entity.iloc[37:41]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Level Attribute\n",
    "\n",
    "What about a variety of other token-level attributes, such as the relative frequency of tokens (how frequently does each token/word appears in the english vocabulary), and whether or not a token matches any of the following categories?\n",
    "\n",
    "- stopword (grammatically functional words that don't contribute too much to the context)\n",
    "- punctuation\n",
    "- whitespace\n",
    "- number\n",
    "- whether the token is included in spaCy's default vocabulary or not?\n",
    "- In terms of the token's relative frequency, spaCy expresses it as the log probability, so a negative number closer to 0 means it appears more often. Or we can say a smaller absolute value means it commonly appears\n",
    "\n",
    "Please refer to the [documentation page](https://spacy.io/docs/api/token) to see all the available attrbutes at the token level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemma</th>\n",
       "      <th>log_probability</th>\n",
       "      <th>stop</th>\n",
       "      <th>punctuation</th>\n",
       "      <th>whitespace</th>\n",
       "      <th>number</th>\n",
       "      <th>out_of_vocab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>staff</td>\n",
       "      <td>staff</td>\n",
       "      <td>-10.720455</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>was</td>\n",
       "      <td>be</td>\n",
       "      <td>-5.404201</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>quite</td>\n",
       "      <td>quite</td>\n",
       "      <td>-8.256200</td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>friendly</td>\n",
       "      <td>friendly</td>\n",
       "      <td>-10.444580</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>-3.072948</td>\n",
       "      <td></td>\n",
       "      <td>Yes</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text     lemma  log_probability stop punctuation whitespace number  \\\n",
       "68     staff     staff       -10.720455                                      \n",
       "69       was        be        -5.404201  Yes                                 \n",
       "70     quite     quite        -8.256200  Yes                                 \n",
       "71  friendly  friendly       -10.444580                                      \n",
       "72         .         .        -3.072948              Yes                     \n",
       "\n",
       "   out_of_vocab  \n",
       "68               \n",
       "69               \n",
       "70               \n",
       "71               \n",
       "72               "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_attrs = [(token.orth_,\n",
    "                token.lemma_,\n",
    "                token.prob,\n",
    "                token.is_stop,\n",
    "                token.is_punct,\n",
    "                token.is_space,\n",
    "                token.like_num,\n",
    "                token.is_oov)\n",
    "                for token in parsed_doc]\n",
    "\n",
    "df = pd.DataFrame(token_attrs,\n",
    "                  columns = ['text',\n",
    "                             'lemma',\n",
    "                             'log_probability',\n",
    "                             'stop',\n",
    "                             'punctuation',\n",
    "                             'whitespace',\n",
    "                             'number',\n",
    "                             'out_of_vocab'])\n",
    "\n",
    "# we convert the boolean columns to only showing Yes for True\n",
    "# and a blank string for False for a cleaner output\n",
    "df.loc[:, 'stop':'out_of_vocab'] = (df.loc[:, 'stop':'out_of_vocab']\n",
    "                                      .applymap(lambda x: 'Yes' if x else ''))\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Parsing\n",
    "\n",
    "Spacy also offers a fast and accurate dependency parser. Let's parse the dependency tree of all the sentences which contains a targeted term that we specified and check what are the adjectives that were commonly used with that term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target word: place\n",
      "depency:\n",
      "Nice\n"
     ]
    }
   ],
   "source": [
    "# toy example of how to get the depency\n",
    "token = parsed_doc[1]\n",
    "print('target word:', token)\n",
    "print('depency:')\n",
    "\n",
    "# to get the dependency for a token\n",
    "# we can access the .children attribute\n",
    "# and iterate through them\n",
    "for child in token.children:\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def valid_word(token):\n",
    "    \"\"\"\n",
    "    Returns False if the spacy token is either\n",
    "    a punctuation, whitespace, number or a pronoun\n",
    "    (indicated by the '-PRON-' flag)\n",
    "    \"\"\"\n",
    "    pron_flag = token.lemma_ != '-PRON-'\n",
    "    word_flag = not (token.is_punct or token.is_space or token.like_num)\n",
    "    valid = word_flag and pron_flag\n",
    "    return valid\n",
    "\n",
    "\n",
    "def post_words(document, target_token, post, topn = 5):\n",
    "    \"\"\"\n",
    "    given a document/corpus, look for the most commonly\n",
    "    associated part of speech tag associated with the specified token\n",
    "    \"\"\"\n",
    "    target_sents = [sent for sent in document.sents if target_token in sent.lower_]    \n",
    "    words = []\n",
    "    for sentence in target_sents:\n",
    "        for token in sentence: \n",
    "            words.extend([child.lemma_\n",
    "                          for child in token.children\n",
    "                          if child.pos_ == post and valid_word(child)])\n",
    "\n",
    "    common_words = Counter(words).most_common(topn)\n",
    "    return common_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapse: 3.2930688858032227\n"
     ]
    }
   ],
   "source": [
    "# lump all the documents into one giant document\n",
    "start = time()\n",
    "corpus = ' '.join(reviews['text'])\n",
    "document = nlp(corpus)\n",
    "elapse = time() - start\n",
    "print('elapse:', elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('great', 41), ('good', 28), ('small', 13), ('which', 13), ('fantastic', 11)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_words = post_words(document, target_token = 'view', post = 'ADJ')\n",
    "common_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the text you'd like to process is general-purpose English language text (i.e., not domain-specific, like medical literature), spaCy is ready to use out-of-the-box. It will probably become a core part of the Python data science ecosystem — it will do for natural language computing what other great libraries have done for numerical computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "The following code chunks 1) preprocess the raw text using spaCy. 2) trains a Phrase model to glue words that commonly appear next to each other into bigrams. 3) trains the Word2vec model.\n",
    "\n",
    "There also a pure python script [here](https://github.com/ethen8181/machine-learning/tree/master/deep_learning/word2vec/word2vec_workflow.py) if you're interested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "from joblib import cpu_count\n",
    "from string import punctuation\n",
    "from gensim.models import Phrases\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def export_unigrams(unigram_path, texts, parser, stopwords,\n",
    "                    batch_size = 10000, n_jobs = -1):\n",
    "    \"\"\"\n",
    "    Preprocessed the raw text and export it to a .txt file,\n",
    "    where each line is one document, for what sort of preprocessing\n",
    "    is done, please refer to the `clean_corpus` function\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    unigram_path : str\n",
    "        output file path of the preprocessed unigram text\n",
    "\n",
    "    texts : iterable\n",
    "        iterable can be simply a list, but for larger corpora,\n",
    "        consider an iterable that streams the sentences directly from\n",
    "        disk/network using Gensim's Linsentence or something along\n",
    "        those line\n",
    "\n",
    "    parser : spacy model object\n",
    "        e.g. parser = spacy.load('en')\n",
    "\n",
    "    stopwords : set\n",
    "        stopword set that will be excluded from the corpus\n",
    "\n",
    "    batch_size : int, default 10000\n",
    "        batch size for the spacy preprocessing\n",
    "\n",
    "    n_jobs : int, default -1\n",
    "        number of jobs/cores/threads to use for the spacy preprocessing\n",
    "    \"\"\"\n",
    "    with open(unigram_path, 'w', encoding = 'utf_8') as f:\n",
    "        for cleaned_text in clean_corpus(texts, parser, stopwords, batch_size, n_jobs):\n",
    "            f.write(cleaned_text + '\\n')\n",
    "\n",
    "\n",
    "def clean_corpus(texts, parser, stopwords, batch_size, n_jobs):\n",
    "    \"\"\"\n",
    "    Generator function using spaCy to parse reviews:\n",
    "    - lemmatize the text\n",
    "    - remove punctuation, whitespace and number\n",
    "    - remove pronoun, e.g. 'it'\n",
    "    \"\"\"\n",
    "    n_threads = cpu_count()\n",
    "    if n_jobs > 0 and n_jobs < n_threads:\n",
    "        n_threads = n_jobs\n",
    "\n",
    "    # use the .pip to process texts as a stream;\n",
    "    # this functionality supports using multi-threads\n",
    "    for parsed_text in parser.pipe(texts, n_threads = n_threads, batch_size = batch_size):\n",
    "        tokens = []\n",
    "        for token in parsed_text:\n",
    "            if valid_word(token) and token.lemma_ not in stopwords:\n",
    "                tokens.append(token.lemma_)\n",
    "\n",
    "        cleaned_text = ' '.join(tokens)\n",
    "        yield cleaned_text\n",
    "\n",
    "\n",
    "def valid_word(token):\n",
    "    \"\"\"\n",
    "    Returns False if the spacy token is either\n",
    "    a punctuation, whitespace, number or a pronoun\n",
    "    (indicated by the '-PRON-' flag)\n",
    "    \"\"\"\n",
    "    pron_flag = token.lemma_ != '-PRON-'\n",
    "    word_flag = not (token.is_punct or token.is_space or token.like_num)\n",
    "    word_len_flag = len(token) >= 2\n",
    "    valid = pron_flag and word_flag and word_len_flag\n",
    "    return valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text preprocessing, elapse 226.76951503753662\n"
     ]
    }
   ],
   "source": [
    "# a set of stopwords built-in to spacy,\n",
    "# we can always expand this set for the\n",
    "# problem that we are working on, here we include\n",
    "# python built-in string punctuation mark\n",
    "nlp = spacy.load('en')\n",
    "STOPWORDS = spacy.en.STOP_WORDS | set(punctuation) | set(ENGLISH_STOP_WORDS)\n",
    "\n",
    "# create a directory called 'model' to\n",
    "# store all outputs in later section\n",
    "MODEL_DIR = 'model'\n",
    "if not os.path.isdir(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)\n",
    "\n",
    "UNIGRAM_PATH = os.path.join(MODEL_DIR, 'unigram.txt')\n",
    "if not os.path.exists(UNIGRAM_PATH):\n",
    "    start = time()\n",
    "    export_unigrams(UNIGRAM_PATH, texts = newsgroups_train.data,\n",
    "                    parser = nlp, stopwords = STOPWORDS)\n",
    "    elapse = time() - start\n",
    "    print('text preprocessing, elapse', elapse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training phrase model, elapse 2.6187009811401367\n"
     ]
    }
   ],
   "source": [
    "PHRASE_MODEL_CHECKPOINT = os.path.join(MODEL_DIR, 'phrase_model')\n",
    "if os.path.exists(PHRASE_MODEL_CHECKPOINT):\n",
    "    phrase_model = Phrases.load(PHRASE_MODEL_CHECKPOINT)\n",
    "else:\n",
    "    # use LineSetence to stream text as oppose to\n",
    "    # loading it all into memory\n",
    "    unigram_sentences = LineSentence(UNIGRAM_PATH)\n",
    "    start = time()\n",
    "    phrase_model = Phrases(unigram_sentences)\n",
    "    elapse = time() - start\n",
    "    print('training phrase model, elapse', elapse)\n",
    "    phrase_model.save(PHRASE_MODEL_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def export_bigrams(unigram_path, bigram_path, phrase_model):\n",
    "    \"\"\"\n",
    "    Use the learned phrase model to create (potential) bigrams,\n",
    "    and output the text that contains bigrams to disk\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    unigram_path : str\n",
    "        input file path of the preprocessed unigram text\n",
    "\n",
    "    bigram_path : str\n",
    "        output file path of the transformed bigram text\n",
    "\n",
    "    phrase_model : gensim's Phrase model object\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    Gensim Phrase Detection\n",
    "    - https://radimrehurek.com/gensim/models/phrases.html\n",
    "    \"\"\"\n",
    "\n",
    "    # after training the Phrase model, create a performant\n",
    "    # Phraser object to transform any sentence (list of\n",
    "    # token strings) and glue unigrams together into bigrams\n",
    "    phraser = Phraser(phrase_model)\n",
    "    with open(bigram_path, 'w') as fout, open(unigram_path) as fin:\n",
    "        for text in fin:\n",
    "            unigram = text.split()\n",
    "            bigram = phraser[unigram]\n",
    "            bigram_sentence = ' '.join(bigram)\n",
    "            fout.write(bigram_sentence + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting words to phrases, elapse 10.983206987380981\n"
     ]
    }
   ],
   "source": [
    "BIGRAM_PATH = os.path.join(MODEL_DIR, 'bigram.txt')\n",
    "if not os.path.exists(BIGRAM_PATH):\n",
    "    start = time()\n",
    "    export_bigrams(UNIGRAM_PATH, BIGRAM_PATH, phrase_model)\n",
    "    elapse = time() - start\n",
    "    print('converting words to phrases, elapse', elapse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two code chunks is a slight digression from the overall process, this is simply used assess the amount of memory that is potentially saved by doing these preprocessing, i.e. loading all the raw text\n",
    "and preprocessed text into memory and comparing the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "def get_size(obj, seen = None):\n",
    "    \"\"\"\n",
    "    Recursively finds size of objects, shamelessly \"borrowed\"\n",
    "    from link listed in reference\n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    https://goshippo.com/blog/measure-real-size-any-python-object/\n",
    "    \"\"\"\n",
    "    size = sys.getsizeof(obj)\n",
    "    if seen is None:\n",
    "        seen = set()\n",
    "\n",
    "    obj_id = id(obj)\n",
    "    if obj_id in seen:\n",
    "        return 0\n",
    "\n",
    "    # Important mark as seen *before* entering recursion to gracefully handle\n",
    "    # self-referential objects\n",
    "    seen.add(obj_id)\n",
    "    if isinstance(obj, dict):\n",
    "        size += sum([get_size(v, seen) for v in obj.values()])\n",
    "        size += sum([get_size(k, seen) for k in obj.keys()])\n",
    "    elif hasattr(obj, '__dict__'):\n",
    "        size += get_size(obj.__dict__, seen)\n",
    "    elif hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes, bytearray)):\n",
    "        size += sum([get_size(i, seen) for i in obj])\n",
    "\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original size uses 1.6859290484964748 times the amount of memory\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "with open(BIGRAM_PATH) as f:\n",
    "    for line in f:\n",
    "        X.append(line)\n",
    "\n",
    "original_size = get_size(newsgroups_train.data)\n",
    "preprocessed_size = get_size(X)\n",
    "ratio = original_size / preprocessed_size\n",
    "print('original size uses {} times the amount of memory'.format(ratio))\n",
    "del X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Word2vec also accepts several key parameters that affect both training speed and quality. Hopefully these will now make more sense after we covered some theory of the algorithm.\n",
    "\n",
    "- `iter`: Number of iterations to train the model\n",
    "- `min_count`: For pruning the internal dictionary. Words that appear only once or twice in a billion-word corpus are probably uninteresting typos and garbage. In addition, there’s not enough data to make any meaningful training on those words, so it’s best to ignore them. A reasonable value for min_count is between 0-100, depending on the size of the dataset.\n",
    "- `size`: Refers to the hidden layers size. Bigger size values require more training data, but can lead to better (more accurate) models. Reasonable values are in the tens to hundreds\n",
    "- `workers`: Number of cores/threads used for training\n",
    "- `window`: Only terms that occur within a window-neighbourhood of a term in a sentence are associated with it during training. The default value is 4. Unless your text contains big sentences, it might be safe to leave it at that.\n",
    "- `sg`: This defines the algorithm. If equal to 1, the skip-gram technique is used. Else, the CBoW method is employed. The word2vec framework we introduced here is skip-gram, where we're using the target word to predict the context word, where CBow is simply flipping it the other way around, i.e. using the context word to predict the target word. Based on the original author's [paper](https://arxiv.org/abs/1310.4546), Skip-gram is reported to be a better framework \n",
    "\n",
    "You can refer to the full list of parameters [here](http://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training word2vec, elapse 5.97011399269104\n"
     ]
    }
   ],
   "source": [
    "WORD2VEC_CHECKPOINT = os.path.join(MODEL_DIR, 'word2vec')\n",
    "if os.path.exists(WORD2VEC_CHECKPOINT):\n",
    "    word2vec = Word2Vec.load(WORD2VEC_CHECKPOINT)\n",
    "else:\n",
    "    sentences = LineSentence(BIGRAM_PATH)\n",
    "    start = time()\n",
    "    word2vec = Word2Vec(sentences, workers = cpu_count())\n",
    "    elapse = time() - start\n",
    "    print('training word2vec, elapse', elapse)\n",
    "    word2vec.save(WORD2VEC_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that we can print out similar words to a targeted word that we've specified and use our human judgement of whether the embedding that was learned makes sense. The next code chunk is simply satisfying a personal interest in how the `most_similar` function is implemented in gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# once we’re finished training the model (i.e. no more updates, only querying)\n",
    "# we can store the word vectors and delete the model to trim unneeded model memory\n",
    "vocab = word2vec.wv.vocab\n",
    "word_vectors = word2vec.wv.syn0\n",
    "index2word = word2vec.wv.index2word\n",
    "del word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "def most_similar(word_vectors, vocab, index2word,\n",
    "                 positive = None, negative = None, topn = 5):\n",
    "    \"\"\"\n",
    "    No frill version of the function `word2vec.wv.most_similar`;\n",
    "    Find the top-N most similar words. Positive words contribute\n",
    "    positively towards the similarity, negative words negatively\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    word_vectors : 2d ndarray\n",
    "        the learned word vectors\n",
    "        \n",
    "    vocab : word2vec.wv.vocab\n",
    "        dictionary like object where the key is the word\n",
    "        and the value has a .index attribute that allows\n",
    "        us to look up the index for a given word\n",
    "        \n",
    "    index2word : word2vec.wv.index2word\n",
    "        list like object that serves as the looking up the word\n",
    "        of a given index\n",
    "        \n",
    "    positive/negative : list, default None\n",
    "        list of positive or negative words to look for similar words,\n",
    "        positive words will get assign a +1 weight and negative words\n",
    "        will get assign a -1 weight when doing the vector addition\n",
    "    \n",
    "    topn : int\n",
    "        top-n similar words to find\n",
    "    \"\"\"\n",
    "    # normalize word vectors up front makes the cosine distance\n",
    "    # calculation later easier\n",
    "    normed = normalize(word_vectors)\n",
    "    \n",
    "    # assign weight to positive and negative words\n",
    "    if positive is None:\n",
    "        positive = []\n",
    "    else:\n",
    "        positive = [(word, 1.0) if isinstance(word, str) else word\n",
    "                    for word in positive]\n",
    "    if negative is None:\n",
    "        negative = []\n",
    "    else:\n",
    "        negative = [(word, -1.0) if isinstance(word, str) else word\n",
    "                    for word in negative]\n",
    "\n",
    "    # compute the weighted average of all words\n",
    "    all_words, mean = set(), []\n",
    "    for word, weight in positive + negative:\n",
    "        # gensim's Word2vec word2vec.wv.vocab\n",
    "        # stores the index of a given word\n",
    "        word_idx = vocab[word].index\n",
    "        word_vector = normed[word_idx]\n",
    "        mean.append(weight * word_vector)\n",
    "        all_words.add(word_idx) \n",
    "    \n",
    "    # find topn most similar words measured by cosine distance\n",
    "    mean_vector = np.mean(mean, axis = 0)\n",
    "    mean_vector /= np.sqrt(np.sum(mean_vector ** 2))\n",
    "    dists = np.dot(normed, mean_vector)\n",
    "    best = np.argsort(dists)[::-1][:(topn + len(all_words))]\n",
    "    result = [(index2word[sim], float(dists[sim]))\n",
    "              for sim in best if sim not in all_words]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('modem', 0.9702850580215454),\n",
       " ('board', 0.9628041386604309),\n",
       " ('upgrade', 0.9603150486946106),\n",
       " ('printer', 0.9596825838088989),\n",
       " ('cd_rom', 0.9565122127532959)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = 'computer'\n",
    "most_similar(word_vectors, vocab, index2word, positive = [word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Thoughts\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "The most crucial decisions that affect the performance are the choice of the model\n",
    "architecture, the size of the vectors, the subsampling rate, and the size of the training window. Because Word2vec training is an unsupervised task, there’s no good way to objectively evaluate the result, it all depends on the end application.\n",
    "\n",
    "## Applications\n",
    "\n",
    "Word2Vec and the concept of word embeddings originated in the domain of NLP, however the idea of words in the context of a sentence or a surrounding word window can be generalized to other problem domain dealing with sequences or sets of related data points. For example:\n",
    "\n",
    "- A direct application of Word2Vec to a classical engineering task was recently presented by Spotify. They abstracted the ideas behind Word2Vec to apply them not simply to words in sentences but to any object in any sequence, in this case to songs in a playlist. Songs are treated as words and other songs in a playlist as their surrounding context, depending on whether the playlists in question were genre specific the vocabulary encompassed a number of songs from that collection. Now in order to recommend songs to a user one merely has to examine a neighborhood of the 'song embeddings' of songs the user already likes\n",
    "- Similarly, we could recommend a user who to connect to in a social network setting by examining the graph of relationships, where the nodes represent words and a path through the graph represents a sentence, then nodes that occur in the context of similar other nodes, would be close together in the vector space.\n",
    "\n",
    "These examples show that the general applicability of Word2Vec based algorithms is very rich and that it behooves practitioners to examine their problem domain with respect to sequences of objects that occur in some meaningful context.\n",
    "\n",
    "\n",
    "## Resources\n",
    "\n",
    "A lot of the non-mathematical notes were taken from Chris McCormick's blog post. If you wish to learn more about the topic, the following link also contains different resources that Chris has curated (including a commented version of word2vec's original C code, derivation of word2vec's gradient update, etc.). [Blog: Word2Vec Resources](http://mccormickml.com/2016/04/27/word2vec-resources/)\n",
    "\n",
    "If you would like to practice using a deep learning framework such as Tensorflow to implement the word2vec model. The following resources are good places to start, the reason that its not included in this documentation is because it was an order of magnitude slower than Gensim's Word2vec and the result weren't as good as well.\n",
    "\n",
    "- [Note: CS 20SI Lecture note 4: How to structure your model in TensorFlow](http://web.stanford.edu/class/cs20si/lectures/notes_04.pdf)\n",
    "- [Blog: Word2Vec word embedding tutorial in Python and TensorFlow](http://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "- [Blog: Word2vec Tutorial](http://rare-technologies.com/word2vec-tutorial/)\n",
    "- [Blog: Demystifying Word2Vec](http://www.deeplearningweekly.com/blog/demystifying-word2vec)\n",
    "- [Blog: Word2Vec Tutorial - The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)\n",
    "- [Blog: Word2Vec Tutorial Part 2 - Negative Sampling](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/)\n",
    "- [Blog: Natural Language Processing Made Easy – using SpaCy (in Python)](https://www.analyticsvidhya.com/blog/2017/04/natural-language-processing-made-easy-using-spacy-%E2%80%8Bin-python/)\n",
    "- [Paper: Yoav Goldberg, Omer Levy (2014) word2vec Explained: deriving Mikolov et al.'s negative-sampling word-embedding method](https://arxiv.org/abs/1402.3722)\n",
    "- [Paper: Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean (2013) Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546)\n",
    "- [Youtube: PyData DC 2016: Modern NLP in Python](https://www.youtube.com/watch?v=6zm9NC9uRkk)\n",
    "- [Notebook: PyData DC 2016: Modern NLP in Python](http://nbviewer.jupyter.org/github/skipgram/modern-nlp-in-python/blob/master/executable/Modern_NLP_in_Python.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "120px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
