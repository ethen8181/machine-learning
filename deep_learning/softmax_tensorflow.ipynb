{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "@import url('http://fonts.googleapis.com/css?family=Source+Code+Pro');\n",
       "@import url('http://fonts.googleapis.com/css?family=Vollkorn');\n",
       "@import url('http://fonts.googleapis.com/css?family=Arimo');\n",
       "@import url('http://fonts.googleapis.com/css?family=Fira_sans');\n",
       "    \n",
       "    div.cell {\n",
       "        width: 1000px;\n",
       "        margin-left: 0% !important;\n",
       "        margin-right: auto;\n",
       "    }\n",
       "    div.text_cell code {\n",
       "        background: transparent;\n",
       "        color: #000000;\n",
       "        font-weight: 600;\n",
       "        font-size: 12pt;\n",
       "        font-style: bold;\n",
       "        font-family:  'Source Code Pro', Consolas, monocco, monospace;\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "\t}\n",
       "\t\n",
       "    div.input_area {\n",
       "        background: #F6F6F9;\n",
       "        border: 1px solid #586e75;\n",
       "    }\n",
       "\n",
       "    .text_cell_render h1 {\n",
       "        font-weight: 200;\n",
       "        font-size: 30pt;\n",
       "        line-height: 100%;\n",
       "        color:#c76c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 1em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    } \n",
       "    h2 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "        text-align: left;\n",
       "    }\n",
       "    .text_cell_render h2 {\n",
       "        font-weight: 200;\n",
       "        font-size: 16pt;\n",
       "        font-style: italic;\n",
       "        line-height: 100%;\n",
       "        color:#c76c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 1.5em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    } \n",
       "    h3 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h3 {\n",
       "        font-weight: 200;\n",
       "        font-size: 14pt;\n",
       "        line-height: 100%;\n",
       "        color:#d77c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 2em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    }\n",
       "    h4 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h4 {\n",
       "        font-weight: 100;\n",
       "        font-size: 14pt;\n",
       "        color:#d77c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "        white-space: nowrap;\n",
       "    }\n",
       "    h5 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 200;\n",
       "        font-style: normal;\n",
       "        color: #1d3b84;\n",
       "        font-size: 16pt;\n",
       "        margin-bottom: 0em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "        white-space: nowrap;\n",
       "    }\n",
       "    div.text_cell_render{\n",
       "        font-family: 'Fira sans', verdana,arial,sans-serif;\n",
       "        line-height: 125%;\n",
       "        font-size: 115%;\n",
       "        text-align:justify;\n",
       "        text-justify:inter-word;\n",
       "    }\n",
       "    div.output_wrapper{\n",
       "        margin-top:0.2em;\n",
       "        margin-bottom:0.2em;\n",
       "    }\n",
       "\n",
       "    code{\n",
       "      font-size: 70%;\n",
       "    }\n",
       "    .rendered_html code{\n",
       "    background-color: transparent;\n",
       "    }\n",
       "    ul{\n",
       "        margin: 2em;\n",
       "    }\n",
       "    ul li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.5em; \n",
       "    }\n",
       "    ul li li{\n",
       "        padding-left: 0.2em; \n",
       "        margin-bottom: 0.2em; \n",
       "        margin-top: 0.2em; \n",
       "    }\n",
       "    ol{\n",
       "        margin: 2em;\n",
       "    }\n",
       "    ol li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.5em; \n",
       "    }\n",
       "    ul li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.2em; \n",
       "    }\n",
       "    a:link{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    a:visited{\n",
       "       font-weight: bold;\n",
       "       color: #1d3b84;\n",
       "    }\n",
       "    a:hover{\n",
       "       font-weight: bold;\n",
       "       color: #1d3b84;\n",
       "    }\n",
       "    a:focus{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    a:active{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    .rendered_html :link {\n",
       "       text-decoration: underline; \n",
       "    }\n",
       "    .rendered_html :hover {\n",
       "       text-decoration: none; \n",
       "    }\n",
       "    .rendered_html :visited {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .rendered_html :focus {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .rendered_html :active {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "    } \n",
       "    hr {\n",
       "      color: #f3f3f3;\n",
       "      background-color: #f3f3f3;\n",
       "      height: 1px;\n",
       "    }\n",
       "    blockquote{\n",
       "      display:block;\n",
       "      background: #fcfcfc;\n",
       "      border-left: 5px solid #c76c0c;\n",
       "      font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "      width:680px;\n",
       "      padding: 10px 10px 10px 10px;\n",
       "      text-align:justify;\n",
       "      text-justify:inter-word;\n",
       "      }\n",
       "      blockquote p {\n",
       "        margin-bottom: 0;\n",
       "        line-height: 125%;\n",
       "        font-size: 100%;\n",
       "      }\n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    scale:100,\n",
       "                        availableFonts: [],\n",
       "                        preferredFont:null,\n",
       "                        webFont: \"TeX\",\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code for loading the format for the notebook\n",
    "import os\n",
    "\n",
    "# path : store the current path to convert back to it later\n",
    "path = os.getcwd()\n",
    "os.chdir(os.path.join('..', 'notebook_format'))\n",
    "from formats import load_style\n",
    "load_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethen 2017-06-22 14:04:27 \n",
      "\n",
      "CPython 3.5.2\n",
      "IPython 5.4.1\n",
      "\n",
      "numpy 1.12.1\n",
      "pandas 0.20.2\n",
      "matplotlib 2.0.2\n",
      "tensorflow 1.1.0\n"
     ]
    }
   ],
   "source": [
    "os.chdir(path)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. magic for inline plot\n",
    "# 2. magic to print version\n",
    "# 3. magic so that the notebook will reload external python modules\n",
    "%matplotlib inline\n",
    "%load_ext watermark\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "%watermark -a 'Ethen' -d -t -v -p numpy,pandas,matplotlib,tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow\n",
    "\n",
    "TensorFlow provides multiple APIs. The lowest level API--TensorFlow Core-- provides you with complete programming control. We recommend TensorFlow Core for machine learning researchers and others who require fine levels of control over their models\n",
    "\n",
    "## Hello World\n",
    "\n",
    "We think of TensorFlow Core programs as consisting of two discrete sections:\n",
    "\n",
    "- Building the computational graph.\n",
    "- Running the computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note that this is simply telling tensorflow to \n",
    "# create a constant operation, nothing gets\n",
    "# executed until we start a session and run it\n",
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "hello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, TensorFlow!'\n"
     ]
    }
   ],
   "source": [
    "# start the session and run the graph\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of tensorflow as a system to define our computation, and using the operation that we've defined it will construct a computation graph (where each operation becomes a node in the graph). The computation graph that we've defined will not be `run` unless we give it some context and explicitly tell it to do so. In this case, we create the `Session` that encapsulates the environment in which the objects are evaluated (execute the operations that are defined in the graph).\n",
    "\n",
    "Consider another example that simply add and multiply two constant numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mutiply:  6.0\n",
      "add:  5.0\n",
      "add:  5.0\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant(2.0, tf.float32)\n",
    "b = tf.constant(3.0) # also tf.float32 implicitly\n",
    "c = a + b\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print('mutiply: ', sess.run(a * b))\n",
    "    print('add: ', sess.run(c)) # note that we can define the add operation outside \n",
    "    print('add: ', sess.run(a + b)) # or inside the .run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example above is not especially interesting because it always produces a constant result. A graph can be parameterized to accept external inputs, known as `placeholders`. Think of it as the input data we would give to machine learning algorithm at some point.\n",
    "\n",
    "We can do the same operation as above by first defining a `placeholder` (note that we must specify the data type). Then `feed` in values using `feed_dict` when we `run` it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mutiply:  6.0\n",
      "add:  5.0\n"
     ]
    }
   ],
   "source": [
    "a = tf.placeholder(tf.float32)\n",
    "b = tf.placeholder(tf.float32)\n",
    "\n",
    "# define some operations\n",
    "add = a + b\n",
    "mul = a * b\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print('mutiply: ', sess.run(mul, feed_dict = {a: 2, b: 3}))\n",
    "    print('add: ', sess.run(add, feed_dict = {a: 2, b: 3}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some matrix operations are the same compared to numpy. e.g. \t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.  4.]\n",
      " [ 5.  6.]\n",
      " [ 6.  7.]]\n",
      "[ 3.5  5.5  6.5]\n",
      "[1 1 1]\n",
      "[ 3.5  5.5  6.5]\n",
      "[1 1 1]\n"
     ]
    }
   ],
   "source": [
    "c = np.array([[3.,4], [5.,6], [6.,7]])\n",
    "print(c)\n",
    "print(np.mean(c, axis = 1))\n",
    "print(np.argmax(c, axis = 1))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    result = sess.run(tf.reduce_mean(c, axis = 1))\n",
    "    print(result)\n",
    "    print(sess.run(tf.argmax(c, axis = 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functionality of `numpy.mean` and `tensorflow.reduce_mean` are the same. When axis argument parameter is 1, it computes mean across (3,4) and (5,6) and (6,7), so 1 defines across which axis the mean is computed (axis = 1, means the operation is along the column, so it will compute the mean for each row). When it is 0, the mean is computed across(3,5,6) and (4,6,7), and so on. The same can be applied to argmax which returns the index that contains the maximum value along an axis.\n",
    "\n",
    "## Linear Regression\n",
    "\n",
    "We'll start off by writing a simple linear regression model. To do so, we first need to understand the difference between `tf.Variable` and `tf.placeholder`. \n",
    "\n",
    "> [Stackoverflow](http://stackoverflow.com/questions/36693740/whats-the-difference-between-tf-placeholder-and-tf-variable). The difference is that with `tf.Variable` you have to provide an initial value when you declare it. With `tf.placeholder` you don't have to provide an initial value and you can specify it at run time with the `feed_dict` argument inside `Session.run`.\n",
    "> In short, we will use `tf.Variable` for trainable variables such as weights (W) and biases (B) for our model. On the other hand, `tf.placeholder` is used to feed actual training examples.\n",
    "\n",
    "Also note that, constants are automatically initialized when we call `tf.constant`, and their value can never change. By contrast, variables are not initialized when we call `tf.Variable`. To initialize all the variables in a TensorFlow program, we must explicitly call a special operation called `tf.global_variables_initializer()`. Things will become clearer with the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01  # learning rate for the optimizer (gradient descent)\n",
    "n_epochs = 1000  # number of iterations to train the model\n",
    "display_epoch = 100  # display the cost for every display_step iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make up some trainig data\n",
    "X_train = np.asarray([3.3, 4.4, 5.5, 6.71, 6.93, 4.168, 9.779, 6.182, 7.59, \n",
    "                      2.167, 7.042, 10.791, 5.313, 7.997, 5.654, 9.27, 3.1], dtype = np.float32)\n",
    "y_train = np.asarray([1.7, 2.76, 2.09, 3.19, 1.694, 1.573, 3.366, 2.596, 2.53, \n",
    "                      1.221, 2.827, 3.465, 1.65, 2.904, 2.42, 2.94, 1.3], dtype = np.float32)\n",
    "\n",
    "# placeholder for the input data\n",
    "X = tf.placeholder(tf.float32)\n",
    "Y = tf.placeholder(tf.float32)\n",
    "\n",
    "# give the model's parameter a randomized initial value\n",
    "W = tf.Variable(np.random.randn(), tf.float32, name = 'weight')\n",
    "b = tf.Variable(np.random.randn(), tf.float32, name = 'bias')\n",
    "\n",
    "# Construct the formula for the linear model\n",
    "# we can also do\n",
    "# pred = tf.add(tf.multiply(X, W), b)\n",
    "pred = W * X + b\n",
    "\n",
    "# we then define the loss function that the model is going to optimize on,\n",
    "# here we use the standard mean squared error, which is sums the squares of the\n",
    "# prediction and the true y divided by the number of observations, note\n",
    "# that we're computing the difference between the prediction and the y label\n",
    "# from the placeholder\n",
    "cost = tf.reduce_mean(tf.pow(pred - Y, 2))\n",
    "\n",
    "# after defining the model structure and the function to optimize on,\n",
    "# tensorflow provides several optimizers that can do optimization task\n",
    "# for us, the simplest one being gradient descent\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "train = optimizer.minimize(cost)\n",
    "\n",
    "# initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100, cost: 0.18693093955516815\n",
      "Epoch: 200, cost: 0.1741979718208313\n",
      "Epoch: 300, cost: 0.1663670837879181\n",
      "Epoch: 400, cost: 0.16155104339122772\n",
      "Epoch: 500, cost: 0.15858913958072662\n",
      "Epoch: 600, cost: 0.15676751732826233\n",
      "Epoch: 700, cost: 0.15564727783203125\n",
      "Epoch: 800, cost: 0.15495829284191132\n",
      "Epoch: 900, cost: 0.15453457832336426\n",
      "Epoch: 1000, cost: 0.15427397191524506\n",
      "Optimization Finished!\n",
      "Training cost: 0.15427397191524506, W: 0.24345557391643524, b: 0.856789231300354\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAFsCAYAAAADhPr/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl4lPW5//H3NyGEEAhLFIQQVsWwGMKirT2W0qIgoiJq\nVJQiInUp1V9qFbElbkGLnh7LsUfrAiqKoEbAnaX11HpasQphkUWQnSBYFiWQDDBJvr8/AjHDTMiE\nzMzzzMzndV25ZO55JnPzEPOZ+1mNtRYRERFxpwSnGxAREZHaKahFRERcTEEtIiLiYgpqERERF1NQ\ni4iIuJiCWkRExMUU1CIiIi6moBYREXExBbWIiIiLKahFRERcrJHTDQB89NFHNjk5uV6v8Xg8pKSk\nhKkjqY3WuzO03p2h9e6MeFnvZWVlewcPHnx6Xcu5IqiTk5PJysqq12uKiorq/RppOK13Z2i9O0Pr\n3Rnxst6Lioq2BbOcNn2LiIi4mIJaRETExRTUIiIiLqagFhERcTFXHEx2MkeOHOHw4cMYY3zqrVq1\noqSkxKGu4lco17u1liZNmlDfI/5FROKJq4O6tLQUgLS0NL+gTkxMJDU11Ym24loo17u1lrKyMsrL\ny/VvKSJSi6A2fRtjZhljdhtjSowxG4wx42tZbqwxpsIYc6jG16BTba6iooLU1FS/kJbYYIwhNTWV\niooKp1sREXGtYCfqqcAt1toyY0wW8JExZrm1dlmAZZdYay8IXYsiIiLxK6iJ2lq72lpbdvzhsa9u\nYesqxj3xxBPceeedIV+2Lq1bt2bz5s1BLTt16lRuvfXWkLyviIicuqD3URtjngbGAinAcuCDWhbt\na4zZC+wHXgF+b60tb2CfrjV79myeeuoptm7dSvPmzRk+fDj3338/LVq0qPU1d911V9Dfvz7LOmXC\nhAm0b9+e3/3ud063IiISc4IOamvtL40xdwDnA4OAIwEW+xjoDWwDegGvA+XA709c0BhzC3ALwNy5\ncykrKztxEVq1akViYmLAfiorK6sPNnPKs88+yzPPPMMTTzzBBRdcwO7du/nd737HiBEjmD9/Po0b\nN/Z7TXl5OY0aOXcMX1lZWVDrzev1Ul5e7rdsoPXu9Xo5evToKf977Nu3j40bN57Sa+NFWVkZRUVF\nTrcRd7TenaH17stYa+v/ImOeAdZaa5+sY7nrgHustf1PttySJUtsoOu6lpSUkJaWFvA1paWlJz1S\nOKmwkJSCAhJ27qQyIwNPfj7e3NyTtVEvJSUl9OrViyeffJKRI0dW1w8dOkTfvn154IEHGD16NFOn\nTmXdunU0adKEBQsWMGXKFL7++mu2bNnCs88+C8Brr73Go48+SmlpKbfddhuzZs3iv//7vxk0aBBT\np06tXnb79u3k5OTw1FNP8eijj+LxeLj99tv5zW9+A8CyZcu477772LBhAykpKVx22WVMmTKl+gND\n69atWbp0KV27dvX7+2zbto0JEyawatUq+vfvz1lnncWBAweqexw7diyffvopHo+H3r1784c//IEe\nPXrw0ksvMXHiRIwxNG7cmAsuuIA5c+Ywbdo0Xn75Zfbu3Uv79u2ZPHkyl156aa3rsrZ/Z6lSVFRE\nv379nG4j7mi9O8Nt6/2jTd9y8Eg5F5+dTlJi6C4/UlRUtGzw4MED6lruVN+xEcHto7ZAxA/ZTios\nJDUvj8TiYoy1JBYXk5qXR1JhYcje47PPPuPw4cNcdtllPvVmzZpx0UUX8dFHH1XXFixYwOWXX87W\nrVvJPeHDwpdffsk999zDc889x7p16ygpKWHXrl0nfe9PP/2Uzz77jLfeeov//M//ZP369UDVqVOP\nPPIIGzduZNGiRfz9739nxowZQf19fvGLX9CnTx+++uor7rnnHl577TWf5y+88EI+//xzli9fTnZ2\ndvX+67Fjx5Kbm8udd97Jjh07mDNnDgCdO3fm/fffZ+vWrUycOJHbbruN3bt3B9WLiIgblB2tYMj0\n5Tz6t6386ZNidl98ZUhzJFh1BrUxpo0x5jpjTDNjTKIxZigwCvgwwLLDjDFtj/05C8gH3g5103VJ\nKSjAeDy+vXk8pBQUhOw99u/fT3p6esDN2G3btmXfvn3Vj88991yGDx9OQkKC363b3nnnHYYOHcoP\nf/hDGjduzH333Vfn6WgTJ04kJSWF3r1707t3b1avXg1ATk4O5557Lo0aNaJjx46MHTuWTz75pM6/\nS3FxMcuXL+e3v/0tycnJ/OhHP2Lo0KE+y4wePZrmzZuTnJzMpEmTWL169UkvfHLFFVfQrl07EhIS\nuPLKK+natas2ZYlI1Fi8YR9XvLzKp3bOso9DPvQFI5idpRa4HXiGqmDfBuRZa98xxnQE1gI9rbXb\ngcHAS8aYZsA3wCzg0bB0fhIJO3fWq34qWrduzb59+wLuc/7mm29IT0+vfpyRkVHr99m9e7fP802b\nNqV169Ynfe+2bdtW/zklJaV63/DGjRuZPHkyK1asoKysjIqKCvr06VPn32XXrl20bNnSZ1dCZmYm\nO4+tr4qKCqZMmcLbb7/N3r17SUio+ny3b9++WjdZv/baazz99NNs374dqNpVUfPDi4iIGx0pr2Tk\ny6sor/x+t/Ad7z/HmL+/XvXg2NAXyl2pdalzorbW7rHW/sRa29Jam2atPcda+/yx57Zba5sdC2ms\ntXdba9taa1OttV2ttfdba73h/kucqLKWYKytfirOO+88kpOTeffdd33qhw4d4q9//SsDBw6srp1s\nQm7bti1ff/119WOPx8P+/ftPqae7776bs846i88//5zt27czefJkgjkG4YwzzuC7777zORisuLi4\n+s9vvvkmH3zwAfPnz2ft2rWsXLkSoNbvvWPHDvLy8nj88cfZtGkTW7dupUePHkH1IiLilI+3fMtl\nL630CelFD131fUgfE8qhLxgxeVMOT34+9oRNzDYlBU9+fsjeIy0tjYkTJzJp0iT++te/4vV62b59\nO+PGjaN9+/Zce+21QX2fyy+/nEWLFvGvf/2Lo0eP8thjj51yoB06dIjmzZvTrFkzNmzYwIsvvhjU\n6zIzM8nJyWHq1KkcPXqUTz/9lIULF/p83+TkZFq1aoXH46HghF0Ibdq0YevWrdWPS0tLMcZUb1V4\n9dVXWbdu3Sn9nUREws1bUck1s75gyodbq2uj+rTl0ydvpHXpd37Lh3LoC0ZMBrU3N5fSadOo6NAB\nawwVHTpQOm1ayDdV3HnnnUyePJn777+fTp06cdFFF5GRkcFbb70V9I0mevTowdSpUxk/fjw9evQg\nNTWV008//ZRuVPHwww8zd+5cOnbsSF5eHldccUXQr33++edZtmwZ3bp147HHHuO6666rfu7aa68l\nMzOT3r1787Of/YwBA3wPUhw9ejTr16+nc+fOjB49mqysLCZMmMDQoUM5++yzWbt2LT/4wQ/q/fcR\nEQm3z3eUMPzFlXx3+PvLfbw6qhc3nds+IkNfME7p9KxQC8fpWdHq0KFDdOnShaVLl9KpUyen2/ET\njvWu07Pq5rbTVeKF1rszIrHeKyott8xdx44D318S5LIep3HHf2T6LBfOU32DPT3L1XfPihcLFy5k\n4MCBWGu5//776dmzJx07dnS6LRGRmLRq1yHufv8rn9qLuT3JaOG/JdObmxvRA8cCUVC7wAcffMBt\nt92GtZa+ffsyffp03TFMRCTErLX8+t2vWPvv7w+c/Wm3Vtz3086O9RQMBbULPPnkkzz55Ekv8iYi\nIg2wYU8Zv3p7vU/tuauy6NwqpZZXuIeCWkREYpa1lvzFm/lsx/cXaDovM42CIV2jZsulglpERGLS\n1m893DL3S5/an0Z05+zTo+tAZAW1iIjEnMc+2sqHG7+tftyzTSp/vOysqJmia4rJ86hFRMQ9kgoL\nScvOpmV6OmnZ2WG9VvbXJUcYMn25T0j/YfhZTLu8e1SGNGiiFhGRMDp+N8PjN0o6fjfDUgj5aU9P\nfbKDt9furX7coUUyz1/Vg8SE6Azo4zRRh0FxcTGZmZlUVFSE7T0mTJjAI488EvC52bNnM2zYsOrH\nmZmZPpf4FBGJlEjczXBP6VGGTF/uE9JThnblhdyeUR/SoIm6Qfr06cOePXtITEysrn322Wd06NCB\nHTt2VNcuu+wycnNzGTNmTHWtdevWLF26lK5du4a9z5q9iIhEUrjvZvjS0q+ZveKb6sctmzTi1VG9\nSEqMnTlUQd1As2fPZtCgQU63ISLiSpUZGSTWuBtfzXpDfOfxcs2rq31qk3/WmYFdWzXo+7pR7Hzk\ncJHt27fTunVrysvLmTJlCkuWLOHee+8lMzOTiRMnMnz4cAAGDhxIZmYm8+bNA2DRokUMHDiQzp07\nM3ToUNasWVP9PVetWsWgQYPo2LEj48aN4/Dhw0H307p1azZv3gxUbTK/5557uPbaa+nYsSMXXngh\nW7ZsqV52w4YNjBw5kq5du3Leeecxf/78UKwSEYlT4bixxRsrv/EJ6UYJhnfH9onJkAYFddhNnjyZ\n888/n8cee4wdO3bw+OOP8/777wPw8ccfs2PHDq688kpWrVrFHXfcwRNPPMGmTZsYO3Ys119/PUeO\nHOHo0aOMHj2aa665hk2bNjFixAi/+2DXx7x585g4cSKbN2+ma9euTJkyBai64caVV17J1VdfzYYN\nG5g+fTr33HMPX375ZR3fUUQksFDezbD0aAVDpi9n+udfV9fu+nFHPhiXQ3Kj2I2zqNr0PWT68oi8\nz+LxfYNe9uc//3n1PuoLLriAWbNmndJ7zpw5kxtvvLH6FpKjRo3ij3/8I0uXLsUYQ3l5ObfffjvG\nGEaMGMHTTz99Su8DMHz4cPr37w9Abm4ukydPBqom+o4dO3LDDTcAkJ2dzWWXXcbbb79NoLubiYgE\nIxQ3tnhv3V6e/Kfv8TZvjcmmaePEWl4RO6IqqN3olVdeCck+6h07dvDaa6/x/PPPV9e8Xi+7d+8G\noF27dj7nAGZmZvp9j2C1bdu2+s8pKSkcOnQIqDpafdmyZXTu3Ln6+YqKCq655ppTfi8RkYbweCsY\nMXOVT+32H2YwsncbhzqKvKgK6pqTbjTdjzqYk+wzMjK46667+M1vfuP33D//+U927dqFtbb6exUX\nF9OlS5eQ9pmRkcGPfvQj7ZcWEVf43437mfrRNp/am6PPIa1JVEVXg8XuRn0XOf3009m2zfeHrU2b\nNj7nNo8ZM4YXX3yRpUuXYq2ltLSUxYsXc/DgQc4991wSExN59tln8Xq9vPvuuxQVFYW8zyFDhrBp\n0yZef/11vF4vXq+XoqIi1q9fX/eLRURCpLwSrpi50iekb+zfjsXj+8ZdSIOCOiJuvfVW3nnnHbp0\n6cKkSZMAuPfee5kwYQKdO3dm/vz59O3bl2nTpnHvvffSpUsXBgwYwOzZswFo3LgxL7/8MnPmzKFb\nt27Mnz+fSy+9NOR9Nm/enLlz5zJv3jx69uxJVlYWDz30EEePHg35e4mIBPLJtu+YvMJQ5q2srr12\nfW9u6HuGg105y1hrne6BJUuW2EAHK5WUlJCWlhbwNdG06TuWhGO9n+zfWaoUFRXRr18/p9uIO1rv\nkVNRabnxjTX8+5C3unb1OW245QcNO9/azYqKipYNHjx4QF3Lxd82BBERcZWinSVMWrDJp/bKtb1o\n27yxQx25i4JaREQcUWktv5y/ns37v78W+MXd0xnUbK9CugYFtYiIRNzab0rJe3eDT23G1T3IbNmE\noqK9tbwqPimoRUQkYqy1TPxgIyt3HaquXdC5BfdfGP4bFEUrBbWIiETEpn1l3D7f93TPP488m27p\nTR3qKDooqEVEJOx+MXcd2779/mZCOe2b8diwM4O6IFS8c3VQJyYmUlpaStOmTfWPGYOstZSVlfnc\nz1tEYsuSbQd44C+bfWrTLutOz7Y6vTZYrg7q1NRUjhw5QklJiV9Q79u3j/T0dIc6i1+hXO/WWpo0\naUJycnJIvp+IuEugGyktvDmHBA1e9eLqoAZITk4O+It848aNIb/WtdRN611E6rJq1yHufv8rn9qo\nPm256dz2DnUU3Vwf1CIiEj0CTdHvje1D4xi+X3S4ac2JyEklFRaSlp1Ny/R00rKzSSosdLolcaFN\n+8r8Qvri7uksHt9XId1AmqhFpFZJhYWk5uVhPFVXjkosLiY1L49SwJub62xz4hrDX1iBt9L3vhFv\njcmmaWMdKBoK+pgjIrVKKSioDunjjMdDSkGBQx2Jm+w8cIQh05f7hPQPMtNYPL6vQjqENFGLSK0S\ndu6sV13ix42vr2HXQd9b4L5xQ29apiQ51FHsUlCLSK0qMzJILC4OWJf4tK/Uy6g5q31qZ6an8PRI\n/1sVS2goqEWkVp78fJ991AA2JQVPfr6DXYlT7npvA6t3l/rUZl3XizbNdKercFJQi0itvLm5lFK1\nrzph504qMzLw5OfrQLI4c/BIOVe98oVPrWWTRrwx+hyHOoovCmoROSlvbq6COY498uEW/r7lO5/a\n9Kt60LFVE4c6ij8KahER8ePxVjBi5iq/+uLxfR3oJr4pqEVExMdTnxTz9to9PrUnL+9OVhvdSMMJ\nCmoREQHAW1HJ8BdX+tU1RTtLQS0iIry6fDczl+3yqU0d1o1+GWkOdSTHKahFROJYRaVl2Asr/Oqa\not1DQS0iEqee+Hg7Czfs86lN/llnBnZt5VBHEoiCWkQkzlhrGTrDf4pedHMOxhgHOpKTUVCLiMSR\nmct28ery3T61YWen8+sfd3SoI6mLglpEJE6ceL9ogAXjckhM0BTtZgpqEZEY9+7aPfzpE9+bq5zb\nIY1HLu7mUEdSHwpqEZEYFmiKfm9sHxo3SnCgGzkVCmoRkRj08ZZvmfLhVp9aZotkZuT2dKYhOWUK\nahGRGBNoip4/JpvUxokOdCMNpaAWEYkRK78+yD0fbPSpJSUY3h+X41BHEgoKahGRGBBoip5zfW/S\nmyY50I2EkoJaRCSKbdpXxu3z1/vVdQnQ2BFUUBtjZgEXAk2B3cDj1trptSz7a+DeY8u+CdxurT0S\nmnZFROS4QFP0i7k9yGjRxIFuJFyCPT5/KtDVWpsGXA5MMcb0P3EhY8xQYBIwGOgEdAUeClGvIiIC\n7Dp4JGBILx7fVyEdg4KaqK21q2s+PPbVDVh2wqI3AjOstWsAjDEPA7OpCm8REWmgQAH9PyPOpvvp\nTR3oRiIh6DPejTFPG2PKgC+BXcAHARbrBdS86/hKoK0xJr1BXYqIxLnvPN5ap2iFdGwz1trgFzYm\nETgfGAQ8Zq31nvD8JmCCtXbhscdJwFGgi7V26wnL3gLcAjB37tz+LVq0qFfjZWVlNG2qH85I03p3\nhta7M9yy3h9eCWUVvtfjHnempXuaQw2FmVvWewQsGzx48IC6FqrXUd/W2grgH8aY0cDtwJMnLHII\nqPmjczx9Dwb4Xs8BzwEsWbLEZmVl1acVioqK6NevX71eIw2n9e4MrXdnOL3ePd4KRsxc5VeP9SO6\nnV7vkVJUVBTUcqd6elYjqvZRn2gN0Ad449jjPsA31tp9AZYVEZFaTHjrS77a6/GpTRrUiZ+d2dqh\njsQpdQa1MaYN8DPgPcBD1Wlao459nehl4CVjzKtU7cfOB14KVbMiIrHOW1HJ8BdX+tVjfYqW2gUz\nUVuqNnM/Q9XBZ9uAPGvtO8aYjsBaoKe1dru1dqEx5nHgb0AKMBd4IDyti4jElof/upl/bD3gU7vt\nhxlc2buNQx2JG9QZ1NbaPcBPanluO9DshNoTwBMh6U5EJA5UWsvFM1b41SM1RScVFpJSUEDCzp1U\nZmTgyc/Hm5sbkfeWuukSoiIiDvrzp8XMX73Hp5Z7Tht+8YOMiLx/UmEhqXl5GE/V/vDE4mJS8/Io\nBYW1SyioRUQcEui86EU352CMCbB0eKQUFFSH9HHG4yGloEBB7RIKahGRCHt95TfM+Pxrn9qgri35\n7c+6RLyXhJ0761WXyFNQi4ic4Pg+25+FYZ9toCn6g3E5NEqI3BRdU2VGBonFxQHr4g5BX0JURCQe\nHN9nm1hcjLG2ep9tUmFhg77v4g37/EK6Z5tUFo/v61hIA3jy87EpKT41m5KCJz/foY7kRJqoRURq\nCMc+20BT9Ns3ZpOSlHhK3y+UvLm5lIKO+nYxBbWISA2h3Gf7r+0HyF+82afWumkjXrv+nFPqLVy8\nubkKZhdTUIuI1BCqfbaBpug3R59DWhP92pX60T5qEZEaGrrPdt2/S2u9HaVCWk6FfmpERGpoyD7b\nQAE967petGnWOAydSrxQUIuInOD4Pttgb7e4/dvDjJ+7zq+uG2lIKCioRUQaINAU/eyVWXRpnRJg\naZH6U1CLiJyCPaVHuWHOGr+6pmgJNQW1iEg9BZqin7j0LHqf0SzA0iINo6O+RSSskgoLScvOpmV6\nOmnZ2Q2+wpeTDh4pr/WIboW0hIsmahEJm1i6heLPX1vDN4eO+tQeuqgr53dq4VBHEi80UYtI2Jzs\ncpzR4kh5JUOmL/cL6cXj+yqkJSI0UYtI2ET7LRSf2wCbi1b61H59QSbDsk5zqCOJRwpqEQmbaL2F\nYnml5ZIXVgC+d7XSEd3iBG36FpGwicZbKP7izXXHQvp7N/Zvp5AWx2iiFpGwiaZbKFprGTpjhV99\n0c05GOPc/aJFFNQiElbRcAvFBxZvZsn2Az61s09vyk2ZpQppcZw2fUvExdJ5tRL9hkxf7hfSC8bl\n8KcRZzvUkYgvTdQSUbF0Xq1Etz8vKWb+mj0+tWaNE5k3JtuhjkQCU1BLRJ3svFoFtURKoKuLvTu2\nD8mNtJFR3EdBLREV7efVSnSb+8W/efZf/j9rOqJb3ExBLREVrefVSvQLNEW/Ofoc0pro16C4m7bz\nSERF43m1Et3+tunbWm+koZCWaKCfUomoaDqvVqJfoIB+5dpetG3e2IFuRE6NgloiLhrOq5XotuLr\ng0z8YKNfXfuiJRopqEUkpgSaov888my6pTd1oBuRhlNQi0hMWPtNKXnvbvCra4qWaKegFpGoF2iK\nnjqsG/0y0hzoRiS0FNQiErV2HjjMTYXr/OqaoiWWKKhFJCoFmqJv/UEGV53TxoFuRMJHQS0iUeU7\nj5drXl3tV9cULbFKQS0iUSPQFD2i52lM+FGmA92IRIaCWkRc73B5JZe/tNKvrila4oGCWkRcLdAU\nPaBDcx69+EwHuhGJPAW1iLhSRaVl2Asr/OqaoiXeKKhFxHUufXEFRyusT61NsyRmXdfboY5EnKOg\nFhHXsNYydIb/FL3w5hwSjHGgIxHnKahFxBXueHs96/eU+dW1qVvinYJaRBwX6ICx927qQ+PEBAe6\nEXEX/V8gEsWSCgtJy86mZXo6adnZJBUWOt1SvTz+0daAIb14fF+FtMgxmqhFolRSYSGpeXkYjweA\nxOJiUvPyKIWouN93oICe9/NzaJasX0siNekjq0iUSikoqA7p44zHQ0pBgUMdBWdW0a5ap2iFtIg/\n/V8hEqUSdu6sV90NAgX0rOt60aZZYwe6EYkOmqhFolRlRka96k5atGFfrVO0Qlrk5DRRi0QpT36+\nzz5qAJuSgic/38Gu/AUK6D+PPJtu6U0d6EYk+iioRaKUNzeXUqr2VSfs3EllRgae/HzXHEi2rLiE\n+xZu8qvrvGiR+lFQi0Qxb26ua4K5pkBT9KMXd2NAhzQHuhGJbtpHLSIhs/ab0lr3RYc7pKP9nHKR\n2miiFpGQCBTQdw/syJDu6WF/72g/p1zkZDRRi0iDfF1ypNYpOhIhDdF7TrlIMOoMamNMsjFmhjFm\nmzHmoDFmhTFmWC3LjjXGVBhjDtX4GhTyrkXEFYZMX87YN9b61C7JSo/4AWPReE65SLCC2fTdCNgB\n/ATYDlwCvGGMOcdauzXA8kustReErkURcZuSw+VcPesLv7pTR3RXZmSQWFwcsC4S7eqcqK21pdba\nB621W621ldba94AtQP/wtycibjNk+nK/kO7dNtXR0648+fnYlBSfmhvPKRc5FfU+mMwY0xboDqyp\nZZG+xpi9wH7gFeD31tryU29RRNzgaHkll7600q/uhvOi3X5OuUhDGGtt8AsbkwQsADZZa28N8HxX\nwALbgF7A68Ar1trfB1j2FuAWgLlz5/Zv0aJFvRovKyujaVNd2SjStN6d4fR6n1Rk/GrJCZaHchxo\nJoKcXu/xKo7W+7LBgwcPqGuhoIPaGJMAzAbSgBHWWm8Qr7kOuMdae9LN5EuWLLFZWVlB9XFcUVER\n/fr1q9drpOG03p3h1HqvtJaLZ6zwqy+6OQdj/MM71ujn3Rnxst6LioqCCuqgNn2bqv8jZwBtgUuC\nCeljLBD7/zeLxKBAp1yBOzZ1i8STYPdR/xnoAVxorfXUttCx07aKrLXfGGOygHxAlwcSiTKBQnrB\nuBwSE/S5WyTS6gxqY0wn4FbgCLC7xuauW4H/A9YCPa2124HBwEvGmGbAN8As4NEw9C0iYTD2jbV8\nXXLEr64pWsQ5dQa1tXYbJ9983azGsncDd4egLxGJsEBT9Ns3ZpOSlOhANyJynK71LRLnHvzLZj7Z\ndsCvrilaxB0U1CJxLNAUPWdUb9JTkxzoRkQC0U05RMLIrbdenPHZzlpvpKGQFnEXTdQiYeLWWy8G\nCuhnr8yiS+uUAEuLiNM0UYuEidtuvbjgy721TtEKaRH30kQtEiZuuvVioICeOqwb/TLSIt6LiNSP\nglokTNxw68WlxSX8duEmv7qO6BaJHgpqkTDx5Of77KOGyN56MdAUfffAjgzpnh6R9xeR0FBQi4SJ\nU7de3LSvjNvnr/era4oWiU4KapEw8ubmRvQI70BT9PU5bRk7oH3EehCR0FJQi8SAvaVHuX7OGr+6\npmiR6KegFolygabogV1aMnlwFwe6EZFQU1CLRCmPt4IRM1f51TVFi8QWBbVIFAo0RWe2SGZGbk8H\nuhGRcFJQi0SRikrLsBdW+NU1RYvELgW1SJS45IUVlFdan1q75o2ZeW0vhzoSkUhQUIu4nLWWSUUG\n8A3pRTfnYIxxpikRiRgFtYiL/XL+l2zc5/Gra1O3SPxQUIu4VKADxt6/qQ9JibrpnUg8UVCLuMzv\n/7aVv22fBMvxAAAUf0lEQVT61q+uKVokPimoRVwk0BQ9f0w261evdKAbEXEDBbWIC7y49GvmrPjG\nr64pWkQU1CIOCzRFvzqqF6enNnagGxFxGwW1iEMWfLmXP/5jh19dU7SI1KSgFnFAoCn62Suz6NI6\nxYFuRMTNFNQiEfT5jhJ+t2iTX11TtIjURkEtEiGBpujHhp1J34zmDnQjItFCQS0SZl/tLWPCW+v9\n6pqiRSQYCmqRMAo0RU/8SScuPKu1A92ISDRSUIuEwb4yL6Nmr/ara4oWkfpSUIuEWKApevx57bkm\nu60D3YhItFNQi4RI6dEKRr68yq+uKVpEGkJBLRICgaboMf3OYHS/dg50IyKxREEt0gDeikqGv+h/\nwwxN0SISKgpqkVN04+tr2HXwqE9taPfW/GZgJ4c6EpFYpKAWqadKa7l4xgq/uqZoEQkHBbVIPfx2\n4UaWFh/0qfVp14z/HH6WQx2JSKxTUIsEKdABYwtvziHBGAe6EZF4oaAWqcOT/9jBe1/u9amlN01i\nzvW9HepIROKJglrkJAJN0e/d1IfGiQkOdCMi8UhBLRLA/27cz9SPtvnVdcCYiESaglrkBIGm6Plj\nskltnOhANyIS7xTUIscsLS7htws3+dSaJycy9+fZDnUkIqKgFgECT9Fv3NCblilJDnQjIvI9BbXE\ntfV7Srnj7Q1+de2LFhG3UFBL3Ao0Rc+8tiftmicH/T2SCgtJKSggYedOKjMy8OTn483NDWWbIhLn\nFNQSd4oPHGZc4Tq/en2n6KTCQlLz8jAeDwCJxcWk5uVRCgprEQkZBbXElUBT9J9Hnk239Kb1/l4p\nBQXVIX2c8XhIKShQUItIyCioJS7sK/MyavZqv3pD9kUn7NxZr7qIyKlQUEvMu+SFFZRXWp/aH4af\nSXa75g36vpUZGSQWFwesi4iEiq6DKDGr9GgFQ6Yv9wvpxeP7NjikATz5+diUFJ+aTUnBk5/f4O8t\nInKcJmqJSeMK11J84IhPLX9wF37cpWXI3sObm0sp6KhvEQkrBbXElKPllVz60kq/erjOi/bm5iqY\nRSSsFNQSM367cCNLiw/61O78j0wu7XGaQx2JiDScglqiXkWlZdgLK/zqurqYiMQCBbVEtT/+33YW\nrN/nUxvd9wzG9G/nUEciIqFV51HfxphkY8wMY8w2Y8xBY8wKY8ywkyz/a2PMbmNMiTHmBWNM8Ndj\nFAmStZYh05f7hfSim3MU0iISU4I5PasRsAP4CdACmAy8YYzpfOKCxpihwCRgMNAJ6Ao8FKJeRQB4\npWgXQ2f4buq+uHs6i8f3xRjjUFciIuFR56Zva20p8GCN0nvGmC1Af2DrCYvfCMyw1q4BMMY8DMym\nKrxFGizQJUAXjMshMUEBLSKxqd4XPDHGtAW6A2sCPN0LqHluzEqgrTEm/dTaE6ny3rq9fiHdP6M5\ni8f3VUiLSEwz1tq6lzq+sDFJwAJgk7X21gDPbwImWGsX1lj+KNDFWrv1hGVvAW4BmDt3bv8WLVrU\nq/GysjKaNq3/jRSkYZxY75OK/IO4IMeSFEfX1dPPuzO03p0RR+t92eDBgwfUtVDQR30bYxKAV6gK\n3l/VstghIK3G4+Ppe/DEBa21zwHPASxZssRmZWUF2woARUVF9OvXr16vkYaL5Hr/x9bvePivW3xq\nGWnJvHhNz4i8v5vo590ZWu/OiJf1XlRUFNRyQQW1qTpCZwbQFrjEWuutZdE1QB/gjWOP+wDfWGv3\n1bK8SECB9kXP+/k5NEvWGYUiEl+C3Xj4Z6AHcJm11nOS5V4GbjbG9DTGtALygZca1qLEk1W7DvmF\ndIKpuniJQlpE4lGdv/mMMZ2AW4EjwO4ap7/cCvwfsBboaa3dbq1daIx5HPgbkALMBR4IR+MSewJN\n0XNG9SY9NcmBbkRE3CGY07O2ASc7rLbZCcs/ATzRwL4kjmzZ7+HWeV/61XUJUBERXUJUHBZoin4h\ntwcdWjRxoBsREfdRUIsjdh88wpjX1/rVNUWLiPhSUEvEBZqin7y8O1ltUh3oRkTE3RTUEjEHDpeT\nO+sLv7qmaBGR2imoJSJyZ33BgcPlPrVHhnbj3My0Wl4hIiKgoJYw83grGDFzlV9dU7SISHAU1BI2\nd7y9nvV7ynxqE3/SiQvPau1QRyIi0UdBLSFXXmm55IUVfnVN0SIi9aeglpCa8uEWPt7ynU/tlvPa\nc3V2W4c6EhGJbgpqCQlrLUNnaIoWEQk1BbU02OIN+/jDx9t9alf1Pp1bf9jBoY5ERGKHgloaJNDF\nSxbenEOCOdnl4UVEJFgKajklS7Yd4IG/bPap3fqDDK46p41DHYmIxCYFtdRboCl6wbgcEhM0RYuI\nhJqCWoK2atchJhX5hvGoPm256dz2DnUkIhL7FNQSlEBT9Htj+9C4UYID3YiIxA8FtZzUxr1l/PKt\n9T61i7unc9fAjg51JCISXxTUUqvhL6zAW2l9ag/1sZx/rkJaRCRStN1S/Ow8cIQh05f7hPQPMtNY\nPL4vyYkONiYiEoc0UYuPG19fw66DR31qb9zQm5YpSQ51JCIS3xTUAsC+Ui+j5qz2qZ2ZnsLTI7Mc\n6khEREBBLcBd721g9e5Sn9qs63rRplljhzoSEZHjFNRxrORwOVfP+sKn1rJJI94YfY5DHYmIyIkU\n1HHqkQ+38PcTbkc5/aoedGzVxKGOREQkEAV1nPF4Kxgxc5VfXbejFBFxJwV1HHnqk2LeXrvHp/bk\n5d3JapPqUEciIlIXBXUc8FZUMvzFlX51TdEiIu6noI5xs5bv5uVlu3xqU4d1o19GmkMdiYhIfSio\nY1SltVw8Y4VfXVO0iEh0UVDHoPfW7eXJf+7wqU0e3JmBXVo51JGIiJwqXes7CiQVFpKWnU3L9HTS\nsrNJKiwMuJy1liHTl/uF9KKbcxTSIiJRSkHtckmFhaTm5ZFYXIyxlsTiYlLz8vzC+qNN3zL0hE3d\nd/5HJovH98UYE8mWJY4F+6FSRIKnTd8ul1JQgPF4fGrG4yGloABvbi4AQ6Yv93vdwptzSFBASwQd\n/1B5/Of1+IfKUqj+WRWR+tNE7XIJO3fWWl9aXOIX0jcNaMfi8X0V0hJxJ/tQKSKnThO1y1VmZJBY\nXOxXP++xv8LCTT61D8bl0ChBAS3OONmHShE5dZqoXc6Tn49NSal+/EXHHpz7+Ic+y4zsfTqLx/dV\nSIujKjMy6lUXkeBoonY5b24upVRtVvzhnTP9nn9nbB+aNNLnLXGeJz/fZx81gE1JwZOf72BXItFP\nv+GjwFcXXuoX0j/t1orF4/sqpMNMRzEHz5ubS+m0aVR06IA1hooOHSidNk0Hkok0kCZql3tg8WaW\nbD/gU5v783Nonqx/unDTUcz1583N1boRCTGNYy61v8zLkOnLfUK6T7tmLB7fVyEdITqKWUTcQL/x\nXei/Pt7Gog37fWpvjj6HtCb654okHcUsIm6g3/wucvBIOVe98oVPbfCZrbh3UGdnGopztZ0ap6OY\nRSSSFNQuMf2znbyx6t8+tTmjepOemuRQR6KjmEXEDRTUDvN4Kxgxc5VPbUCH5jx68ZkOdSTH1Tw1\nLmHnTiozMvDk5+tgKRGJKAW1gz7Z9h0P/mWLT+2la3rSPi3ZoY7kRDqKWUScpqB2QEWlZczra9hT\n6q2unZmewtMjsxzsSkRE3EhBHWHLiku474RrdE+/qgcdWzVxqCMREXEznUcdIZXWctu8L31CetjZ\n6Swe3zcmQlpX8BIRCQ9N1BGw5ptD/Prdr3xqL+T2oEOL6A9o0BW8RETCSUEdRtZa7nl/I6t2H6qu\n/bhLS/IHd3Gwq9A72RW8FNQiIg2joA6TTfvKuH3+ep/aMyOz6JqeUssropeu4CUiEj4K6jB46C+b\n+ee276/R3S+jOb+/uBvGxOb9onUFLxGR8FFQh9D27w4z/s11PrX/vrw7PdqkOtRRZOgKXiIi4aOg\nDpETb6TR/bSmPDmiOwkxOkXXpCt4iYiEj4K6gXYfPMKY19f61B675Ez6tm/uUEfO0BW8RETCI6jz\nqI0xvzLGLDXGHDHGvHSS5cYaYyqMMYdqfA0KVbNu88ynxT4h3a55YxaMy4m7kBYRkfAJdqL+GpgC\nDAXqOmx5ibX2ggZ15XL7yryMmr3ap/bQRV05v1MLhzoSEZFYFVRQW2vnARhjBgAdwtqRy71StItX\ninZXP27WOJHXbuhN40Rd5E1EREIvHPuo+xpj9gL7gVeA31try8PwPhFVcricq2d94VO776ed+Wm3\nVs40JCIiccFYa4Nf2JgpQAdr7dhanu8KWGAb0At4HXjFWvv7AMveAtwCMHfu3P4tWtRvs3FZWRlN\nmzat12tO1T/+De8V+x69/XCOpXEcDtGRXO/yPa13Z2i9OyOO1vuywYMHD6hroZBO1NbazTUefmGM\neRi4B/ALamvtc8BzAEuWLLFZWfW7xWNRURH9+vVrQLd1Kz1awciXV/nU/t8FmQzPOi2s7+tmkVjv\n4k/r3Rla786Il/VeVFQU1HLhPj3LAlF5IvGC9fv44/9t96nNH5NNauNEhzoSEZF4FFRQG2MaHVs2\nEUg0xjQByk/c92yMGQYUWWu/McZkAflAVN3v8HB5JVfMXElljT0Ct5zXnquz2zrXlIiIxK1g97JO\nBjzAJGD0sT9PNsZ0PHaudMdjyw0GVhljSoEPgHnAoyHuOWz+vvlbLn/JN6QLR5+jkBYREccEe3rW\ng8CDtTzdrMZydwN3N7irCPNWVDJq9mpKjlRU10b3PYMx/ds52JWIiIguIcq/th8gf/Fmn9qcUb1J\nT01yqCMREZHvxW1QV1Raxr+5jp0lR6prV/Q6nV+eH9fXcxEREZeJy6Be+fVB7vlgo09t5rU9adc8\n2aGOREREAouroK60lv/3zgbW7ymrrl14Vmsm/qSTg12JiIjULm6Cev2eUu54e4NP7fmrsujUqq57\njIiIiDgn5oPaWsvvFm1iafHB6tr5HVvw4EVdMCYqr8UiIiJxJKaDest+D7fO+9Kn9j9XnE330+Li\nGrIiIhIDYjaoH/3fLXy0+bvqx73PSOW/hp+lKVpERKJKzAV12dEKrjjhRhpPXHoWvc9oVssrRERE\n3Cvmgvp/N31b/edOrZrwzMgsEhM0RYuISHSKuaD+j84t2FfmpVfbVAZ0SHO6HRERkQaJuaBulZLE\njbpGt4iIxIhg754lIiIiDlBQi4iIuJiCWkRExMUU1CIiIi6moBYREXExBbWIiIiLKagdlFRYSFp2\nNi3T00nLziapsNDplkRExGVi7jzqaJFUWEhqXh7G4wEgsbiY1Lw8SgFvbq6zzYmIiGtoonZISkFB\ndUgfZzweUgoKHOpIRETcSEHtkISdO+tVFxGR+KSgdkhlRka96iIiEp8U1A7x5OdjU1J8ajYlBU9+\nvkMdiYiIGymoHeLNzaV02jQqOnTAGkNFhw6UTpumA8lERMSHjvp2kDc3V8EsIiInpYlaRETExRTU\nIiIiLhZTQa0rfYmISKyJmX3UutKXiIjEopiZqHWlLxERiUUxE9S60peIiMSimAlqXelLRERiUcwE\nta70JSIisShmglpX+hIRkVgUM0d9g670JSIisSdmJmoREZFYpKAWERFxMQW1iIiIiymoRUREXExB\nLSIi4mIKahERERdTUIuIiLiYglpERMTFFNQiIiIupqAWERFxMWOtdboHPvzwwz3Atvq8Zv/+/ae1\nbt16b5haklpovTtD690ZWu/OiKP13mnw4MGn17WQK4L6VBhjllprBzjdR7zReneG1rsztN6dofXu\nS5u+RUREXExBLSIi4mLRHNTPOd1AnNJ6d4bWuzO03p2h9V5D1O6jFhERiQfRPFGLiIjEPAW1iIiI\ni0VVUBtjko0xM4wx24wxB40xK4wxw5zuK14YY84yxhw2xsxyupd4Yoy5zhizzhhTaozZZIz5sdM9\nxTpjTGdjzAfGmG+NMbuNMf9jjGnkdF+xxhjzK2PMUmPMEWPMSyc8N9gY86UxpswY8zdjTCeH2nRc\nVAU10AjYAfwEaAFMBt4wxnR2sKd48hTwudNNxBNjzEXAY8BNQHNgILDZ0abiw9PAHqAdkEPV75xf\nOtpRbPoamAK8ULNojDkNmAfkA62BpcDrEe/OJaLqE6K1thR4sEbpPWPMFqA/sNWJnuKFMeY64Dvg\nE+BMh9uJJw8BD1trPz32eKeTzcSRLsD/WGsPA7uNMQuBXg73FHOstfMAjDEDgA41nroSWGOtLTz2\n/IPAXmNMlrX2y4g36rBom6h9GGPaAt2BNU73EsuMMWnAw8BdTvcST4wxicAA4HRjzEZjTPGxTbAp\nTvcWB6YB1xpjmhpjMoBhwEKHe4onvYCVxx8cG9I2EqcflqI2qI0xScCrwMx4/IQVYQXADGttsdON\nxJm2QBJwNfBjqjbB9qVql4+E18dAb6AEKKZq0+tbjnYUX5oBB06olVC1+yfuRGVQG2MSgFeAo8Cv\nHG4nphljcoALgT863Usc8hz775+stbustXuBJ4BLHOwp5h37/bKQqn2kqcBpQCuqjhWQyDgEpJ1Q\nawEcdKAXx0VdUBtjDDCDqmnjKmut1+GWYt0goDOw3RizG7gbuMoYU+RkU/HAWvstVdNczasS6QpF\n4dca6EjVPuoj1tp9wIvoA1IkrQH6HH9gjEkFuhGnuzmjLqiBPwM9gMustZ66FpYGe46q/0Fyjn09\nA7wPDHWyqTjyInCHMaaNMaYV8GvgPYd7imnHtlxsAW4zxjQyxrQEbgRWOdtZ7Dm2fpsAiUCiMabJ\nsdPg5gO9jTFXHXv+AWBlvO7mjKqgPnYe3a1UBcZuY8yhY183ONxazLLWlllrdx//omqT1GFr7R6n\ne4sTBVSdErcBWAcsBx5xtKP4cCVVB5DtoeogJi9VH5IktCZTtYtnEjD62J8nH/v9chVVP+vfAucB\n1znVpNN0rW8REREXi6qJWkREJN4oqEVERFxMQS0iIuJiCmoREREXU1CLiIi4mIJaRETExRTUIiIi\nLqagFhERcTEFtYiIiIv9fzb2eoJZR0WBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x117b2c748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# change default figure and font size\n",
    "plt.rcParams['figure.figsize'] = 8, 6 \n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Fit on all the training data\n",
    "    for epoch in range(n_epochs):\n",
    "        sess.run(train, feed_dict = {X: X_train, Y: y_train})\n",
    "\n",
    "        # Display logs per epoch step\n",
    "        if (epoch + 1) % display_epoch == 0:\n",
    "            # run the cost to obtain the value for the cost function at each step\n",
    "            c = sess.run(cost, feed_dict = feed_dict)\n",
    "            print(\"Epoch: {}, cost: {}\".format(epoch + 1, c))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "    c = sess.run(cost, feed_dict = feed_dict)\n",
    "    weight = sess.run(W)\n",
    "    bias = sess.run(b)\n",
    "    print(\"Training cost: {}, W: {}, b: {}\".format(c, weight, bias))\n",
    "\n",
    "    # graphic display\n",
    "    plt.plot(X_train, y_train, 'ro', label = 'Original data')\n",
    "    plt.plot(X_train, weight * X_train + bias, label = 'Fitted line')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Using Softmax\n",
    "\n",
    "MNIST is a simple computer vision dataset. It consists of images of handwritten digits like these:\n",
    "\n",
    "<img src='images/mnist.png'>\n",
    "\n",
    "Each image is 28 pixels by 28 pixels, which is essentially a $28 \\times 28$ array of numbers. To use it in a context of a machine learning problem, we can flatten this array into a vector of $28 \\times 28 = 784$, this will be the number of features for each image. It doesn't matter how we flatten the array, as long as we're consistent between images. Note that, flattening the data throws away information about the 2D structure of the image. Isn't that bad? Well, the best computer vision methods do exploit this structure. But the simple method we will be using here, a softmax regression (defined below), won't.\n",
    "\n",
    "The dataset also includes labels for each image, telling us the each image's label. For example, the labels for the above images are 5, 0, 4, and 1. Here we're going to train a softmax model to look at images and predict what digits they are. The possible label values in the MNIST dataset are numbers between 0 and 9, hence this will be a 10-class classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# convenient one-liner to load the dataset\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The downloaded data is split into three parts, 55,000 data points of training data (mnist.train), 10,000 points of test data (mnist.test), and 5,000 points of validation data (mnist.validation).\n",
    "\n",
    "Every part of the dataset contains the data and label and we can access them via `.images` and `.labels`. e.g. the training images are mnist.train.images and the train labels are mnist.train.labels (one-hot encoded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pixels \n",
    "print(mnist.train.images.shape)\n",
    "mnist.train.images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  1.,  0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot encoded labels\n",
    "print(mnist.train.labels.shape)\n",
    "mnist.train.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code chunk, we define the overall computational graph/structure for the softmax classifier using the cross entropy cost function as the objective. Recall that the formula for this function can be denoted as:\n",
    "\n",
    "$$L = -\\sum_i y'_i \\log(y_i)$$\n",
    "\n",
    "Where y is our predicted probability distribution, and yâ€² is the true distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define some global variables\n",
    "learning_rate = 0.5 \n",
    "batch_size = 100\n",
    "n_iterations = 1000\n",
    "n_features = mnist.train.images.shape[1]\n",
    "n_class = mnist.train.labels.shape[1]\n",
    "\n",
    "# define the input and output \n",
    "# here None means that a dimension can be of any length,\n",
    "# which is what we want, since the number of observations\n",
    "# we have can vary;\n",
    "# note that the shape argument to placeholder is optional, \n",
    "# but it allows TensorFlow to automatically catch bugs stemming \n",
    "# from inconsistent tensor shapes\n",
    "X = tf.placeholder(tf.float32, [None, n_features])\n",
    "y = tf.placeholder(tf.float32, [None, n_class])\n",
    "\n",
    "# initialize both W and b as tensors full of zeros. \n",
    "# these are parameters that the model is later going to learn,\n",
    "# Notice that W has a shape of [784, 10] because we want to multiply \n",
    "# the 784-dimensional image vectors by it to produce 10-dimensional \n",
    "# vectors of evidence for the difference classes. b has a shape of [10] \n",
    "# so we can add it to the output.\n",
    "W = tf.Variable(tf.zeros([n_features, n_class]))\n",
    "b = tf.Variable(tf.zeros([n_class]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# to define the softmax classifier and cross entropy cost\n",
    "# we can do the following\n",
    "\n",
    "# matrix multiplication using the .matmul command\n",
    "# and add the softmax output\n",
    "output = tf.nn.softmax(tf.matmul(X, W) + b)\n",
    "\n",
    "# cost function: cross entropy, the reduce mean is simply the average of the\n",
    "# cost function across all observations\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y * tf.log(output), axis = 1))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# but for numerical stability reason, the tensorflow documentation\n",
    "# suggests using the following function\n",
    "output = tf.matmul(X, W) + b\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = y, logits = output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we defined the structure of our model, we'll:\n",
    "\n",
    "1. Define a optimization algorithm the train it. In this case, we ask TensorFlow to minimize our defined cross_entropy cost using the gradient descent algorithm with a learning rate of 0.5. There are also other off the shelf [optimizers](https://www.tensorflow.org/api_guides/python/train#optimizers) that we can use that are faster for more complex models.\n",
    "2. We'll also add an operation to initialize the variables we created\n",
    "3. Define helper \"function\" to evaluate the prediction accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# here we're return the predicted class of each observation using argmax\n",
    "# and see if the ouput (prediction) is equal to the target variable (y)\n",
    "# since equal is a boolean type tensor, we cast it to a float type to compute\n",
    "# the actual accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(y, axis = 1), tf.argmax(output, axis = 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to run it. During each step of the loop, we get a \"batch\" of one hundred random data points (defined by `batch_size`) from our training set. We run train_step feeding in the batches data to replace the placeholders.\n",
    "\n",
    "Using small batches of random data is called stochastic training -- in this case, stochastic gradient descent. Ideally, we'd like to use all our data for every step of training because that would give us a better sense of what we should be doing, but that's expensive. So, instead, we use a different subset every time. Doing this is cheap and has much of the same benefit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05\n",
      "0.89\n",
      "0.9\n",
      "0.91\n",
      "0.87\n",
      "0.93\n",
      "0.94\n",
      "0.9\n",
      "0.93\n",
      "0.92\n",
      "test: 0.9166\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess: \n",
    "    # initialize the variable, train the \"batch\" gradient descent\n",
    "    # for a specified number of iterations and evaluate on accuracy score\n",
    "    # remember the key to the feed_dict dictionary must match the variable we use\n",
    "    # as the placeholder for the data in the beginning\n",
    "    sess.run(init)\n",
    "    for i in range(n_iterations):\n",
    "        X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "        _, acc = sess.run([train_step, accuracy], feed_dict = {X: X_batch, y: y_batch})\n",
    "        \n",
    "        # simply prints the training data's accuracy for every 100 iteration\n",
    "        if i % 100 == 0:\n",
    "            print(acc)\n",
    "    \n",
    "    # after training evaluate the accuracy on the testing data\n",
    "    acc = sess.run(accuracy, feed_dict = {X: mnist.test.images, y: mnist.test.labels})\n",
    "    print('test:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we did not have to worry about computing the gradient to update the model, the nice thing about Tensorflow is that, once we've defined the structure of our model it has the capability to automatically differentiate mathematical expressions. This means we no longer need to compute the gradients ourselves! In this example, our softmax classifier obtained pretty nice result around 92%. But we can certainly do better with more advanced techniques such as convolutional deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "- [Blog: What is a TensorFlow Session?](http://danijar.com/what-is-a-tensorflow-session/)\n",
    "- [Github: Tensorflow Examples - Linear Regression](https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/2_BasicModels/linear_regression.ipynb)\n",
    "- [Tensorflow Documentation: Getting Started With TensorFlow](https://www.tensorflow.org/get_started/get_started)\n",
    "- [TensorFlow Documentation: MNIST For ML Beginners](https://www.tensorflow.org/get_started/mnist/beginners)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "169px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
