{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "@import url('http://fonts.googleapis.com/css?family=Source+Code+Pro');\n",
       "@import url('http://fonts.googleapis.com/css?family=Vollkorn');\n",
       "@import url('http://fonts.googleapis.com/css?family=Arimo');\n",
       "@import url('http://fonts.googleapis.com/css?family=Fira_sans');\n",
       "    \n",
       "    div.cell {\n",
       "        width: 1000px;\n",
       "        margin-left: 0% !important;\n",
       "        margin-right: auto;\n",
       "    }\n",
       "    div.text_cell code {\n",
       "        background: transparent;\n",
       "        color: #000000;\n",
       "        font-weight: 600;\n",
       "        font-size: 12pt;\n",
       "        font-style: bold;\n",
       "        font-family:  'Source Code Pro', Consolas, monocco, monospace;\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "\t}\n",
       "\t\n",
       "    div.input_area {\n",
       "        background: #F6F6F9;\n",
       "        border: 1px solid #586e75;\n",
       "    }\n",
       "\n",
       "    .text_cell_render h1 {\n",
       "        font-weight: 200;\n",
       "        font-size: 30pt;\n",
       "        line-height: 100%;\n",
       "        color:#c76c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 1em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    } \n",
       "    h2 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "        text-align: left;\n",
       "    }\n",
       "    .text_cell_render h2 {\n",
       "        font-weight: 200;\n",
       "        font-size: 16pt;\n",
       "        font-style: italic;\n",
       "        line-height: 100%;\n",
       "        color:#c76c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 1.5em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    } \n",
       "    h3 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h3 {\n",
       "        font-weight: 200;\n",
       "        font-size: 14pt;\n",
       "        line-height: 100%;\n",
       "        color:#d77c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 2em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    }\n",
       "    h4 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h4 {\n",
       "        font-weight: 100;\n",
       "        font-size: 14pt;\n",
       "        color:#d77c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "        white-space: nowrap;\n",
       "    }\n",
       "    h5 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 200;\n",
       "        font-style: normal;\n",
       "        color: #1d3b84;\n",
       "        font-size: 16pt;\n",
       "        margin-bottom: 0em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "        white-space: nowrap;\n",
       "    }\n",
       "    div.text_cell_render{\n",
       "        font-family: 'Fira sans', verdana,arial,sans-serif;\n",
       "        line-height: 125%;\n",
       "        font-size: 115%;\n",
       "        text-align:justify;\n",
       "        text-justify:inter-word;\n",
       "    }\n",
       "    div.output_wrapper{\n",
       "        margin-top:0.2em;\n",
       "        margin-bottom:0.2em;\n",
       "    }\n",
       "\n",
       "    code{\n",
       "      font-size: 70%;\n",
       "    }\n",
       "    .rendered_html code{\n",
       "    background-color: transparent;\n",
       "    }\n",
       "    ul{\n",
       "        margin: 2em;\n",
       "    }\n",
       "    ul li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.5em; \n",
       "    }\n",
       "    ul li li{\n",
       "        padding-left: 0.2em; \n",
       "        margin-bottom: 0.2em; \n",
       "        margin-top: 0.2em; \n",
       "    }\n",
       "    ol{\n",
       "        margin: 2em;\n",
       "    }\n",
       "    ol li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.5em; \n",
       "    }\n",
       "    ul li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.2em; \n",
       "    }\n",
       "    a:link{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    a:visited{\n",
       "       font-weight: bold;\n",
       "       color: #1d3b84;\n",
       "    }\n",
       "    a:hover{\n",
       "       font-weight: bold;\n",
       "       color: #1d3b84;\n",
       "    }\n",
       "    a:focus{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    a:active{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    .rendered_html :link {\n",
       "       text-decoration: underline; \n",
       "    }\n",
       "    .rendered_html :hover {\n",
       "       text-decoration: none; \n",
       "    }\n",
       "    .rendered_html :visited {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .rendered_html :focus {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .rendered_html :active {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "    } \n",
       "    hr {\n",
       "      color: #f3f3f3;\n",
       "      background-color: #f3f3f3;\n",
       "      height: 1px;\n",
       "    }\n",
       "    blockquote{\n",
       "      display:block;\n",
       "      background: #fcfcfc;\n",
       "      border-left: 5px solid #c76c0c;\n",
       "      font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "      width:680px;\n",
       "      padding: 10px 10px 10px 10px;\n",
       "      text-align:justify;\n",
       "      text-justify:inter-word;\n",
       "      }\n",
       "      blockquote p {\n",
       "        margin-bottom: 0;\n",
       "        line-height: 125%;\n",
       "        font-size: 100%;\n",
       "      }\n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    scale:100,\n",
       "                        availableFonts: [],\n",
       "                        preferredFont:null,\n",
       "                        webFont: \"TeX\",\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code for loading the format for the notebook\n",
    "import os\n",
    "\n",
    "# path : store the current path to convert back to it later\n",
    "path = os.getcwd()\n",
    "os.chdir( os.path.join('..', 'notebook_format') )\n",
    "from formats import load_style\n",
    "load_style(plot_style = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethen 2017-06-07 11:13:13 \n",
      "\n",
      "CPython 3.5.2\n",
      "IPython 5.3.0\n",
      "\n",
      "numpy 1.12.1\n",
      "pandas 0.19.2\n",
      "matplotlib 2.0.0\n",
      "pyspark 2.1.0+hadoop2.7\n"
     ]
    }
   ],
   "source": [
    "os.chdir(path)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create the SparkSession class,\n",
    "# which is the entry point into all functionality in Spark\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master('local[7]') # set it to run on 7 cores on local\n",
    "         .appName('Crime')\n",
    "         .config(conf = SparkConf())\n",
    "         .getOrCreate())\n",
    "\n",
    "# set the log level to ERROR to prevent \n",
    "# the terminal from showing too many information\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')\n",
    "\n",
    "# 1. magic for inline plot\n",
    "# 2. magic to print version\n",
    "# 3. magic so that the notebook will reload external python modules\n",
    "%matplotlib inline\n",
    "%load_ext watermark\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "%watermark -a 'Ethen' -d -t -v -p numpy,pandas,matplotlib,pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Exercise\n",
    "\n",
    "Please use the following [link](https://drive.google.com/file/d/0Bz1lG0yegjarWWRoYjI1SWtfUlU/view?usp=sharing) to download the dataset.\n",
    "\n",
    "## Question 1\n",
    "\n",
    "By using SparkSQL generate a bar chart of average crime events by month. Find an explanation of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ID='10078659', Case Number='HY267429', Date='05/19/2015 11:57:00 PM', Block='010XX E 79TH ST', IUCR='143A', Primary Type='WEAPONS VIOLATION', Description='UNLAWFUL POSS OF HANDGUN', Location Description='STREET', Arrest='true', Domestic='false', Beat='0624', District='006', Ward='8', Community Area='44', FBI Code='15', X Coordinate='1184626', Y Coordinate='1852799', Year='2015', Updated On='05/26/2015 12:42:06 PM', Latitude='41.751242944', Longitude='-87.599004724', Location='(41.751242944, -87.599004724)')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# have a peek at the data\n",
    "data_path = 'Crimes_-_2001_to_present.csv'\n",
    "df = spark.read.csv(data_path, sep = ',', header = True)\n",
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ID='10078659', Case Number='HY267429', Date='05/19/2015 11:57:00 PM', Block='010XX E 79TH ST', IUCR='143A', Primary Type='WEAPONS VIOLATION', Description='UNLAWFUL POSS OF HANDGUN', Location Description='STREET', Arrest='true', Domestic='false', Beat='0624', District='006', Ward='8', Community Area='44', FBI Code='15', X Coordinate='1184626', Y Coordinate='1852799', Year='2015', Updated On='05/26/2015 12:42:06 PM', Latitude='41.751242944', Longitude='-87.599004724', Location='(41.751242944, -87.599004724)', Day='05/19/2015', Month='05')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract the month and year from the date column\n",
    "split_col = F.split(df['Date'], ' ')\n",
    "df = df.withColumn('Day', split_col.getItem(0))\n",
    "split_col = F.split(df['Day'], '/')\n",
    "df = df.withColumn('Month', split_col.getItem(0))\n",
    "df = df.withColumn('Year', split_col.getItem(2))\n",
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01</td>\n",
       "      <td>30899.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02</td>\n",
       "      <td>27197.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>03</td>\n",
       "      <td>32860.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04</td>\n",
       "      <td>32948.733333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>05</td>\n",
       "      <td>34767.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>06</td>\n",
       "      <td>36050.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>07</td>\n",
       "      <td>37949.642857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>08</td>\n",
       "      <td>37470.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>09</td>\n",
       "      <td>35192.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>35715.571429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>32051.857143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>29979.571429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Month       Average\n",
       "0     01  30899.800000\n",
       "1     02  27197.133333\n",
       "2     03  32860.333333\n",
       "3     04  32948.733333\n",
       "4     05  34767.266667\n",
       "5     06  36050.428571\n",
       "6     07  37949.642857\n",
       "7     08  37470.857143\n",
       "8     09  35192.428571\n",
       "9     10  35715.571429\n",
       "10    11  32051.857143\n",
       "11    12  29979.571429"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# register the DataFrame as a SQL temporary view, so \n",
    "# we can directly use SQL query to interact with spark DataFrame\n",
    "df.createOrReplaceTempView('crime')\n",
    "sql_query = (\n",
    "    'SELECT Month, COUNT(*) / COUNT(DISTINCT Year) AS Average '\n",
    "    'FROM crime '\n",
    "    'GROUP BY Month '\n",
    "    'ORDER BY Month'\n",
    ")\n",
    "avg_month = spark.sql(sql_query).toPandas()\n",
    "avg_month.to_csv('avg_month.txt', index = False)\n",
    "avg_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf0AAAF5CAYAAACV7fNGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEmNMPQlSWqEoS9JUiP+fwkDvonrRMLiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111f51d68>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = 8, 6\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "plt.bar(avg_month['Month'].astype('float64'), avg_month['Average'])\n",
    "plt.title('Average Crime Count Throughout the Year')\n",
    "plt.xticks(range(1, 13))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judging from the plot below, we can see that the average crime count tends to be higher during the summer. The reason behind this phenomenon might be:\n",
    "\n",
    "- The elevated temperature makes people more irritable, which leads to a rise in aggressive behavior and crime.\n",
    "- During the summer, people tend to leave their windows open more often to cool down or spend more time outside to enjoy outdoor activities. Both of them gives burglars more opportunity to break into people's home."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "- Find the top 10 blocks in crime events in the last 3 years\n",
    "- Find the two beats that are adjacent with the highest correlation in the number of crime events (this will require you looking at the map to determine if the correlated beats are adjacent to each other) over the last 5 years\n",
    "- Determine if the number of crime events is different between Mayors Daly and Emanuel at a granularity of your choice (not only at the city level). Find an explanation of results. Side information: Rahm Emanuel is the current mayor of Chicago, and Richard Daley was his predecessor (left the office in 2011)\n",
    "\n",
    "The following section also contains a small example of how to use the `mapPartition` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1901', '52'), ('1902', '70')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# suppose our task if to find the maximum temperature of the year 1901, 1902\n",
    "temp = sc.parallelize(['1901,52', '1901,45', '1902,50', '1902,70', '2000,100'])\n",
    "\n",
    "# since map works with a single record, for each input there has to be an output\n",
    "# hence we have to write a separate map and filter function to filter out the record\n",
    "# that does not belong to 1901, 1902\n",
    "(temp\n",
    " .map(lambda x: x.split(','))\n",
    " .filter(lambda x: x[0] in {'1901', '1902'})\n",
    " .reduceByKey(lambda x, y: max(x, y))\n",
    " .collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1901', '52'), ('1902', '70')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_year(iterator):\n",
    "    # loop through each data point in\n",
    "    # the chunk of data and do whatever\n",
    "    for line in iterator:\n",
    "        year, temperature = line.split(',')\n",
    "        if year in {'1901', '1902'}:\n",
    "            yield year, temperature\n",
    "\n",
    "# by using mapPartition we can use a function to work \n",
    "# with each partition (i.e. chunk of data), thus we \n",
    "# can combine the map and filter logic within each partition\n",
    "(temp\n",
    " .mapPartitions(filter_year)\n",
    " .reduceByKey(lambda x, y: max(x, y))\n",
    " .collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2013"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract the year from the date column\n",
    "data_path = 'Crimes_-_2001_to_present.csv'\n",
    "df = spark.read.csv(data_path, sep = ',', header = True)\n",
    "\n",
    "split_col = F.split(df['Date'], ' ')\n",
    "df = df.withColumn('Day', split_col.getItem(0))\n",
    "split_col = F.split(df['Day'], '/')\n",
    "df = df.withColumn('Month', split_col.getItem(0).cast('int'))\n",
    "df = df.withColumn('Year', split_col.getItem(2).cast('int'))\n",
    "\n",
    "# obtain the starting year for the notion of last three year\n",
    "unique_years = (df\n",
    "                .select(df['Year'])\n",
    "                .distinct()\n",
    "                .orderBy('Year')\n",
    "                .rdd.map(lambda x: x.Year)\n",
    "                .collect())\n",
    "n_years = 3\n",
    "year_threshold = unique_years[-n_years]\n",
    "year_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Block</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001XX N STATE ST</td>\n",
       "      <td>1745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000X W TERMINAL ST</td>\n",
       "      <td>1340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>008XX N MICHIGAN AVE</td>\n",
       "      <td>1083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>076XX S CICERO AVE</td>\n",
       "      <td>1037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000X N STATE ST</td>\n",
       "      <td>794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>051XX W MADISON ST</td>\n",
       "      <td>661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>064XX S DR MARTIN LUTHER KING JR DR</td>\n",
       "      <td>628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>083XX S STEWART AVE</td>\n",
       "      <td>604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>046XX W NORTH AVE</td>\n",
       "      <td>571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>009XX W BELMONT AVE</td>\n",
       "      <td>550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Block  count\n",
       "0                     001XX N STATE ST   1745\n",
       "1                  0000X W TERMINAL ST   1340\n",
       "2                 008XX N MICHIGAN AVE   1083\n",
       "3                   076XX S CICERO AVE   1037\n",
       "4                     0000X N STATE ST    794\n",
       "5                   051XX W MADISON ST    661\n",
       "6  064XX S DR MARTIN LUTHER KING JR DR    628\n",
       "7                  083XX S STEWART AVE    604\n",
       "8                    046XX W NORTH AVE    571\n",
       "9                  009XX W BELMONT AVE    550"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 10 blocks in terms of crime events\n",
    "n_top = 10\n",
    "crime_per_block = (df\n",
    "                   .filter(df['Year'] >= year_threshold)\n",
    "                   .groupBy('Block')\n",
    "                   .count()\n",
    "                   .orderBy('count', ascending = False)\n",
    "                   .limit(n_top)\n",
    "                   .toPandas())\n",
    "crime_per_block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For part 1, the top 10 blocks in terms of number of crime events are places either in downtown Chicago or the South part of Chicago. This is probably due to the fact that those areas are densely populated, hence increases the chance of crime occurence.\n",
    "\n",
    "---\n",
    "\n",
    "For the problem of finding the two beats that are adjacent with the highest correlation in the number of crime events, I decided to use the [rdd `corr`](https://spark.apache.org/docs/2.1.0/mllib-statistics.html#correlations) function from the mllib package to perform the operation. Even thought there's a [DataFrame `corr`](http://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.DataFrame.corr) function, I couldn't find an easy way to utilize that API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('0111', 2011), 1040),\n",
       " (('0111', 2012), 1445),\n",
       " (('0111', 2013), 1644),\n",
       " (('0111', 2014), 1632),\n",
       " (('0111', 2015), 530),\n",
       " (('0112', 2011), 736),\n",
       " (('0112', 2012), 1471),\n",
       " (('0112', 2013), 1499),\n",
       " (('0112', 2014), 1343),\n",
       " (('0112', 2015), 449)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare correlation across years\n",
    "n_years = 5\n",
    "year_threshold = unique_years[-n_years]\n",
    "\n",
    "def filter_year(iterator):\n",
    "    for beat, year in iterator:\n",
    "        if year >= year_threshold:\n",
    "            yield (beat, year), 1\n",
    "\n",
    "# the sort by key is just to make the output\n",
    "# cleaner, we don't actually need it\n",
    "crime_per_group = (df\n",
    "                   .select(df['Beat'], df['Year'])\n",
    "                   .rdd.mapPartitions(filter_year)\n",
    "                   .reduceByKey(lambda x, y: x + y)\n",
    "                   .sortByKey())\n",
    "crime_per_group.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1613', '2011,517;2012,505;2013,431;2014,470;2015,166;'),\n",
       " ('1422', '2011,1535;2012,1465;2013,1249;2014,945;2015,302;'),\n",
       " ('1411', '2011,958;2012,936;2013,772;2014,721;2015,214;')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def beat2key(x):\n",
    "    (beat, year), count = x\n",
    "    # choose a random delimiter to separate the\n",
    "    # two values and the next value that will be\n",
    "    # concatenated to it\n",
    "    value = str(year) + ',' + str(count) + ';'\n",
    "    return beat, value\n",
    "\n",
    "# combine the count for each beat together\n",
    "# into one string value\n",
    "crime_per_beat = (crime_per_group\n",
    "                  .map(beat2key)\n",
    "                  .reduceByKey(lambda x, y: x + y))\n",
    "crime_per_beat.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2011',\n",
       "  '1613,517;1422,1535;1411,958;0934,1340;1021,1518;0234,1040;0232,1022;2431,684;0114,18;1522,2559;1531,1798;1922,634;0431,1595;1614,680;0411,1534;0612,2272;2023,645;2011,1027;1733,1132;0631,1942;1831,1630;1222,786;2515,1609;1914,14;0533,1442;0123,1266;0225,8;0935,1221;1933,1050;0222,925;1713,824;2422,1487;1112,2095;0333,1125;2433,1195;1233,851;1813,542;0614,1346;2535,1709;2212,1429;1223,1057;1913,793;0922,948;0824,1760;2412,1232;1113,1599;0911,1335;1915,16;0311,882;1022,1366;1224,917;0914,1571;2211,933;1634,1096;0433,1163;0531,1268;2031,554;1623,1055;2512,1681;1013,1268;0322,1504;0122,1949;1232,409;2012,552;0715,1121;1653,286;1214,5;2522,1219;0622,889;1711,850;1421,1114;1722,1178;1732,1258;0213,737;2513,678;0422,1386;0613,1548;1023,895;0725,1677;2525,922;1431,1011;0511,2594;1433,1365;0512,2071;0214,722;1131,913;1132,1714;0231,310;0814,881;1213,859;0132,2183;0221,505;1133,1689;0825,2499;1612,494;0313,1502;0925,652;1125,473;1414,784;1811,915;1215,2;1211,1470;2013,623;1225,6;0811,1158;1014,1268;1934,15;1121,1797;0734,1115;0815,1546;1231,795;1921,15;0623,1730;1011,2029;1423,1067;2234,1587;1911,523;1123,1236;0835,1669;0722,1059;1834,2213;0424,1814;2532,1783;1513,1175;0423,2834;0735,1753;1115,1344;2523,1113;0931,1051;1731,1303;2524,1095;0312,1840;2222,1371;0235,8;1511,1484;0334,1281;1832,1335;1434,1204;1723,1611;1114,998;0223,897;1512,1083;1822,870;2534,1762;1655,101;1412,1012;1935,13;0733,1398;0713,2036;2022,572;1821,1499;0314,1056;0813,1767;0133,587;1134,1529;0212,446;0834,1676;0421,3004;0913,1310;0321,2162;1532,1873;1432,1146;2024,752;0632,1835;0532,1319;1924,1270;0633,1000;1633,1316;1651,430;1632,857;2531,1480;0833,1818;2514,1375;0711,1453;0524,1600;1221,9;0624,2520;1712,1281;1652,181;1523,1794;1654,383;0233,1426;0921,656;1925,24;1124,1114;0111,1040;0723,1536;1135,1716;0611,1856;0732,1152;0121,12;1012,1007;0414,2383;1833,1798;0523,1881;2424,1068;0113,850;0923,1040;2423,810;1024,1238;2223,1470;0522,1987;1424,1377;0821,1203;1611,616;2233,1430;1923,1358;1235,11;1413,876;0324,1859;0513,1772;0224,722;2533,2286;1624,1022;2411,1141;0731,1209;1932,1157;2032,548;1631,749;1031,1449;0124,717;0924,1405;0634,1452;0832,2179;0432,1523;1912,694;0215,14;1122,1792;2521,1526;0131,1331;0323,1758;0932,1204;0812,847;1622,1238;0112,736;1812,959;1212,1250;2432,1066;2221,1104;2033,486;0831,2136;0434,828;0912,1787;1621,342;0714,1266;0621,2243;1034,1078;1033,1038;0211,919;1533,2309;0412,1709;0915,1281;1724,1007;2511,1025;0332,1747;0822,1599;1234,12;1111,1545;0726,1429;0933,990;1823,664;2232,1422;0413,1618;0331,1636;1931,937;0712,1904;1814,861;2413,1514;0823,2574;2213,911;1032,786;1824,1234;1524,1412;0724,1209;')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_count_per_year(iterator):\n",
    "    for beat, value in iterator:\n",
    "        # filter out the last ';'\n",
    "        year_and_count = value.split(';')[:-1]\n",
    "        if len(year_and_count) == n_years:\n",
    "            year_and_count = [tuple(e.split(',')) for e in year_and_count]\n",
    "            for year, count in year_and_count:\n",
    "                value = beat + ',' + count + ';'\n",
    "                yield year, value\n",
    "\n",
    "# convert the single string value into numeric value and\n",
    "# filter out any beat that does not have all the values,\n",
    "# i.e. does not have records for all the year;\n",
    "# also convert year as key since the correlation function\n",
    "# assumes that each column is a record as oppose to each row\n",
    "count_per_year = (crime_per_beat\n",
    "                  .mapPartitions(get_count_per_year)\n",
    "                  .reduceByKey(lambda x, y: x + y))\n",
    "count_per_year.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "274"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_unique_beats(count_per_year):\n",
    "    unique_beats = []\n",
    "    _, value = count_per_year\n",
    "    beat_and_count = value.split(';')[:-1]\n",
    "    beat_and_count = [tuple(e.split(',')) for e in beat_and_count]\n",
    "    beat_and_count = sorted(beat_and_count)\n",
    "    for beat, _ in beat_and_count:\n",
    "        unique_beats.append(beat)\n",
    "    \n",
    "    return unique_beats\n",
    "\n",
    "\n",
    "# load all the distinct beat into memory,\n",
    "# feasible since there's only around 300 of them\n",
    "unique_beats = get_unique_beats(count_per_year.first())\n",
    "len(unique_beats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.95677052,  0.897641  , ...,  0.7133157 ,\n",
       "         0.71141596,  0.54571851],\n",
       "       [ 0.95677052,  0.        ,  0.87074554, ...,  0.64021917,\n",
       "         0.6539807 ,  0.48406046],\n",
       "       [ 0.897641  ,  0.87074554,  0.        , ...,  0.93041864,\n",
       "         0.94033636,  0.84316419],\n",
       "       ..., \n",
       "       [ 0.7133157 ,  0.64021917,  0.93041864, ...,  0.        ,\n",
       "         0.99396707,  0.97564078],\n",
       "       [ 0.71141596,  0.6539807 ,  0.94033636, ...,  0.99396707,\n",
       "         0.        ,  0.97296516],\n",
       "       [ 0.54571851,  0.48406046,  0.84316419, ...,  0.97564078,\n",
       "         0.97296516,  0.        ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "def get_beat_count(x):\n",
    "    _, value = x\n",
    "    beat_and_count = value.split(';')[:-1]\n",
    "    beat_and_count = [tuple(e.split(',')) for e in beat_and_count]\n",
    "    beat_and_count = sorted(beat_and_count)\n",
    "    count = [int(count) for _, count in beat_and_count]\n",
    "    return count      \n",
    "\n",
    "# fill the diagonal with 0 instead of 1 so they will\n",
    "# not be considered when retrieving the largest correlation\n",
    "raw_count = count_per_year.map(get_beat_count)\n",
    "corr_m = Statistics.corr(raw_count, method = 'pearson')\n",
    "corr_m.flags['WRITEABLE'] = True\n",
    "np.fill_diagonal(corr_m, 0.)\n",
    "corr_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 41, 242, 162, 264,  80,  44, 122, 208,  51, 247, 252, 187, 159,\n",
       "        180, 169, 169, 204, 175,  37,  42,  45, 253,  41,  98, 199,  89,\n",
       "        226,  57, 212, 271, 164,  18,  60,  70, 173, 254,  40,  98, 254,\n",
       "        158,  65, 264,  42, 242, 262,  33, 133,  60, 205,  70,  99,  53,\n",
       "        190, 271,  87,  37, 207, 257,  53, 202, 240,  22, 260,  71,  36,\n",
       "        201,  35, 176, 249, 164,  44,  55,  41,  88, 273,  32, 254,  51,\n",
       "        267, 125,  56,  37, 128, 258, 264, 229, 164, 244,  88, 235, 232,\n",
       "        171, 256, 190,  27,  43, 207,  35, 139, 197]),\n",
       " array([182, 233,  86, 216, 142,  49,  35,  29,  30,  72, 176, 182, 210,\n",
       "        128, 215,  36, 201, 246, 105, 167,  88,  73, 187,  48, 233, 216,\n",
       "        231, 170,  52, 252, 234, 238,  92,  92, 205, 197,  47, 249, 118,\n",
       "         54,  68,  89, 262, 199, 202, 122, 134,  93,  94,  60, 189,  76,\n",
       "        179, 182,  27, 128, 124,  66, 125, 200,  87, 216, 256, 142, 215,\n",
       "        239,  33, 271,  48,  71,  32,  36, 271,  30, 204, 255,  63,  88,\n",
       "        170, 215, 180, 123,  56,  80,  22, 264,  51,  26, 164, 214,  22,\n",
       "         39, 253,  33,  53, 111, 250,  98,   6, 118]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatten the 2d correlation matrix and\n",
    "# sort it by decreasing order, then we grab\n",
    "# the top n elements;\n",
    "# since correlation matrix is symmetric,\n",
    "# we take the largest n * 2 and with a stride of 2\n",
    "n_largest = 100\n",
    "corr_result = np.ravel(corr_m)\n",
    "indices = np.argsort(corr_result)[::-1]\n",
    "largest_indices = indices[:(n_largest * 2):2]\n",
    "\n",
    "# obtain the corresponding index in the\n",
    "# original correlation matrix (e.g. element (3, 2)\n",
    "# contains the largest value thus it comes first)\n",
    "rows, cols = np.unravel_index(largest_indices, corr_m.shape)\n",
    "rows, cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After computing the year-level crime number correlation between all the pairwise beats combination and outputting the top 100 combination, the two adjacent beats that have the highest correlation is listed below (this is done by manually inspecting the top 100 pairs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beat1</th>\n",
       "      <th>beat2</th>\n",
       "      <th>corr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1121</td>\n",
       "      <td>1122</td>\n",
       "      <td>0.999749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   beat1 beat2      corr\n",
       "46  1121  1122  0.999749"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = []\n",
    "for i, j in zip(rows, cols):\n",
    "    result = [unique_beats[i], unique_beats[j], corr_m[i, j]]\n",
    "    results.append(result)\n",
    "\n",
    "df_corr = pd.DataFrame(results, columns = ['beat1', 'beat2', 'corr'])\n",
    "df_corr[ (df_corr['beat1'] == '1121') & (df_corr['beat2'] == '1122') ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For part 3, determining if the number of crime events is different between Mayors Daly and Emanuel, we will compute the monthly crime occurences per beat and perform a pair t-test to evaluate whether the difference is significant or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+------------+----------+-----+\n",
      "|Beat|Month|EmanuelCount|DaleyCount| Diff|\n",
      "+----+-----+------------+----------+-----+\n",
      "|0332|    4|         465|      1672|-1207|\n",
      "|0412|    4|         478|      1789|-1311|\n",
      "|0413|    8|         434|      1877|-1443|\n",
      "|0434|    7|         253|       859| -606|\n",
      "|0624|    6|         540|      2863|-2323|\n",
      "|1113|    1|         503|      1518|-1015|\n",
      "|1331|    8|          91|      1358|-1267|\n",
      "|1532|    8|         472|      2437|-1965|\n",
      "|1613|    9|         138|       611| -473|\n",
      "|1655|    5|          33|        16|   17|\n",
      "|1814|    7|         326|      1186| -860|\n",
      "|1834|    8|         582|      2654|-2072|\n",
      "|2031|    3|         144|       697| -553|\n",
      "|2211|    7|         240|      1255|-1015|\n",
      "|2423|   11|         144|      1005| -861|\n",
      "|2522|    4|         327|      1346|-1019|\n",
      "|2534|    8|         424|      2220|-1796|\n",
      "|0123|    2|         339|      1154| -815|\n",
      "|0212|    7|         312|      1355|-1043|\n",
      "|0214|    7|         222|       934| -712|\n",
      "+----+-----+------------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(data_path, sep = ',', header = True)\n",
    "split_col = F.split(df['Date'], ' ')\n",
    "df = df.withColumn('Day', split_col.getItem(0))\n",
    "split_col = F.split(df['Day'], '/')\n",
    "df = df.withColumn('Month', split_col.getItem(0).cast('int'))\n",
    "df = df.withColumn('Year', split_col.getItem(2).cast('int'))\n",
    "\n",
    "emanuel = (df\n",
    "           .filter(df['Year'] > 2011)\n",
    "           .groupBy('Beat', 'Month')\n",
    "           .count()\n",
    "           .withColumnRenamed('count', 'EmanuelCount'))\n",
    "\n",
    "daley = (df\n",
    "         .filter(df['Year'] <= 2011)\n",
    "         .groupBy('Beat', 'Month')\n",
    "         .count()\n",
    "         .withColumnRenamed('count', 'DaleyCount'))\n",
    "\n",
    "joined = (emanuel\n",
    "          .join(daley, on = ['Beat', 'Month'])\n",
    "          .withColumn('Diff', (F.col('EmanuelCount') - F.col('DaleyCount')).alias('double')))\n",
    "joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-1061.9955960070463, 512.3930046068413, 3406)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the pair t-test\n",
    "test_info = (joined\n",
    "             .select(F.mean(F.col('Diff')).alias('Mean'),\n",
    "                     F.stddev(F.col('Diff')).alias('Std'),\n",
    "                     F.count(F.col('Diff')).alias('Count'))\n",
    "             .rdd.map(lambda x: (x.Mean, x.Std, x.Count))\n",
    "             .collect())\n",
    "test_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-120.96001879348958"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the t value is extremely large,\n",
    "# indicating that there is a difference in crime\n",
    "mean, std, count = test_info[0]\n",
    "standard_error = std / np.sqrt(count)\n",
    "tvalue = mean / standard_error\n",
    "tvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe a large pair t-statistic when comparing monthly crime numbers between mayor Daley and Emanuel. This indicates the data provides strong evidence against the null hypothesis in which there is no difference in the number of crimes between the two mayors. And based on the way the number is calculated, a large negative number means there's significantly less number of crimes when Rahm Emanuel (the current mayor of Chicago) is mayor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Predict the number of crime events in the next week at the beat level.\n",
    "\n",
    "- Framed the problem as a supervised machine learning problem, thus lagged features were created. i.e. we use week 1, 2, 3's number to predict the number for week 4. We could have framed it as a time series problem, but here we're simply using it as a chance to familarize ourselves with Spark ML's API\n",
    "- Trained a RandomForest model for every beat level. This decision is due to the fact that if we were to only train one model, it would require us to one-hot encode around 300 different beat values, which is often times not ideal as a categorical variable with too many distinct levels often leads to overfitting\n",
    "- For the RandomForest model, 3-fold cross validation and a grid search on the `maxDepth` parameter was performed\n",
    "\n",
    "---\n",
    "\n",
    "A quick intro into the SQL window function:\n",
    "\n",
    "Window functions operate on a set of rows and return a single value for each row from the underlying query. When we use a window function in a query, we define the window using the `OVER()` clause, which has the folloiwing capabilities:\n",
    "\n",
    "- Defines window partitions to form groups of rows. (PARTITION BY clause)\n",
    "- Orders rows within a partition. (ORDER BY clause)\n",
    "\n",
    "e.g.\n",
    "\n",
    "The following query uses the `AVG()` window function with the `PARTITION BY` clause to determine the average car sales for each dealer in Q1:\n",
    "\n",
    "```sql\n",
    "SELECT \n",
    "    emp_name, dealer_id, sales, \n",
    "    AVG(sales) OVER (PARTITION BY dealer_id) AS avgsales \n",
    "FROM \n",
    "    q1_sales;\n",
    "```\n",
    "\n",
    "<img src=\"window_query.png\" width=\"40%\" height=\"40%\">\n",
    "\n",
    "For more information, including different types of window function the following link gives a very nice overview. [Drill Documentation: SQL Window Functions Introduction](https://drill.apache.org/docs/sql-window-functions-introduction/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|num|\n",
      "+---+---+\n",
      "|  4|9.0|\n",
      "|  3|7.0|\n",
      "|  2|3.0|\n",
      "|  1|5.0|\n",
      "+---+---+\n",
      "\n",
      "+---+---+-------+\n",
      "| id|num|new_col|\n",
      "+---+---+-------+\n",
      "|  1|5.0|    3.0|\n",
      "|  2|3.0|    7.0|\n",
      "|  3|7.0|    9.0|\n",
      "+---+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Reference\n",
    "# ---------\n",
    "# https://stackoverflow.com/questions/34295642/spark-add-new-column-to-dataframe-with-value-from-previous-row\n",
    "d = sc.parallelize([(4, 9.0), (3, 7.0), (2, 3.0), (1, 5.0)]).toDF(['id', 'num'])\n",
    "d.show()\n",
    "\n",
    "# here we define a window to create a lagged feature,\n",
    "# so every row gets shifted up by 1 (determined by the\n",
    "# count parameter of the lag function); remember to drop\n",
    "# the na value since there's no record before id1\n",
    "w = Window().partitionBy().orderBy('id')\n",
    "d = (d\n",
    "     .withColumn('new_col', F.lag(F.col('num'), count = -1).over(w))\n",
    "     .na.drop())\n",
    "d.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ID='10078659', Case Number='HY267429', Date='05/19/2015 11:57:00 PM', Block='010XX E 79TH ST', IUCR='143A', Primary Type='WEAPONS VIOLATION', Description='UNLAWFUL POSS OF HANDGUN', Location Description='STREET', Arrest='true', Domestic='false', Beat='0624', District='006', Ward='8', Community Area='44', FBI Code='15', X Coordinate='1184626', Y Coordinate='1852799', Year='2015', Updated On='05/26/2015 12:42:06 PM', Latitude='41.751242944', Longitude='-87.599004724', Location='(41.751242944, -87.599004724)', DateTime=datetime.datetime(2015, 5, 19, 0, 0), WeekOfYear='21', Time='201521')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "data_path = 'Crimes_-_2001_to_present.csv'\n",
    "df = spark.read.csv(data_path, sep = ',', header = True)\n",
    "\n",
    "def date2weeknumber(date):\n",
    "    \"\"\"\n",
    "    append a 0 in front of single digit week number,\n",
    "    this is used when sorting the week number so that\n",
    "    week 02 will still come before week 10\n",
    "    \"\"\"\n",
    "    if len(date) == 2:\n",
    "        return date\n",
    "    else:\n",
    "        return '0' + date\n",
    "\n",
    "udf_date2weeknumber = F.udf(date2weeknumber, StringType())\n",
    "\n",
    "# convert the Date column to spark timestamp format to retrieve the time,\n",
    "# date formats follow the formats at java.text.SimpleDateFormat\n",
    "#\n",
    "# Reference\n",
    "# ---------\n",
    "# http://docs.oracle.com/javase/6/docs/api/java/text/SimpleDateFormat.html\n",
    "# https://stackoverflow.com/questions/25006607/how-to-get-day-of-week-in-sparksql\n",
    "timestamp_format = 'MM/dd/yyyy'\n",
    "crime = (df\n",
    "         .withColumn('DateTime', \n",
    "                     F.unix_timestamp(F.col('Date'), timestamp_format).cast('timestamp'))\n",
    "         .withColumn('WeekOfYear', F.weekofyear(F.col('DateTime')).cast('string'))\n",
    "         .withColumn('WeekOfYear', udf_date2weeknumber(F.col('WeekOfYear')))\n",
    "         .withColumn('Year', F.year(F.col('DateTime')).cast('string'))\n",
    "         .withColumn('Time', F.concat(F.col('Year'), F.col('WeekOfYear'))))\n",
    "crime.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----+\n",
      "|Beat|  Time|label|\n",
      "+----+------+-----+\n",
      "|0111|200101|   35|\n",
      "|0111|200102|   36|\n",
      "|0111|200103|   43|\n",
      "|0111|200104|   29|\n",
      "|0111|200105|   23|\n",
      "|0111|200106|   36|\n",
      "|0111|200107|   21|\n",
      "|0111|200108|   26|\n",
      "|0111|200109|   29|\n",
      "|0111|200110|   32|\n",
      "|0111|200111|   36|\n",
      "|0111|200112|   29|\n",
      "|0111|200113|   34|\n",
      "|0111|200114|   33|\n",
      "|0111|200115|   22|\n",
      "|0111|200116|   28|\n",
      "|0111|200117|   44|\n",
      "|0111|200118|   40|\n",
      "|0111|200119|   32|\n",
      "|0111|200120|   33|\n",
      "+----+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. don't really need orderBy, it's \n",
    "# just to make the output cleaner;\n",
    "# 2. rename the count column to label to\n",
    "# make it the label column for the modeling part;\n",
    "# 3. we will be using the crime count dataframe a lot\n",
    "# to perform the modeling, thus cache it to store it\n",
    "# in memory\n",
    "crime_count = (crime\n",
    "               .select('Beat', 'Time')\n",
    "               .groupby('Beat', 'Time')\n",
    "               .count()\n",
    "               .withColumnRenamed('count', 'label')\n",
    "               .orderBy('Beat', 'Time')\n",
    "               .cache())\n",
    "\n",
    "crime_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find all the distinct beat and train the model on each\n",
    "# beat to generate the prediction for next week\n",
    "beats = (crime_count\n",
    "         .select('Beat')\n",
    "         .distinct()\n",
    "         .rdd.map(lambda x: x.Beat)\n",
    "         .collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|  Time|label|\n",
      "+------+-----+\n",
      "|200101|   33|\n",
      "|200102|   24|\n",
      "|200103|   29|\n",
      "|200104|   22|\n",
      "|200105|   32|\n",
      "|200106|   35|\n",
      "|200107|   32|\n",
      "|200108|   38|\n",
      "|200109|   34|\n",
      "|200110|   40|\n",
      "|200111|   28|\n",
      "|200112|   41|\n",
      "|200113|   29|\n",
      "|200114|   27|\n",
      "|200115|   34|\n",
      "|200116|   34|\n",
      "|200117|   33|\n",
      "|200118|   45|\n",
      "|200119|   36|\n",
      "|200120|   38|\n",
      "+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# we will use one of the beat to demonstrate the modeling for now\n",
    "beat = '1011'\n",
    "subset = (crime_count\n",
    "          .filter(F.col('Beat') == beat)\n",
    "          .drop('Beat'))\n",
    "subset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_lagged_features(spark_df, window_col, feature_col, n_lags):\n",
    "    \"\"\"\n",
    "    Reference\n",
    "    ---------\n",
    "    https://stackoverflow.com/questions/34295642/spark-add-new-column-to-dataframe-with-value-from-previous-row\n",
    "    \"\"\"\n",
    "    w = Window().partitionBy().orderBy(window_col)\n",
    "    lagged = (subset\n",
    "              .withColumn('Lag1', F.lag(F.col(feature_col), count = 1).over(w))\n",
    "              .na.drop())\n",
    "    \n",
    "    for lag in range(1, n_lags):\n",
    "        previous_col = 'Lag' + str(lag)\n",
    "        current_col = 'Lag' + str(lag + 1)\n",
    "        lagged = (lagged\n",
    "                  .withColumn(current_col, F.lag(F.col(previous_col), count = 1).over(w))\n",
    "                  .na.drop())\n",
    "    \n",
    "    # after creating the lag sort the time by descending order\n",
    "    # so it will be easier to project to the future\n",
    "    lagged = lagged.orderBy(window_col, ascending = False)\n",
    "    return lagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----+----+----+----+\n",
      "|  Time|label|Lag1|Lag2|Lag3|Lag4|\n",
      "+------+-----+----+----+----+----+\n",
      "|201521|    8|  28|  43|  26|  26|\n",
      "|201520|   28|  43|  26|  26|  24|\n",
      "|201519|   43|  26|  26|  24|  34|\n",
      "|201518|   26|  26|  24|  34|  42|\n",
      "|201517|   26|  24|  34|  42|  25|\n",
      "|201516|   24|  34|  42|  25|  24|\n",
      "|201515|   34|  42|  25|  24|  33|\n",
      "|201514|   42|  25|  24|  33|  28|\n",
      "|201513|   25|  24|  33|  28|  29|\n",
      "|201512|   24|  33|  28|  29|  26|\n",
      "|201511|   33|  28|  29|  26|  26|\n",
      "|201510|   28|  29|  26|  26|  29|\n",
      "|201509|   29|  26|  26|  29|  36|\n",
      "|201508|   26|  26|  29|  36|  31|\n",
      "|201507|   26|  29|  36|  31|  33|\n",
      "|201506|   29|  36|  31|  33|  18|\n",
      "|201505|   36|  31|  33|  18|   8|\n",
      "|201504|   31|  33|  18|   8|  30|\n",
      "|201503|   33|  18|   8|  30|  26|\n",
      "|201502|   18|   8|  30|  26|  18|\n",
      "+------+-----+----+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# after creating the lagged features\n",
    "n_lags = 4\n",
    "window_col = 'Time'\n",
    "feature_col = 'label'\n",
    "lagged = create_lagged_features(subset, window_col, feature_col, n_lags)\n",
    "lagged.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+----+----+----+----+--------------------+------------------+\n",
      "|  Time|label|Lag1|Lag2|Lag3|Lag4|            features|        prediction|\n",
      "+------+-----+----+----+----+----+--------------------+------------------+\n",
      "|201521|    8|  28|  43|  26|  26|[28.0,43.0,26.0,2...|29.950242674471788|\n",
      "|201520|   28|  43|  26|  26|  24|[43.0,26.0,26.0,2...|34.118871737062484|\n",
      "|201519|   43|  26|  26|  24|  34|[26.0,26.0,24.0,3...| 30.41978283956728|\n",
      "|201518|   26|  26|  24|  34|  42|[26.0,24.0,34.0,4...| 32.81236669210086|\n",
      "|201517|   26|  24|  34|  42|  25|[24.0,34.0,42.0,2...|33.977122759927475|\n",
      "|201516|   24|  34|  42|  25|  24|[34.0,42.0,25.0,2...|34.202792880152955|\n",
      "|201515|   34|  42|  25|  24|  33|[42.0,25.0,24.0,3...| 34.79527829400224|\n",
      "|201514|   42|  25|  24|  33|  28|[25.0,24.0,33.0,2...| 32.58155857396165|\n",
      "|201513|   25|  24|  33|  28|  29|[24.0,33.0,28.0,2...|31.463988486746636|\n",
      "|201512|   24|  33|  28|  29|  26|[33.0,28.0,29.0,2...|31.617519649490948|\n",
      "|201511|   33|  28|  29|  26|  26|[28.0,29.0,26.0,2...|30.147351661915213|\n",
      "|201510|   28|  29|  26|  26|  29|[29.0,26.0,26.0,2...|30.898887698977568|\n",
      "|201509|   29|  26|  26|  29|  36|[26.0,26.0,29.0,3...|30.076384332324903|\n",
      "|201508|   26|  26|  29|  36|  31|[26.0,29.0,36.0,3...| 32.63119945556682|\n",
      "|201507|   26|  29|  36|  31|  33|[29.0,36.0,31.0,3...| 30.92928907884028|\n",
      "|201506|   29|  36|  31|  33|  18|[36.0,31.0,33.0,1...|  37.7631490819411|\n",
      "|201505|   36|  31|  33|  18|   8|[31.0,33.0,18.0,8.0]| 33.03621523723659|\n",
      "|201504|   31|  33|  18|   8|  30|[33.0,18.0,8.0,30.0]| 32.69072976717794|\n",
      "|201503|   33|  18|   8|  30|  26|[18.0,8.0,30.0,26.0]| 33.36725331611321|\n",
      "|201502|   18|   8|  30|  26|  18|[8.0,30.0,26.0,18.0]|30.628335083703725|\n",
      "+------+-----+----+----+----+----+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# assemble all the columns into one single 'features' column and train\n",
    "# a randomforest model (includes cross validation and grid search on the\n",
    "# maxDepth parameter); we will use r squared to pick to best model; when\n",
    "# having a multi-step process, the recommended way is to define every\n",
    "# stage inside a pipeline\n",
    "input_cols = ['Lag' + str(j) for j in range(1, n_lags + 1)]\n",
    "assembler = VectorAssembler(inputCols = input_cols, outputCol = 'features')\n",
    "rf = RandomForestRegressor(numTrees = 30)\n",
    "stages = [assembler, rf]\n",
    "pipeline = Pipeline(stages = stages)\n",
    "param_grid = ParamGridBuilder().addGrid(rf.maxDepth, [5, 6, 7]).build()\n",
    "evaluator = RegressionEvaluator(labelCol = 'label', \n",
    "                                predictionCol = 'prediction', \n",
    "                                metricName = 'r2')\n",
    "rf_grid = CrossValidator(\n",
    "    estimator = pipeline,\n",
    "    estimatorParamMaps = param_grid,\n",
    "    evaluator = evaluator,\n",
    "    numFolds = 3\n",
    ").fit(lagged)\n",
    "lagged_fitted = rf_grid.transform(lagged)\n",
    "lagged_fitted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38731677679537"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metric = evaluator.evaluate(lagged_fitted)\n",
    "eval_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+----+----+------------------+--------------------+-----------------+\n",
      "|  Time|Lag2|Lag3|Lag4|              Lag1|            features|       prediction|\n",
      "+------+----+----+----+------------------+--------------------+-----------------+\n",
      "|201521|  28|  43|  26|29.950242674471788|[29.9502426744717...|33.21558236005183|\n",
      "+------+----+----+----+------------------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to predict for next week, the prediction column will now become one of\n",
    "# the latestest lagged feature and everything will also get shifted down\n",
    "# by 1 time slot (i.e. prediction -> Lag1, Lag1 -> Lag2 and so on)\n",
    "next_week = lagged_fitted.limit(1).drop('features', 'label', 'Lag' + str(n_lags))\n",
    "\n",
    "for lag in reversed(range(1, n_lags)):\n",
    "    current_col = 'Lag' + str(lag)\n",
    "    next_col = 'Lag' + str(lag + 1)\n",
    "    next_week = next_week.withColumnRenamed(current_col, next_col)\n",
    "    \n",
    "next_week = next_week.withColumnRenamed('prediction', 'Lag1')\n",
    "\n",
    "next_week_pred = rf_grid.transform(next_week)\n",
    "prediction = (next_week_pred\n",
    "              .rdd.map(lambda x: x.prediction)\n",
    "              .collect()[0])\n",
    "next_week_pred.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section ties everything together: loop through all the beats, create the lagged feature, make the prediction for the next week, report the prediction and the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:17<00:00,  8.96s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beat</th>\n",
       "      <th>prediction</th>\n",
       "      <th>r2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1011</td>\n",
       "      <td>33.215582</td>\n",
       "      <td>0.387317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2033</td>\n",
       "      <td>7.776790</td>\n",
       "      <td>0.597345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   beat  prediction        r2\n",
       "0  1011   33.215582  0.387317\n",
       "1  2033    7.776790  0.597345"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# switch to loop over the unqiue_beats variable to predict on all the beats\n",
    "results = []\n",
    "for beat in tqdm(['1011', '2033']):\n",
    "    subset = (crime_count\n",
    "              .filter(F.col('Beat') == beat)\n",
    "              .drop('Beat'))\n",
    "    \n",
    "    # some beats might not have enough records,\n",
    "    # here we simply leave them out\n",
    "    if subset.count() > 30:\n",
    "\n",
    "        # create lagged column for 4 time period\n",
    "        # e.g. use week 1, 2, 3's number to predict the\n",
    "        # number for week 4\n",
    "        n_lags = 4\n",
    "        window_col = 'Time'\n",
    "        feature_col = 'label'\n",
    "        lagged = create_lagged_features(subset, window_col, feature_col, n_lags)\n",
    "\n",
    "        # tune the max depth of a random forest with a 5 fold cross validation\n",
    "        input_cols = ['Lag' + str(j) for j in range(1, n_lags + 1)]\n",
    "        assembler = VectorAssembler(inputCols = input_cols, outputCol = 'features')\n",
    "        rf = RandomForestRegressor(numTrees = 30)\n",
    "        stages = [assembler, rf]\n",
    "        pipeline = Pipeline(stages = stages)\n",
    "        param_grid = ParamGridBuilder().addGrid(rf.maxDepth, [5, 6, 7]).build()\n",
    "        evaluator = RegressionEvaluator(labelCol = 'label', \n",
    "                                        predictionCol = 'prediction', \n",
    "                                        metricName = 'r2')\n",
    "        rf_grid = CrossValidator(\n",
    "            estimator = pipeline,\n",
    "            estimatorParamMaps = param_grid,\n",
    "            evaluator = evaluator,\n",
    "            numFolds = 3\n",
    "        ).fit(lagged)\n",
    "        lagged_fitted = rf_grid.transform(lagged)\n",
    "\n",
    "        # transform the data to perform the prediction for the next week\n",
    "        next_week = (lagged_fitted\n",
    "                     .limit(1)\n",
    "                     .drop('features', 'label', 'Lag' + str(n_lags)))\n",
    "\n",
    "        for lag in reversed(range(1, n_lags)):\n",
    "            current_col = 'Lag' + str(lag)\n",
    "            next_col = 'Lag' + str(lag + 1)\n",
    "            next_week = next_week.withColumnRenamed(current_col, next_col)\n",
    "\n",
    "        next_week = next_week.withColumnRenamed('prediction', 'Lag1')\n",
    "\n",
    "        next_week_pred = rf_grid.transform(next_week)\n",
    "        prediction = (next_week_pred\n",
    "                      .rdd.map(lambda x: x.prediction)\n",
    "                      .collect()[0])\n",
    "\n",
    "        eval_metric = evaluator.evaluate(lagged_fitted)\n",
    "\n",
    "        result = beat, prediction, eval_metric\n",
    "        results.append(result)\n",
    "    \n",
    "df_results = pd.DataFrame(results, columns = ['beat', 'prediction', 'r2'])\n",
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "Find patterns of crimes with arrest with respect to time of the day, day of the week, and month.\n",
    "\n",
    "Base on the results below, we see crimes mostly happen during the evening, the summer and on weekdays. The rationale behind why summer time has the most crimes was already mentioned in question 1 and the fact that most crime happens during the evening is not that surprising as well as that's when there are less people on the street to help you and the lack of sunlight also gives the criminial an advantage (i.e. you might not notice them approaching or remember how their face looks like).\n",
    "\n",
    "As for why there is a huge drop in the number of crimes during Sunday, one possible reason is people tend to spend the evening at home to get ready for Monday. Thus there's less chance of being a subject of criminal activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ID='10078659', Case Number='HY267429', Date='05/19/2015 11:57:00 PM', Block='010XX E 79TH ST', IUCR='143A', Primary Type='WEAPONS VIOLATION', Description='UNLAWFUL POSS OF HANDGUN', Location Description='STREET', Arrest=True, Domestic='false', Beat='0624', District='006', Ward='8', Community Area='44', FBI Code='15', X Coordinate='1184626', Y Coordinate='1852799', Year='2015', Updated On='05/26/2015 12:42:06 PM', Latitude='41.751242944', Longitude='-87.599004724', Location='(41.751242944, -87.599004724)', DateTime=datetime.datetime(2015, 5, 19, 11, 57))]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = 'Crimes_-_2001_to_present.csv'\n",
    "df = spark.read.csv(data_path, sep = ',', header = True)\n",
    "\n",
    "# convert the Date column to spark timestamp format to retrieve the time,\n",
    "# date formats follow the formats at java.text.SimpleDateFormat\n",
    "#\n",
    "# Reference\n",
    "# ---------\n",
    "# http://docs.oracle.com/javase/6/docs/api/java/text/SimpleDateFormat.html\n",
    "# https://stackoverflow.com/questions/25006607/how-to-get-day-of-week-in-sparksql\n",
    "timestamp_format = 'MM/dd/yyyy HH:mm:ss aa'\n",
    "crime = (df\n",
    "         .withColumn('Arrest', df['Arrest'].cast('boolean'))\n",
    "         .filter(F.col('Arrest'))\n",
    "         .withColumn('DateTime', \n",
    "                     F.unix_timestamp(F.col('Date'), timestamp_format).cast('timestamp')))\n",
    "crime.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hour</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>143153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>130618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>118068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>102672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>92808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>108650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>140065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>156953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>165139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>175195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>178018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>162863</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Hour   count\n",
       "0      1  143153\n",
       "1      2  130618\n",
       "2      3  118068\n",
       "3      4  102672\n",
       "4      5   92808\n",
       "5      6  108650\n",
       "6      7  140065\n",
       "7      8  156953\n",
       "8      9  165139\n",
       "9     10  175195\n",
       "10    11  178018\n",
       "11    12  162863"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crime_per_hour = (crime\n",
    "                  .select(F.hour(F.col('DateTime')).alias('Hour'))\n",
    "                  .groupby('Hour')\n",
    "                  .count()\n",
    "                  .orderBy('Hour')\n",
    "                  .toPandas())\n",
    "crime_per_hour.to_csv('crime_per_hour.csv', index = False)\n",
    "crime_per_hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Day</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wednesday</td>\n",
       "      <td>250748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tuesday</td>\n",
       "      <td>247905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Friday</td>\n",
       "      <td>250676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thursday</td>\n",
       "      <td>246924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Saturday</td>\n",
       "      <td>234915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Monday</td>\n",
       "      <td>228115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sunday</td>\n",
       "      <td>214919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Day   count\n",
       "0  Wednesday  250748\n",
       "1    Tuesday  247905\n",
       "2     Friday  250676\n",
       "3   Thursday  246924\n",
       "4   Saturday  234915\n",
       "5     Monday  228115\n",
       "6     Sunday  214919"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'EEEE' stands for the full name of the weekday\n",
    "crime_per_day = (crime\n",
    "                 .select(F.date_format(F.col('DateTime'), 'EEEE').alias('Day'))\n",
    "                 .groupby('Day')\n",
    "                 .count()\n",
    "                 .toPandas())\n",
    "crime_per_day.to_csv('crime_per_day.csv', index = False)\n",
    "crime_per_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>142642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>131856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>150155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>143975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>147941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>139220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>144030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>146161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>139180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>140619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>129077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>119346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Month   count\n",
       "0       1  142642\n",
       "1       2  131856\n",
       "2       3  150155\n",
       "3       4  143975\n",
       "4       5  147941\n",
       "5       6  139220\n",
       "6       7  144030\n",
       "7       8  146161\n",
       "8       9  139180\n",
       "9      10  140619\n",
       "10     11  129077\n",
       "11     12  119346"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crime_per_month = (crime\n",
    "                   .select(F.month(F.col('DateTime')).alias('Month'))\n",
    "                   .groupby('Month')\n",
    "                   .count()\n",
    "                   .orderBy('Month')\n",
    "                   .toPandas())\n",
    "crime_per_month.to_csv('crime_per_month.csv', index = False)\n",
    "crime_per_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# terminate the spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "- [Drill Documentation: SQL Window Functions Introduction](https://drill.apache.org/docs/sql-window-functions-introduction/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "92px",
    "width": "275px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
