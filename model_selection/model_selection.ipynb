{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Model-Selection\" data-toc-modified-id=\"Model-Selection-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Model Selection</a></span><ul class=\"toc-item\"><li><span><a href=\"#K-Fold-Cross-Validation\" data-toc-modified-id=\"K-Fold-Cross-Validation-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>K-Fold Cross Validation</a></span></li><li><span><a href=\"#Grid-Search\" data-toc-modified-id=\"Grid-Search-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Grid Search</a></span></li><li><span><a href=\"#Random-Search\" data-toc-modified-id=\"Random-Search-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Random Search</a></span></li><li><span><a href=\"#Random-&amp;-Grid-Search-Comparison\" data-toc-modified-id=\"Random-&amp;-Grid-Search-Comparison-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Random &amp; Grid Search Comparison</a></span></li></ul></li><li><span><a href=\"#Reference\" data-toc-modified-id=\"Reference-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Reference</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "@import url('http://fonts.googleapis.com/css?family=Source+Code+Pro');\n",
       "@import url('http://fonts.googleapis.com/css?family=Vollkorn');\n",
       "@import url('http://fonts.googleapis.com/css?family=Arimo');\n",
       "@import url('http://fonts.googleapis.com/css?family=Fira_sans');\n",
       "    \n",
       "    div.cell {\n",
       "        width: 1000px;\n",
       "        margin-left: 0% !important;\n",
       "        margin-right: auto;\n",
       "    }\n",
       "    div.text_cell code {\n",
       "        background: transparent;\n",
       "        color: #000000;\n",
       "        font-weight: 600;\n",
       "        font-size: 12pt;\n",
       "        font-style: bold;\n",
       "        font-family:  'Source Code Pro', Consolas, monocco, monospace;\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "\t}\n",
       "\t\n",
       "    div.input_area {\n",
       "        background: #F6F6F9;\n",
       "        border: 1px solid #586e75;\n",
       "    }\n",
       "\n",
       "    .text_cell_render h1 {\n",
       "        font-weight: 200;\n",
       "        font-size: 30pt;\n",
       "        line-height: 100%;\n",
       "        color:#c76c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 1em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    } \n",
       "    h2 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "        text-align: left;\n",
       "    }\n",
       "    .text_cell_render h2 {\n",
       "        font-weight: 200;\n",
       "        font-size: 16pt;\n",
       "        font-style: italic;\n",
       "        line-height: 100%;\n",
       "        color:#c76c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 1.5em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    } \n",
       "    h3 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h3 {\n",
       "        font-weight: 200;\n",
       "        font-size: 14pt;\n",
       "        line-height: 100%;\n",
       "        color:#d77c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 2em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    }\n",
       "    h4 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h4 {\n",
       "        font-weight: 100;\n",
       "        font-size: 14pt;\n",
       "        color:#d77c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "        white-space: nowrap;\n",
       "    }\n",
       "    h5 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 200;\n",
       "        font-style: normal;\n",
       "        color: #1d3b84;\n",
       "        font-size: 16pt;\n",
       "        margin-bottom: 0em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "        white-space: nowrap;\n",
       "    }\n",
       "    div.text_cell_render{\n",
       "        font-family: 'Fira sans', verdana,arial,sans-serif;\n",
       "        line-height: 125%;\n",
       "        font-size: 115%;\n",
       "        text-align:justify;\n",
       "        text-justify:inter-word;\n",
       "    }\n",
       "    div.output_wrapper{\n",
       "        margin-top:0.2em;\n",
       "        margin-bottom:0.2em;\n",
       "    }\n",
       "\n",
       "    code{\n",
       "      font-size: 70%;\n",
       "    }\n",
       "    .rendered_html code{\n",
       "    background-color: transparent;\n",
       "    }\n",
       "    ul{\n",
       "        margin: 2em;\n",
       "    }\n",
       "    ul li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.5em; \n",
       "    }\n",
       "    ul li li{\n",
       "        padding-left: 0.2em; \n",
       "        margin-bottom: 0.2em; \n",
       "        margin-top: 0.2em; \n",
       "    }\n",
       "    ol{\n",
       "        margin: 2em;\n",
       "    }\n",
       "    ol li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.5em; \n",
       "    }\n",
       "    ul li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.2em; \n",
       "    }\n",
       "    a:link{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    a:visited{\n",
       "       font-weight: bold;\n",
       "       color: #1d3b84;\n",
       "    }\n",
       "    a:hover{\n",
       "       font-weight: bold;\n",
       "       color: #1d3b84;\n",
       "    }\n",
       "    a:focus{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    a:active{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    .rendered_html :link {\n",
       "       text-decoration: underline; \n",
       "    }\n",
       "    .rendered_html :hover {\n",
       "       text-decoration: none; \n",
       "    }\n",
       "    .rendered_html :visited {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .rendered_html :focus {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .rendered_html :active {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "    } \n",
       "    hr {\n",
       "      color: #f3f3f3;\n",
       "      background-color: #f3f3f3;\n",
       "      height: 1px;\n",
       "    }\n",
       "    blockquote{\n",
       "      display:block;\n",
       "      background: #fcfcfc;\n",
       "      border-left: 5px solid #c76c0c;\n",
       "      font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "      width:680px;\n",
       "      padding: 10px 10px 10px 10px;\n",
       "      text-align:justify;\n",
       "      text-justify:inter-word;\n",
       "      }\n",
       "      blockquote p {\n",
       "        margin-bottom: 0;\n",
       "        line-height: 125%;\n",
       "        font-size: 100%;\n",
       "      }\n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    scale:100,\n",
       "                        availableFonts: [],\n",
       "                        preferredFont:null,\n",
       "                        webFont: \"TeX\",\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code for loading the format for the notebook\n",
    "import os\n",
    "\n",
    "# path : store the current path to convert back to it later\n",
    "path = os.getcwd()\n",
    "os.chdir(os.path.join('..', 'notebook_format'))\n",
    "\n",
    "from formats import load_style\n",
    "load_style(plot_style = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethen 2018-09-17 17:58:04 \n",
      "\n",
      "CPython 3.6.4\n",
      "IPython 6.4.0\n",
      "\n",
      "numpy 1.14.1\n",
      "pandas 0.23.0\n",
      "sklearn 0.19.1\n",
      "scipy 1.1.0\n",
      "joblib 0.11\n"
     ]
    }
   ],
   "source": [
    "os.chdir(path)\n",
    "\n",
    "# 1. magic to print version\n",
    "# 2. magic so that the notebook will reload external python modules\n",
    "%load_ext watermark\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%watermark -a 'Ethen' -d -t -v -p numpy,pandas,sklearn,scipy,joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice [tweet](https://twitter.com/chrisalbon/status/877524257202253825) that gives a brief description of supervised machine learning:\n",
    "\n",
    "> In machine learning, we have a training set â€” comprised of features (a.k.a inputs, independent variables) and labels (a.k.a. response, target, dependent variables). We use an algorithm to train a set of models with varying hyperparameter values then select the model that best minimizes some cost (a.k.a. loss, objective) function. We then use this model to make prediction on an unseen test set and evaluate the model's final performance. This documentation focuses on methods to select hyperparamters.\n",
    "\n",
    "It is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a test set. This way, we can evaluate how well the model will generalize to a new set of data that were not seen during the training phase. In other words, this process tries avoid the overfitting phenomenon where the algorithm starts memorizing the training set's labels but fail to predict anything for the unseen test data. However, if we were just to perform a train/test split, the results can depend on a particular random choice for the pair of train/test sets. One solution to this problem is a procedure called cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most common technique for model evaluation and model selection in machine learning practice is K-fold cross validation. The main idea behind cross-validation is that each observation in our dataset has the opportunity of being tested. K-fold cross-validation is a special case of cross-validation where we iterate over a dataset set k times. In each round, we split the dataset into $k$ parts: one part is used for validation, and the remaining $k-1$ parts are merged into a training subset for model evaluation. The figure below illustrates the process of 5-fold cross-validation:\n",
    "\n",
    "<img src=\"img/kfolds.png\" width=\"60%\" height=\"60%\">\n",
    "\n",
    "We use a learning algorithm with fixed hyperparameter settings to fit models to the training folds in each iteration. In 5-fold cross-validation, this procedure will result in 5 models fitted on distinct yet partly overlapping training sets and evaluated on non-overlapping validation sets. Eventually, we compute the cross-validation performance as the arithmetic mean over the $k$ performance estimates from the validation sets. The main benefit behind this approach versus a simple train/test split is to reduce the pessimistic bias by using more training data in contrast to setting aside a relatively large portion of the dataset as test data.\n",
    "\n",
    "The following section shows a vanilla implementation of how to generate a K-fold data split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KFolds:\n",
    "    \"\"\"\n",
    "    K-Folds cross-validation\n",
    "    Provides train/test indices to split data in train/test sets. Split\n",
    "    dataset into k consecutive folds; Each fold is then used once as \n",
    "    a validation while the k - 1 remaining folds form the training set\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_splits : int\n",
    "        number of folds. Must be at least 2\n",
    "    \n",
    "    shuffle : bool, default True\n",
    "        whether to shuffle the data before splitting into batches\n",
    "    \n",
    "    seed : int, default 4321\n",
    "        When shuffle = True, pseudo-random number generator state used for\n",
    "        shuffling; this ensures reproducibility\n",
    "    \"\"\"\n",
    "    def __init__(self, n_splits, shuffle = True, seed = 4321):\n",
    "        self.seed = seed\n",
    "        self.shuffle = shuffle\n",
    "        self.n_splits = n_splits\n",
    "        \n",
    "    def split(self, X):\n",
    "        \"\"\"pass in the data to create train/test split for k fold\"\"\"\n",
    "        # shuffle modifies indices inplace\n",
    "        n_samples = X.shape[0]\n",
    "        indices = np.arange(n_samples)\n",
    "        if self.shuffle:\n",
    "            rstate = np.random.RandomState(self.seed)\n",
    "            rstate.shuffle(indices)\n",
    "\n",
    "        for test_mask in self._iter_test_masks(n_samples, indices):\n",
    "            train_index = indices[np.logical_not(test_mask)]\n",
    "            test_index = indices[test_mask]\n",
    "            yield train_index, test_index\n",
    "        \n",
    "    def _iter_test_masks(self, n_samples, indices):\n",
    "        \"\"\"\n",
    "        create the mask for the test set, then the indices that\n",
    "        are not in the test set belongs in the training set\n",
    "        \"\"\"\n",
    "        # indicate the number of samples in each fold, and also\n",
    "        # make sure the ones that are not evenly splitted also\n",
    "        # gets assigned to a fold (e.g. if we do 2 fold on a\n",
    "        # dataset that has 5 samples, then 1 will be left out,\n",
    "        # and has to be assigned to one of the other fold)\n",
    "        fold_sizes = (n_samples // self.n_splits) * np.ones(self.n_splits, dtype = np.int)\n",
    "        fold_sizes[:n_samples % self.n_splits] += 1\n",
    "\n",
    "        current = 0\n",
    "        for fold_size in fold_sizes:\n",
    "            start, stop = current, current + fold_size\n",
    "            test_indices = indices[start:stop]\n",
    "            test_mask = np.zeros(n_samples, dtype = np.bool)\n",
    "            test_mask[test_indices] = True\n",
    "            yield test_mask\n",
    "            current = stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [3 4] TEST: [0 1 2]\n",
      "TRAIN: [0 1 2] TEST: [3 4]\n",
      "\n",
      "confirm results with scikit-learn\n",
      "TRAIN: [3 4] TEST: [0 1 2]\n",
      "TRAIN: [0 1 2] TEST: [3 4]\n"
     ]
    }
   ],
   "source": [
    "# create some sample data\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [5, 6]])\n",
    "y = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "kf = KFolds(n_splits = 2, shuffle = False, seed = 4312)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "print('\\nconfirm results with scikit-learn')\n",
    "kf = KFold(n_splits = 2, shuffle = False, random_state = 4312)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we implemented the simple version of K-fold, we should also keep in mind that some classification problems can exhibit a large imbalance in the distribution of the target classes: for instance there could be several times more negative samples than positive samples. In such cases it is recommended to use stratified sampling as implemented in [`StratifiedKFold`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold). Stratified basically means each set contains approximately the same percentage of samples of each target class as the complete set.\n",
    "\n",
    "---\n",
    "\n",
    "Now that have a sense of what K-fold cross validation is doing, let's look at how it can be used with hyperparameter tuning.\n",
    "\n",
    "So the general process is:\n",
    "\n",
    "- We split our dataset into two parts, a training and an independent test set; we tuck away the test set for the final model evaluation step at the end\n",
    "- In the second step, we can now experiment with various hyperparameter settings; we could use Bayesian Optimization, Randomized Search, or plain old Grid Search (more on this later, think of it as different ways of generating hyperparameter combinations). For each hyperparameter configuration, we apply the K-fold cross validation on the training set, resulting in multiple models and performance estimates. See figure below:\n",
    "\n",
    "<img src=\"img/hyperparameter_search.png\" width=\"75%\" height=\"75%\">\n",
    "\n",
    "- After finding the best set of hyperparameter, we take the best-performing setting for that model and use the complete training set for model fitting.\n",
    "\n",
    "<img src=\"img/refit.png\" width=\"60%\" height=\"60%\">\n",
    "\n",
    "- Then we make use of the independent test set we withheld at the beginning to evaluate the model that we obtained\n",
    "\n",
    "Let's now tackle each step one at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various ways to perform hyperparameter tuning and one of them is Grid Search. This method exhaustively considers all parameter combinations and picks the best one based on the model that gives the best performance (we can specify the performance criteria)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sorted parameters, values:  [('a', [1, 2]), ('b', [True, False])]\n",
      "\n",
      "parameters:  ('a', 'b')\n",
      "values ([1, 2], [True, False])\n",
      "\n",
      "grid search parameters\n",
      "{'a': 1, 'b': True}\n",
      "{'a': 1, 'b': False}\n",
      "{'a': 2, 'b': True}\n",
      "{'a': 2, 'b': False}\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# suppose we have hyperparameter 'a' and 'b' and\n",
    "# we wish to test out value 1, 2 for a and\n",
    "# True, False for b, so grid search will simply\n",
    "# evaluate all four possible combinations\n",
    "param_grid = {'a': [1, 2], 'b': [True, False]}\n",
    "\n",
    "# for reproducibility, always sort the keys of a dictionary\n",
    "# this will become a list of paired tuples\n",
    "items = sorted(param_grid.items())\n",
    "print('sorted parameters, values: ', items)\n",
    "print()\n",
    "\n",
    "# unpack the list of tuples into two lists tuples, so what's originally \n",
    "# a list of items [('a', [1, 2]), ('b', [True, False])], becomes\n",
    "# two lists ('a', 'b'), ([1, 2], [True, False]), with all the keys being the parameter\n",
    "# and the value being the list of possible values that the parameter can take\n",
    "# http://stackoverflow.com/questions/7558908/unpacking-a-list-tuple-of-pairs-into-two-lists-tuples\n",
    "key, value = zip(*items)\n",
    "print('parameters: ', key)\n",
    "print('values', value)\n",
    "print()\n",
    "\n",
    "# unpack the list of values to compute the cartesian product\n",
    "# [(1, True), (1, False), (2, True), (2, False)], and zip it\n",
    "# back to the original key\n",
    "print('grid search parameters')\n",
    "cartesian = product(*value)\n",
    "for v in cartesian:\n",
    "    params = dict(zip(key, v))\n",
    "    print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'a': 1, 'b': True},\n",
       " {'a': 1, 'b': False},\n",
       " {'a': 2, 'b': True},\n",
       " {'a': 2, 'b': False}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm with scikit-learn's output\n",
    "list(ParameterGrid(param_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': True}\n",
      "{'a': 1, 'b': False}\n",
      "{'a': 2, 'b': True}\n",
      "{'a': 2, 'b': False}\n"
     ]
    }
   ],
   "source": [
    "# putting it all into a function\n",
    "def _get_param_grid(param_grid):\n",
    "    \"\"\"\n",
    "    create cartesian product of parameters (grid search),\n",
    "    this will be a generator that will allow looping through\n",
    "    all possible parameter combination\n",
    "    \n",
    "    Reference\n",
    "    ---------\n",
    "    scikit-learn ParameterGrid\n",
    "    - https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/model_selection/_search.py#L44\n",
    "    \"\"\"\n",
    "    # for reproducibility, always sort the keys of a dictionary\n",
    "    items = sorted(param_grid.items())\n",
    "    \n",
    "    # unpack parameter and the range of values\n",
    "    # into separate list; then unpack the range \n",
    "    # of values to compute the cartesian product\n",
    "    # and zip it back to the original key\n",
    "    key, value = zip(*items)\n",
    "    cartesian = product(*value)\n",
    "    for v in cartesian:\n",
    "        params = dict(zip(key, v))\n",
    "        yield params\n",
    "\n",
    "\n",
    "param_grid = {'a': [1, 2], 'b': [True, False]}\n",
    "params = _get_param_grid(param_grid)\n",
    "for p in params:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can tie it with the K-fold that we learned earlier. To do so, we will make use of [joblib](https://pythonhosted.org/joblib/parallel.html), a package that allow us to write parallel processing code relatively easy.\n",
    "\n",
    "An \"embarrassingly parallel\" computing task is one in which each calculation is independent of the ones that came before or after it. For example, squaring each number in the list [1, 2, 3, 4, 5] is embarrassingly parallel because the square of 2 does not depend on 1, 3 on 2, and so on. So when performing these types of calculations, we can and should run these operations using different processors/cores (or course for this simple problem it's definitely not worth it ...). In our case, because fitting the model using different hyperparameters and using different folds of the data to fit the model are all independent computations, the problem falls under the \"embarrassingly parallel\" category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "class GridSearch:\n",
    "    \"\"\"\n",
    "    Exhaustive search over specified parameter values for an estimator\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : estimator object\n",
    "        This is assumed to implement the scikit-learn estimator interface,\n",
    "        it needs to have a .fit and set_params method\n",
    "\n",
    "    param_grid : dict\n",
    "        Dictionary with parameters names (string) as keys and lists of\n",
    "        parameter settings to try as values\n",
    "\n",
    "    scorer : callable function\n",
    "        scorer callable object / function with \n",
    "        signature scorer(estimator, X, y)\n",
    "\n",
    "    cv : int or cross-validation generator (e.g. Kfolds)\n",
    "        Determines the cross-validation splitting strategy. \n",
    "        Possible inputs for cv are:\n",
    "        - integer, to specify the number of folds in a KFold\n",
    "        - An object to be used as a cross-validation generator\n",
    "\n",
    "    fit_params : dict, optional\n",
    "        Additional parameters to pass to the fit method\n",
    "\n",
    "    n_jobs : int, default -1\n",
    "        Number of jobs to run in parallel, if the model already fits\n",
    "        extremely fast on the data, then specify 1 so that there's no \n",
    "        overhead of spawning different processes to do the computation\n",
    "\n",
    "    verbose : bool, default True\n",
    "        Whether to print the fitting progress\n",
    "\n",
    "    pre_dispatch : int, or string, default '2*n_jobs'\n",
    "        Controls the number of jobs that get dispatched during parallel\n",
    "        execution. Reducing this number can be useful to avoid an\n",
    "        explosion of memory consumption when more jobs get dispatched\n",
    "        than CPUs can process. This parameter can be:\n",
    "            - None, in which case all the jobs are immediately\n",
    "              created and spawned. Use this for lightweight and\n",
    "              fast-running jobs, to avoid delays due to on-demand\n",
    "              spawning of the jobs\n",
    "            - An int, giving the exact number of total jobs that are\n",
    "              spawned\n",
    "            - A string, giving an expression as a function of n_jobs,\n",
    "              as in '2*n_jobs'\n",
    "\n",
    "    refit : bool, default True\n",
    "        Refit the estimator with best set of parameters on the entire dataset\n",
    "        If \"False\", it is impossible to make whether predictions using\n",
    "        this GridSearch instance after fitting\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    best_estimator_ : estimator\n",
    "        Best estimator that was chosen by the search, i.e. estimator which \n",
    "        gave best test score. Not available if refit = False\n",
    "\n",
    "    best_score_ : float\n",
    "        Best score on the hold out data\n",
    "\n",
    "    best_params_ : dict\n",
    "        Parameter settings that gave the best results on the hold out data\n",
    "\n",
    "    cv_results_ : DataFrame\n",
    "        records the average train/test score for each hyperparameter, result\n",
    "        is sorted by the test score\n",
    "    \n",
    "    Reference\n",
    "    ---------\n",
    "    scikit-learn GridSearchCV\n",
    "    - http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "    \"\"\"\n",
    "    def __init__(self, estimator, scorer, cv, param_grid,\n",
    "                 fit_params = None, verbose = True, n_jobs = -1, \n",
    "                 pre_dispatch = '2*n_jobs', refit = True):\n",
    "        self.cv = cv\n",
    "        self.refit = refit\n",
    "        self.n_jobs = n_jobs\n",
    "        self.scorer = scorer\n",
    "        self.verbose = verbose\n",
    "        self.estimator = estimator\n",
    "        self.param_grid = param_grid\n",
    "        self.fit_params = fit_params\n",
    "        self.pre_dispatch = pre_dispatch     \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 2d nd.array, shape = [n_samples, n_features]\n",
    "            training vector, where n_samples in the number of samples and\n",
    "            n_features is the number of features.\n",
    "        \n",
    "        y : 1d nd.array, shape = [n_samples]\n",
    "            target/label relative to X for classification or regression\n",
    "        \"\"\"\n",
    "        # object used as a cross-validation generator\n",
    "        # is passed without any modification\n",
    "        if isinstance(self.cv, int):\n",
    "            cv = KFold(n_splits = self.cv, shuffle = True)\n",
    "        else:\n",
    "            cv = self.cv\n",
    "        \n",
    "        # obtain the train/test set index, the parameters\n",
    "        # and perform cross validation\n",
    "        cv_iter = cv.split(X)\n",
    "        \n",
    "        # obtain the hyperparameter settings, we will need to turn it\n",
    "        # to the list or else the iterator will be exhausted after we\n",
    "        # loop through it once (i.e. one of the fold)\n",
    "        params_iterable = list(_get_param_grid(self.param_grid))\n",
    "        fit_params = self.fit_params if self.fit_params is not None else {}\n",
    "        \n",
    "        # hyperparameter tuning + K-fold cross validation\n",
    "        parallel = Parallel(n_jobs = self.n_jobs, verbose = self.verbose, \n",
    "                            pre_dispatch = self.pre_dispatch)\n",
    "        output = parallel(delayed(self._fit_and_score)(deepcopy(self.estimator), \n",
    "                                                       X, y, self.scorer,\n",
    "                                                       train_index, test_index, \n",
    "                                                       parameters, fit_params)\n",
    "                          for train_index, test_index in cv_iter\n",
    "                          for parameters in params_iterable)\n",
    "\n",
    "        # unpack training/testing scores\n",
    "        n_splits = cv.n_splits\n",
    "        n_candidates = len(params_iterable)\n",
    "        train_score, test_score = zip(*output)\n",
    "        train_score = np.array(train_score, dtype = np.float64).reshape(n_splits, n_candidates)\n",
    "        test_score = np.array(test_score, dtype = np.float64).reshape(n_splits, n_candidates)\n",
    "        \n",
    "        # obtain the best score and parameter using the \n",
    "        # best mean test scores across all folds, where\n",
    "        # best here means the higher the better\n",
    "        mean_test_score = np.mean(test_score, axis = 0)\n",
    "        best_index = np.argmax(mean_test_score)\n",
    "        self.best_score_ = mean_test_score[best_index]\n",
    "        self.best_param_ = params_iterable[best_index]\n",
    "\n",
    "        # list the mean, std train and test score\n",
    "        # for each parameters combination;\n",
    "        # not sure if 'params', the column with the\n",
    "        # values in the dictionary format is useful or not\n",
    "        mean_train_score = np.mean(train_score, axis = 0)\n",
    "        std_test_score = np.std(test_score, axis = 0)\n",
    "        std_train_score = np.std(train_score, axis = 0)\n",
    "        cv_results = {'mean_train_score': mean_train_score,\n",
    "                      'std_train_score': std_train_score,\n",
    "                      'mean_test_score': mean_test_score,\n",
    "                      'std_test_score': std_test_score}\n",
    "\n",
    "        # ensure the columns appear in this order\n",
    "        # (train score, test score, parameters)\n",
    "        # and order by the best test score\n",
    "        cols = ['mean_train_score', 'std_train_score', \n",
    "                'mean_test_score', 'std_test_score']\n",
    "        cv_results = pd.DataFrame(cv_results, columns = cols)\n",
    "        df_params = pd.DataFrame(params_iterable)\n",
    "        cv_results = pd.concat([cv_results, df_params], axis = 1)\n",
    "        cv_results['params'] = params_iterable\n",
    "        cv_results = (cv_results.\n",
    "                      sort_values(['mean_test_score', 'std_test_score'], ascending = False).\n",
    "                      reset_index(drop = True))\n",
    "        self.cv_results_ = cv_results\n",
    "        \n",
    "        # refit on the entire dataset after performing cross validation\n",
    "        if self.refit:\n",
    "            best_estimator = deepcopy(self.estimator)\n",
    "            best_estimator.set_params(**self.best_param_)\n",
    "            best_estimator.fit(X, y, **fit_params)\n",
    "            self.best_estimator_ = best_estimator\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _fit_and_score(self, estimator, X, y, scorer, \n",
    "                       train_index, test_index,\n",
    "                       parameters, fit_params):\n",
    "        # create the train/test split\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # fit the model using the current hyperparameter setting\n",
    "        estimator.set_params(**parameters)\n",
    "        estimator.fit(X_train, y_train, **fit_params)\n",
    "\n",
    "        # obtain the train/test score\n",
    "        y_pred_train = estimator.predict(X_train)\n",
    "        y_pred_test = estimator.predict(X_test)\n",
    "        train_score = scorer(y_train, y_pred_train)\n",
    "        test_score = scorer(y_test, y_pred_test)\n",
    "        output = [train_score, test_score]\n",
    "        return output\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"call predict on the estimator with the best found parameter\"\"\"\n",
    "        if not self.refit:\n",
    "            raise ValueError('Only available if refit=True')\n",
    "        \n",
    "        return self.best_estimator_.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 out of  12 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>criterion</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.024944</td>\n",
       "      <td>gini</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': None}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.983333</td>\n",
       "      <td>0.009428</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.009428</td>\n",
       "      <td>gini</td>\n",
       "      <td>3.0</td>\n",
       "      <td>{'criterion': 'gini', 'max_depth': 3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.970000</td>\n",
       "      <td>0.014142</td>\n",
       "      <td>0.953333</td>\n",
       "      <td>0.009428</td>\n",
       "      <td>entropy</td>\n",
       "      <td>3.0</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': 3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.946667</td>\n",
       "      <td>0.033993</td>\n",
       "      <td>entropy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'criterion': 'entropy', 'max_depth': None}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_train_score  std_train_score  mean_test_score  std_test_score  \\\n",
       "0          1.000000         0.000000         0.953333        0.024944   \n",
       "1          0.983333         0.009428         0.953333        0.009428   \n",
       "2          0.970000         0.014142         0.953333        0.009428   \n",
       "3          1.000000         0.000000         0.946667        0.033993   \n",
       "\n",
       "  criterion  max_depth                                       params  \n",
       "0      gini        NaN     {'criterion': 'gini', 'max_depth': None}  \n",
       "1      gini        3.0        {'criterion': 'gini', 'max_depth': 3}  \n",
       "2   entropy        3.0     {'criterion': 'entropy', 'max_depth': 3}  \n",
       "3   entropy        NaN  {'criterion': 'entropy', 'max_depth': None}  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# load a toy dataset,\n",
    "# ideally we should also perform a train/test split\n",
    "# here, and use the test set as our final validation\n",
    "# of the model's performance\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# grid search parameters\n",
    "cv = 3\n",
    "scorer = accuracy_score\n",
    "clf = RandomForestClassifier()\n",
    "param_grid = {'max_depth': [3, None],\n",
    "              'criterion': ['gini', 'entropy']}\n",
    "\n",
    "# fit grid search\n",
    "grid_search = GridSearch(estimator = clf, scorer = scorer, cv = cv, param_grid = param_grid)\n",
    "grid_search.fit(X, y)\n",
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While grid search is a widely used method for parameter optimization, other search methods such as random search have more favorable properties. So instead of listing out the list of values to try for each parameter and then trying out all possible combinations of these values, in random search each parameter is sampled from a distribution. The user can specify the distribution to sample from and how many parameters should be sampled (this can be a big benefit if we're limited on computation budget).\n",
    "\n",
    "The rationale behind why random search can be better than the much more robust-looking grid-search can be nicely illustrated by the figure below:\n",
    "\n",
    "<img src=\"img/search_comparison.png\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "The idea is that in most cases the bumpy surface of the objective/cost function is not as bumpy in all dimensions. In other words, some parameters have much less effect on the objective/cost function than others. Hence, given the same amount of trials, using random search allows us to explore the space more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import randint\n",
    "\n",
    "# we can use distributions from the scipy.stats module\n",
    "# or we can write our own distribution, but to use \n",
    "# scikit-learn, it has to have a .rvs method\n",
    "# that generates a random number from the distribution\n",
    "# e.g. a random uniform distribution\n",
    "randint(low = 3, high = 8).rvs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'a': 6, 'b': True},\n",
       " {'a': 7, 'b': True},\n",
       " {'a': 3, 'b': False},\n",
       " {'a': 4, 'b': False},\n",
       " {'a': 4, 'b': True}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _get_param_sampler(param_distributions, n_iters, random_state):\n",
    "    \"\"\"\n",
    "    generator on parameters sampled from given distributions\n",
    "    \n",
    "    param_distributions : dict\n",
    "        Dictionary with parameters names (string) as keys and distributions\n",
    "        or lists of parameters to try. Distributions must provide a ``rvs``\n",
    "        method for sampling (such as those from scipy.stats.distributions).\n",
    "        If a list is given, it is sampled uniformly.\n",
    "    \n",
    "    n_iters : int\n",
    "        Number of parameter settings that are sampled. n_iters trades\n",
    "        off runtime vs quality of the solution\n",
    "        \n",
    "    random_state : int\n",
    "        seed for random sampling\n",
    "    \n",
    "    Reference\n",
    "    ---------\n",
    "    scikit-learn's ParameterSampler\n",
    "    - https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/model_selection/_search.py#L182\n",
    "    \"\"\"\n",
    "    # for reproducibility, always sort the keys of a dictionary\n",
    "    items = sorted(param_distributions.items())\n",
    "    \n",
    "    rstate = np.random.RandomState(random_state)\n",
    "    for _ in range(n_iters):\n",
    "        params = dict()\n",
    "        for k, v in items:\n",
    "            if hasattr(v, 'rvs'):\n",
    "                params[k] = v.rvs(random_state = rstate)\n",
    "            else:\n",
    "                params[k] = v[rstate.randint(len(v))]\n",
    "\n",
    "        yield params\n",
    "\n",
    "\n",
    "n_iters = 5\n",
    "random_state = 1234\n",
    "param_distributions = {'a': randint(low = 3, high = 8), 'b': [True, False]}\n",
    "list(_get_param_sampler(param_distributions, n_iters, random_state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random & Grid Search Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section demonstrates how to use the scikit-learn `GridSearchCV` and `RandomizedSearchCV`. On this particular example using randomized search and the grid search resulted in similar performance when tuning the same parameters, while the run time for randomized search is drastically lower. Note that your mileage may vary. This result is not saying that `RandomizedSearchCV` will always outperform `GridSearchCV`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 3.09 seconds for 24 parameter settings\n",
      "Best score obtained: 0.9287701725097385\n",
      "Parameters:\n",
      "\tcriterion: entropy\n",
      "\tmax_depth: 10\n",
      "\tmax_features: 7\n",
      "\n",
      "GridSearchCV took 5.67 seconds for 48 parameter settings\n",
      "Best score obtained: 0.9248747913188647\n",
      "Parameters:\n",
      "\tcriterion: gini\n",
      "\tmax_depth: 7\n",
      "\tmax_features: 11\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "from scipy.stats import randint\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "# load another toy dataset\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "\n",
    "# build a classifier\n",
    "clf = RandomForestClassifier(n_estimators = 20)\n",
    "\n",
    "def report(results):\n",
    "    \"\"\"report best scores and corresponding parameters\"\"\"\n",
    "    print('Best score obtained: {0}'.format(results.best_score_))\n",
    "    print('Parameters:')\n",
    "    for param, value in results.best_params_.items():\n",
    "        print('\\t{}: {}'.format(param, value))\n",
    "\n",
    "\n",
    "# specify parameters and distributions to sample from\n",
    "param_dist = {'max_depth': randint(low = 3, high = 11),\n",
    "              'max_features': randint(low = 3, high = 11),\n",
    "              'criterion': ['gini', 'entropy']}\n",
    "\n",
    "# run randomized search\n",
    "# set the number of samples to\n",
    "# be half of the grid search\n",
    "n_iter_search = 24\n",
    "random_search = RandomizedSearchCV(clf, param_distributions = param_dist,\n",
    "                                   n_iter = n_iter_search, random_state = 789)\n",
    "start = time()\n",
    "random_search.fit(X, y)\n",
    "elapse = time() - start\n",
    "message = 'RandomizedSearchCV took {:.2f} seconds for {} parameter settings'\n",
    "print(message.format(elapse, n_iter_search))\n",
    "report(random_search)\n",
    "print()\n",
    "\n",
    "# use a full grid over all parameters\n",
    "param_grid = {'max_depth': np.arange(3, 11),\n",
    "              'max_features': [1, 3, 11],\n",
    "              'criterion': ['gini', 'entropy']}\n",
    "\n",
    "# run grid search\n",
    "grid_search = GridSearchCV(clf, param_grid = param_grid)\n",
    "start = time()\n",
    "grid_search.fit(X, y)\n",
    "elapse = time() - start\n",
    "n_iter_search = len(grid_search.cv_results_['params'])\n",
    "message = 'GridSearchCV took {:.2f} seconds for {} parameter settings'\n",
    "print(message.format(elapse, n_iter_search))\n",
    "report(grid_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Github: scikit-learn's KFold](https://github.com/scikit-learn/scikit-learn/blob/14031f6/sklearn/model_selection/_split.py#L347)\n",
    "- [Github: scikit-learn's GridSearch](https://github.com/scikit-learn/scikit-learn/blob/14031f6/sklearn/model_selection/_search.py#L685)\n",
    "- [Scikit-learn Documentation: Tuning the hyper-parameters of an estimator](http://scikit-learn.org/stable/modules/grid_search.html)\n",
    "- [Scikit-learn Documentation: Cross-validation: evaluating estimator performance](http://scikit-learn.org/stable/modules/cross_validation.html)\n",
    "- [Scikit-learn Documentation: Comparing randomized search and grid search for hyperparameter estimation](http://scikit-learn.org/stable/auto_examples/model_selection/randomized_search.html#sphx-glr-auto-examples-model-selection-randomized-search-py)\n",
    "- [Blog: Smarter Parameter Sweeps (or Why Grid Search Is Plain Stupid)](https://medium.com/rants-on-machine-learning/smarter-parameter-sweeps-or-why-grid-search-is-plain-stupid-c17d97a0e881)\n",
    "- [Blog: Model evaluation, model selection, and algorithm selection in machine learning Part III - Cross-validation and hyperparameter tuning](https://sebastianraschka.com/blog/2016/model-evaluation-selection-part3.html)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {
    "height": "102px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
