{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Logistic Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Logistic-Function\" data-toc-modified-id=\"Logistic-Function-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Logistic Function</a></span></li><li><span><a href=\"#Interpreting-the-Intercept\" data-toc-modified-id=\"Interpreting-the-Intercept-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Interpreting the Intercept</a></span></li><li><span><a href=\"#Defining-The-Cost-Function\" data-toc-modified-id=\"Defining-The-Cost-Function-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Defining The Cost Function</a></span></li><li><span><a href=\"#Gradient\" data-toc-modified-id=\"Gradient-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Gradient</a></span></li><li><span><a href=\"#Stochastic/Mini-batch-Gradient\" data-toc-modified-id=\"Stochastic/Mini-batch-Gradient-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Stochastic/Mini-batch Gradient</a></span></li><li><span><a href=\"#Implementation\" data-toc-modified-id=\"Implementation-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Implementation</a></span></li><li><span><a href=\"#Comparing-Result-and-Convergence-Behavior\" data-toc-modified-id=\"Comparing-Result-and-Convergence-Behavior-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Comparing Result and Convergence Behavior</a></span></li><li><span><a href=\"#Pros-and-Cons-of-Logistic-Regression\" data-toc-modified-id=\"Pros-and-Cons-of-Logistic-Regression-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Pros and Cons of Logistic Regression</a></span></li></ul></li><li><span><a href=\"#Reference\" data-toc-modified-id=\"Reference-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Reference</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "@import url('http://fonts.googleapis.com/css?family=Source+Code+Pro');\n",
       "@import url('http://fonts.googleapis.com/css?family=Vollkorn');\n",
       "@import url('http://fonts.googleapis.com/css?family=Arimo');\n",
       "@import url('http://fonts.googleapis.com/css?family=Fira_sans');\n",
       "    \n",
       "    div.cell {\n",
       "        width: 1000px;\n",
       "        margin-left: 0% !important;\n",
       "        margin-right: auto;\n",
       "    }\n",
       "    div.text_cell code {\n",
       "        background: transparent;\n",
       "        color: #000000;\n",
       "        font-weight: 600;\n",
       "        font-size: 12pt;\n",
       "        font-style: bold;\n",
       "        font-family:  'Source Code Pro', Consolas, monocco, monospace;\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "\t}\n",
       "\t\n",
       "    div.input_area {\n",
       "        background: #F6F6F9;\n",
       "        border: 1px solid #586e75;\n",
       "    }\n",
       "\n",
       "    .text_cell_render h1 {\n",
       "        font-weight: 200;\n",
       "        font-size: 30pt;\n",
       "        line-height: 100%;\n",
       "        color:#c76c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 1em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    } \n",
       "    h2 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "        text-align: left;\n",
       "    }\n",
       "    .text_cell_render h2 {\n",
       "        font-weight: 200;\n",
       "        font-size: 16pt;\n",
       "        font-style: italic;\n",
       "        line-height: 100%;\n",
       "        color:#c76c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 1.5em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    } \n",
       "    h3 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h3 {\n",
       "        font-weight: 200;\n",
       "        font-size: 14pt;\n",
       "        line-height: 100%;\n",
       "        color:#d77c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 2em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    }\n",
       "    h4 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h4 {\n",
       "        font-weight: 100;\n",
       "        font-size: 14pt;\n",
       "        color:#d77c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "        white-space: nowrap;\n",
       "    }\n",
       "    h5 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 200;\n",
       "        font-style: normal;\n",
       "        color: #1d3b84;\n",
       "        font-size: 16pt;\n",
       "        margin-bottom: 0em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "        white-space: nowrap;\n",
       "    }\n",
       "    div.text_cell_render{\n",
       "        font-family: 'Fira sans', verdana,arial,sans-serif;\n",
       "        line-height: 125%;\n",
       "        font-size: 115%;\n",
       "        text-align:justify;\n",
       "        text-justify:inter-word;\n",
       "    }\n",
       "    div.output_wrapper{\n",
       "        margin-top:0.2em;\n",
       "        margin-bottom:0.2em;\n",
       "    }\n",
       "\n",
       "    code{\n",
       "      font-size: 70%;\n",
       "    }\n",
       "    .rendered_html code{\n",
       "    background-color: transparent;\n",
       "    }\n",
       "    ul{\n",
       "        margin: 2em;\n",
       "    }\n",
       "    ul li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.5em; \n",
       "    }\n",
       "    ul li li{\n",
       "        padding-left: 0.2em; \n",
       "        margin-bottom: 0.2em; \n",
       "        margin-top: 0.2em; \n",
       "    }\n",
       "    ol{\n",
       "        margin: 2em;\n",
       "    }\n",
       "    ol li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.5em; \n",
       "    }\n",
       "    ul li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.2em; \n",
       "    }\n",
       "    a:link{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    a:visited{\n",
       "       font-weight: bold;\n",
       "       color: #1d3b84;\n",
       "    }\n",
       "    a:hover{\n",
       "       font-weight: bold;\n",
       "       color: #1d3b84;\n",
       "    }\n",
       "    a:focus{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    a:active{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    .rendered_html :link {\n",
       "       text-decoration: underline; \n",
       "    }\n",
       "    .rendered_html :hover {\n",
       "       text-decoration: none; \n",
       "    }\n",
       "    .rendered_html :visited {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .rendered_html :focus {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .rendered_html :active {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "    } \n",
       "    hr {\n",
       "      color: #f3f3f3;\n",
       "      background-color: #f3f3f3;\n",
       "      height: 1px;\n",
       "    }\n",
       "    blockquote{\n",
       "      display:block;\n",
       "      background: #fcfcfc;\n",
       "      border-left: 5px solid #c76c0c;\n",
       "      font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "      width:680px;\n",
       "      padding: 10px 10px 10px 10px;\n",
       "      text-align:justify;\n",
       "      text-justify:inter-word;\n",
       "      }\n",
       "      blockquote p {\n",
       "        margin-bottom: 0;\n",
       "        line-height: 125%;\n",
       "        font-size: 100%;\n",
       "      }\n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    scale:100,\n",
       "                        availableFonts: [],\n",
       "                        preferredFont:null,\n",
       "                        webFont: \"TeX\",\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code for loading the format for the notebook\n",
    "import os\n",
    "\n",
    "# path : store the current path to convert back to it later\n",
    "path = os.getcwd()\n",
    "os.chdir(os.path.join('..', 'notebook_format'))\n",
    "\n",
    "from formats import load_style\n",
    "load_style(plot_style = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethen 2018-09-15 15:50:42 \n",
      "\n",
      "CPython 3.6.4\n",
      "IPython 6.4.0\n",
      "\n",
      "numpy 1.14.1\n",
      "pandas 0.23.0\n",
      "matplotlib 2.2.2\n",
      "sklearn 0.19.1\n"
     ]
    }
   ],
   "source": [
    "os.chdir(path)\n",
    "\n",
    "# 1. magic for inline plot\n",
    "# 2. magic to print version\n",
    "# 3. magic so that the notebook will reload external python modules\n",
    "# 4. magic to enable retina (high resolution) plots\n",
    "# https://gist.github.com/minrk/3301035\n",
    "%matplotlib inline\n",
    "%load_ext watermark\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "%watermark -a 'Ethen' -d -t -v -p numpy,pandas,matplotlib,sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic regression** is an excellent tool to know for classification problems, which are problems where the output value that we wish to predict only takes on only a small number of discrete values. Here we'll focus on the binary classification problem, where the output can take on only two distinct classes. To make our examples more concrete, we will consider the Glass dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ri</th>\n",
       "      <th>na</th>\n",
       "      <th>mg</th>\n",
       "      <th>al</th>\n",
       "      <th>si</th>\n",
       "      <th>k</th>\n",
       "      <th>ca</th>\n",
       "      <th>ba</th>\n",
       "      <th>fe</th>\n",
       "      <th>glass_type</th>\n",
       "      <th>household</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.51966</td>\n",
       "      <td>14.77</td>\n",
       "      <td>3.75</td>\n",
       "      <td>0.29</td>\n",
       "      <td>72.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>9.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>1.51115</td>\n",
       "      <td>17.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>75.41</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1.52213</td>\n",
       "      <td>14.21</td>\n",
       "      <td>3.82</td>\n",
       "      <td>0.47</td>\n",
       "      <td>71.77</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9.57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1.52213</td>\n",
       "      <td>14.21</td>\n",
       "      <td>3.82</td>\n",
       "      <td>0.47</td>\n",
       "      <td>71.77</td>\n",
       "      <td>0.11</td>\n",
       "      <td>9.57</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1.52320</td>\n",
       "      <td>13.72</td>\n",
       "      <td>3.72</td>\n",
       "      <td>0.51</td>\n",
       "      <td>71.75</td>\n",
       "      <td>0.09</td>\n",
       "      <td>10.06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ri     na    mg    al     si     k     ca   ba    fe  glass_type  \\\n",
       "id                                                                           \n",
       "22   1.51966  14.77  3.75  0.29  72.02  0.03   9.00  0.0  0.00           1   \n",
       "185  1.51115  17.38  0.00  0.34  75.41  0.00   6.65  0.0  0.00           6   \n",
       "40   1.52213  14.21  3.82  0.47  71.77  0.11   9.57  0.0  0.00           1   \n",
       "39   1.52213  14.21  3.82  0.47  71.77  0.11   9.57  0.0  0.00           1   \n",
       "51   1.52320  13.72  3.72  0.51  71.75  0.09  10.06  0.0  0.16           1   \n",
       "\n",
       "     household  \n",
       "id              \n",
       "22           0  \n",
       "185          1  \n",
       "40           0  \n",
       "39           0  \n",
       "51           0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data'\n",
    "col_names = ['id', 'ri', 'na', 'mg', 'al', 'si', 'k', 'ca', 'ba', 'fe', 'glass_type']\n",
    "glass = pd.read_csv(url, names = col_names, index_col = 'id')\n",
    "glass.sort_values('al', inplace = True)\n",
    "\n",
    "# convert the glass type into binary outcome\n",
    "# types 1, 2, 3 are window glass\n",
    "# types 5, 6, 7 are household glass\n",
    "glass['household'] = glass['glass_type'].map({1: 0, 2: 0, 3: 0, 5: 1, 6: 1, 7: 1})\n",
    "glass.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our task is to predict the `household` column using the `al` column. Let's visualize the relationship between the input and output and also train the logsitic regression to see the outcome that it produces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-bf96a6adbd77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlogreg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglass\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'al'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# sklearn doesn't accept 1d-array, convert it to 2d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglass\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'household'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlogreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   4370\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4371\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4372\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4374\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(C = 1e9)\n",
    "X = glass['al'].reshape(-1, 1) # sklearn doesn't accept 1d-array, convert it to 2d\n",
    "y = np.array(glass['household'])\n",
    "logreg.fit(X, y)\n",
    "\n",
    "# predict the probability that each observation belongs to class 1\n",
    "# The first column indicates the predicted probability of class 0, \n",
    "# and the second column indicates the predicted probability of class 1\n",
    "glass['household_pred_prob'] = logreg.predict_proba(X)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot the predicted probability (familiarize yourself with the S-shape)\n",
    "# change default figure and font size\n",
    "plt.rcParams['figure.figsize'] = 8, 6 \n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "plt.scatter(glass['al'], glass['household'])\n",
    "plt.plot(glass['al'], glass['household_pred_prob'])\n",
    "plt.xlabel('al')\n",
    "plt.ylabel('household')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, logistic regression can output the probabilities of observation belonging to a specific class and these probabilities can be converted into class predictions by choosing a cutoff value (e.g. probability higher than 0.5 is classified as class 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **Logistic Regression**, the log-odds of a categorical response being \"true\" (1) is modeled as a linear combination of the features:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\log \\left({p\\over 1-p}\\right) &= w_0 + w_1x_1, ..., w_jx_j \\nonumber \\\\\n",
    "    &= w^Tx \\nonumber\n",
    "\\end{align*}\n",
    "\n",
    "Where:\n",
    "\n",
    "- $w_{0}$ is the intercept term, and $w_1$ to $w_j$ represents the parameters for all the other features (a total of j features).\n",
    "- By convention of we can assume that $x_0 = 1$, so that we can re-write the whole thing using the matrix notation $w^Tx$.\n",
    "\n",
    "This is called the **logit function**. The equation can be re-arranged into the **logistic function**:\n",
    "\n",
    "$$p = \\frac{e^{w^Tx}} {1 + e^{w^Tx}}$$\n",
    "\n",
    "Or in the more commonly seen form:\n",
    "\n",
    "$$h_w(x) = \\frac{1}{ 1 + e^{-w^Tx} }$$ \n",
    "\n",
    "Let's take a look at the plot of the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 263,
       "width": 373
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_values = np.linspace(-5, 5, 100)\n",
    "y_values = [1 / (1 + np.exp(-x)) for x in x_values]\n",
    "plt.plot(x_values, y_values)\n",
    "plt.title('Logsitic Function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **logistic function** has some nice properties. The y-value represents the probability and it is always bounded between 0 and 1, which is want we wanted for probabilities. For an x value of 0 you get a 0.5 probability. Also as you get more positive x value you get a higher probability, on the other hand, a more negative x value results in a lower probability.\n",
    "\n",
    "Toy sample code of how to predict the probability given the data and the weight is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_probability(data, weights):\n",
    "    \"\"\"probability predicted by the logistic regression\"\"\"\n",
    "    score = np.dot(data, weights)\n",
    "    predictions = 1 / (1 + np.exp(-score))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the Intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check logistic regression's coefficient does in fact generate the log-odds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LogisticRegression' object has no attribute 'intercept_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-8004b41e2cab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# convert log-odds to odds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# convert odds to probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlogodds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercept_\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlogreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0modds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogodds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0modds\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0modds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LogisticRegression' object has no attribute 'intercept_'"
     ]
    }
   ],
   "source": [
    "# compute predicted log-odds for al = 2 using the equation\n",
    "# convert log-odds to odds\n",
    "# convert odds to probability\n",
    "logodds = logreg.intercept_ + logreg.coef_[0] * 2\n",
    "odds = np.exp(logodds)\n",
    "prob = odds / (1 + odds)\n",
    "print(prob)\n",
    "\n",
    "logreg.predict_proba(2)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the coefficient for al\n",
    "print('a1', logreg.coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation:** 1 unit increase in `al` is associated with a 4.18 unit increase in the log-odds of the observation being classified as `household 1`. We can confirm that again by doing the calculation ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logodds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-dc62ce77692d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# increasing al by 1 (so that al now becomes 3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# increases the log-odds by 4.18\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlogodds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogodds\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlogreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0modds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogodds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0modds\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0modds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logodds' is not defined"
     ]
    }
   ],
   "source": [
    "# increasing al by 1 (so that al now becomes 3)\n",
    "# increases the log-odds by 4.18\n",
    "logodds = logodds + logreg.coef_[0]\n",
    "odds = np.exp(logodds)\n",
    "prob = odds / (1 + odds)\n",
    "print(prob)\n",
    "\n",
    "logreg.predict_proba(3)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining The Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When utilizing logistic regression, we are trying to learn the $w$ values in order to maximize the probability of correctly classifying our glasses. Let's say someone did give us some $w$ values of the logistic regression model, how would we determine if they were good values or not? What we would hope is that for the household of class 1, the probability values are close to 1 and for the household of class 0 the probability is close to 0.\n",
    "\n",
    "But we don't care about getting the correct probability for just one observation, we want to correctly classify all our observations. If we assume our data are independent and identically distributed (think of it as all of them are treated equally), we can just take the product of all our individually calculated probabilities and that becomes the objective function we want to maximize. So in math:  \n",
    "\n",
    "$$\\prod_{class1}h_w(x)\\prod_{class0}1 - h_w(x)$$ \n",
    "\n",
    "The $\\prod$ symbol means take the product of the $h_w(x)$ for the observations that are classified as that class. You will notice that for observations that are labeled as class 0, we are taking 1 minus the logistic function. That is because we are trying to find a value to maximize, and since observations that are labeled as class 0 should have a probability close to zero, 1 minus the probability should be close to 1. This procedure is also known as the **maximum likelihood estimation** and the following link contains a nice discussion of maximum likelihood using linear regression as an example. [Blog: The Principle of Maximum Likelihood](http://suriyadeepan.github.io/2017-01-22-mle-linear-regression/)\n",
    "\n",
    "Next we will re-write the original cost function as:\n",
    "\n",
    "$$\\ell(w) = \\sum_{i=1}^{N}y_{i}log(h_w(x_{i})) + (1-y_{i})log(1-h_w(x_{i}))$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- We define $y_{i}$ to be 1 when the $i_{th}$ observation is labeled class 1 and 0 when labeled as class 0, then we only compute $h_w(x_{i})$ for observations that are labeled class 1 and $1 - h_w(x_{i})$ for observations that are labeled class 0, which is still the same idea as the original function.\n",
    "- Next we'll transform the original $h_w(x_{i})$ by taking the log. As we'll later see this logarithm transformation will make our cost function more convenient to work with, and because the logarithm is amonotonically increasingfunction, the logarithm of a function achieves itsmaximumvalue at the same points as the function itself. When we take the log, our product across all data points, it becomes a sum. See [log rules](http://www.mathwords.com/l/logarithm_rules.htm) for more details (Hint: log(ab) = log(a) + log(b)).\n",
    "- The $N$ simply represents the total number of the data.\n",
    "\n",
    "Often times you'll also see the notation above be simplified in the form of a maximum likelihood estimator:\n",
    "\n",
    "$$ \\ell(w) = \\sum_{i=1}^{N} log \\big( P( y_i \\mid x_i, w ) \\big) $$\n",
    "\n",
    "The equation above simply denotes the idea that , $\\mathbf{w}$ represents the parameters we would like to estimate the parameters $w$ by maximizing conditional probability of $y_i$ given $x_i$.\n",
    "\n",
    "Now by definition of probability in the logistic regression model: $h_w(x_{i}) = 1 \\big/ 1 + e^{-w^T x_i}$ and $1- h_w(x_{i}) = e^{ -w^T x_i } \\big/ ( 1 + e^{ -w^T x_i } )$. By substituting these expressions into our $\\ell(w)$ equation and simplifying it further we can obtain  a simpler expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\ell(w)\n",
    "&= \\sum_{i=1}^{N}y_{i}log(h_w(x_{i})) + (1-y_{i})log(1-h_w(x_{i})) \\nonumber \\\\\n",
    "&= \\sum_{i=1}^{N} y_{i} log( \\frac{1}{ 1 + e^{ -w^T x_i } } ) + ( 1 - y_{i} )\n",
    "log( \\frac{ e^{ -w^T x_i } }{ 1 + e^{ -w^T x_i } } ) \\nonumber \\\\\n",
    "&= \\sum_{i=1}^{N} -y_{i} log( 1 + e^{ -w^T x_i } ) + ( 1 - y_{i} )\n",
    "( -w^T x_i - log( 1 + e^{ -w^T x_i } ) ) \\nonumber \\\\\n",
    "&= \\sum_{i=1}^{N} ( y_{i} - 1 ) ( w^T x_i ) - log( 1 + e^{ -w^T x_i } ) \\nonumber\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the formula above to compute the log likelihood for the entire dataset, which is used to assess the convergence of the algorithm. Toy code provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_log_likelihood(data, label, weights):\n",
    "    \"\"\"\n",
    "    the function uses a simple check to prevent overflow problem, \n",
    "    where numbers gets too large to represent and is converted to inf\n",
    "    an example of overflow is provided below, when this problem occurs,\n",
    "    simply use the original score (without taking the exponential)\n",
    "    \n",
    "    scores = np.array( [ -10000, 200, 300 ] )\n",
    "    logexp = np.log( 1 + np.exp(-scores) )\n",
    "    logexp  \n",
    "    \"\"\"\n",
    "    scores = np.dot(data, weights)\n",
    "    logexp = np.log(1 + np.exp(-scores))\n",
    "    \n",
    "    # simple check to prevent overflow\n",
    "    mask = np.isinf(logexp)\n",
    "    logexp[mask] = -scores[mask]\n",
    "    \n",
    "    log_likelihood = np.sum((label - 1) * scores - logexp) / data.shape[0]    \n",
    "    return log_likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** We made one tiny modification to the log likelihood function We added a ${1/N}$ term which averages the log likelihood across all data points. The ${1/N}$ term will make it easier for us to compare stochastic gradient ascent with batch gradient ascent later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we obtain the formula to assess our algorithm, we'll dive into the meat of the algorithm, which is to derive the gradient for the formula (the derivative of the formula with respect to each coefficient):\n",
    "\n",
    "$$\\ell(w) = \\sum_{i=1}^{N} ( y_{i} - 1 ) ( w^T x_i ) - log( 1 + e^{ -w^T x_i } )$$\n",
    "\n",
    "And it turns out the derivative of log likelihood with respect to to a single coefficient $w_j$ is as follows (the form is the same for all coefficients):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\ell(w)}{\\partial w_j} = \\sum_{i=1}^N (x_{ij})\\left( y_i - \\frac{1}{ 1 + e^{-w^Tx_i} } \\right )\n",
    "$$\n",
    "\n",
    "To compute it, you simply need the following two terms:\n",
    "\n",
    "- $\\left( y_i - \\frac{1}{ 1 + e^{-w^Tx_i} } \\right )$ is the vector containing the difference between the predicted probability and the original label.\n",
    "- $x_{ij}$ is the vector containing the $j_{th}$ feature's value.\n",
    " \n",
    "For a step by step derivation, consider going through the following link. [Blog: Maximum likelihood and gradient descent demonstration](https://zlatankr.github.io/posts/2017/03/06/mle-gradient-descent), it uses a slightly different notation, but the walkthrough should still be pretty clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic/Mini-batch Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with computing the gradient (or so called batched gradient) is the term $\\sum_{i=1}^{N}$. This means that we must sum the contributions over all the data points to calculate the gradient, and this can be problematic if the dataset we're studying is extremely large. Thus, in stochastic gradient, we can use a single point as an approximation to the gradient:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\ell_i(w)}{\\partial w_j} = (x_{ij})\\left( y_i - \\frac{1}{ 1 + e^{-w^Tx_i} } \\right )\n",
    "$$\n",
    "\n",
    "**Note1:** Because the **Stochastic Gradient** algorithm uses each row of data in turn to update the gradient, if our data has some sort of implicit ordering, this will negatively affect the convergence of the algorithm. At an extreme, what if we had the data sorted so that all positive reviews came before negative reviews?  In that case, even if most reviews are negative, we might converge on an answer of +1 because we never get to see the other data. To avoid this, one practical trick is to shuffle the data before we begin so the rows are in random order.\n",
    "\n",
    "**Note2:** Stochastic gradient compute the gradient using only 1 data point to update the the parameters, while batch gradient uses all $N$ data points. An alternative to these two extremes is a simple change that allows us to use a **mini-batch** of $B \\leq N$ data points to calculate the gradient. This simple approach is faster than batch gradient but less noisy than stochastic gradient that uses only 1 data point. Given a mini-batch (or a set of data points) $\\mathbf{x}_{i}, \\mathbf{x}_{i+1} \\ldots \\mathbf{x}_{i+B}$, the gradient function for this mini-batch of data points is given by:\n",
    "\n",
    "$$\n",
    "\\sum_{s = i}^{i+B} \\frac{\\partial\\ell_s(w)}{\\partial w_j} = \\frac{1}{B} \\sum_{s = i}^{i+B} (x_{sj})\\left( y_i - \\frac{1}{ 1 + e^{-w^Tx_i} } \\right )\n",
    "$$\n",
    "\n",
    "Here, the $\\frac{1}{B}$ means that we are normalizing the gradient update rule by the batch size $B$. In other words, we update the coefficients using the **average gradient over data points** (instead of using a pure summation). By using the average gradient, we ensure that the magnitude of the gradient is approximately the same for all batch sizes. This way, we can more easily compare various batch sizes and study the effect it has on the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall our task is to find the optimal value for each individual weight to lower the cost. This requires taking the partial derivative of the cost/error function with respect to a single weight, and then running gradient descent for each individual weight to update them. Thus, for any individual weight $w_j$, we'll compute the following:\n",
    "\n",
    "$$ w_j^{(t + 1)} = w_j^{(t)} + \\alpha * \\sum_{s = i}^{i+B} \\frac{\\partial\\ell_s(w)}{\\partial w_j}$$ \n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\alpha$ denotes the the learning rate or so called step size, in other places you'll see it denoted as $\\eta$.\n",
    "- $w_j^{(t)}$ denotes the weight of the $j_{th}$ feature at iteration $t$.\n",
    "\n",
    "And we'll do this iteratively for each weight, many times, until the whole network's cost function is minimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the code together into one cell\n",
    "\n",
    "def predict_probability(data, weights):\n",
    "    \"\"\"probability predicted by the logistic regression\"\"\"\n",
    "    score = np.dot(data, weights)\n",
    "    predictions = 1 / (1 + np.exp(-score))\n",
    "    return predictions\n",
    "\n",
    "def compute_avg_log_likelihood(data, label, weights):\n",
    "    \"\"\"\n",
    "    the function uses a simple check to prevent overflow problem, \n",
    "    where numbers gets too large to represent and is converted to inf\n",
    "    an example of overflow is provided below, when this problem occurs,\n",
    "    simply use the original score (without taking the exponential)\n",
    "    \n",
    "    scores = np.array([-10000, 200, 300])\n",
    "    logexp = np.log(1 + np.exp(-scores))\n",
    "    logexp  \n",
    "    \"\"\"\n",
    "    scores = np.dot(data, weights)\n",
    "    logexp = np.log(1 + np.exp(-scores))\n",
    "    \n",
    "    # simple check to prevent overflow\n",
    "    mask = np.isinf(logexp)\n",
    "    logexp[mask] = -scores[mask]\n",
    "    \n",
    "    log_likelihood = np.sum((label - 1) * scores - logexp) / data.shape[0]    \n",
    "    return log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(data, label, step_size, batch_size, max_iter):\n",
    "     \n",
    "    # weights of the model are initialized as zero\n",
    "    data_num = data.shape[0]\n",
    "    feature_num = data.shape[1]   \n",
    "    weights = np.zeros(data.shape[1])\n",
    "    \n",
    "    # `i` keeps track of the starting index of current batch\n",
    "    # and shuffle the data before starting\n",
    "    i = 0 \n",
    "    permutation = np.random.permutation(data_num)\n",
    "    data, label = data[permutation], label[permutation]\n",
    "      \n",
    "    # do a linear scan over data, for each iteration update the weight using \n",
    "    # batches of data, and store the log likelihood record to visualize convergence\n",
    "    log_likelihood_record = []\n",
    "    for _ in range(max_iter):\n",
    "        \n",
    "        # extract the batched data and label use it to compute\n",
    "        # the predicted probability using the current weight and the errors\n",
    "        batch = slice(i, i + batch_size)\n",
    "        batch_data, batch_label = data[batch], label[batch]\n",
    "        predictions = predict_probability(batch_data, weights)\n",
    "        errors = batch_label - predictions\n",
    "        \n",
    "        # loop over each coefficient to compute the derivative and update the weight\n",
    "        for j in range(feature_num): \n",
    "            derivative = np.dot(errors, batch_data[:, j])      \n",
    "            weights[j] += step_size * derivative / batch_size\n",
    "        \n",
    "        # track whether log likelihood is increasing after\n",
    "        # each weight update\n",
    "        log_likelihood = compute_avg_log_likelihood(\n",
    "            data = batch_data, \n",
    "            label = batch_label,\n",
    "            weights = weights\n",
    "        )\n",
    "        log_likelihood_record.append(log_likelihood)\n",
    "               \n",
    "        # update starting index of for the batches\n",
    "        # and if we made a complete pass over data, shuffle again \n",
    "        # and refresh the index that keeps track of the batch\n",
    "        i += batch_size\n",
    "        if i + batch_size > data_num:\n",
    "            permutation = np.random.permutation(data_num)\n",
    "            data, label = data[permutation], label[permutation]\n",
    "            i = 0\n",
    "                \n",
    "    # We return the list of log likelihoods for plotting purposes.\n",
    "    return weights, log_likelihood_record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Result and Convergence Behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the logistic regression code that we've implemented and compare the predicted auc score with scikit-learn's implementation. This only serves to check that the predicted results are similar and that our toy code is correctly implemented. Then we'll also explore the convergence difference between batch gradient descent and stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-3186cf779859>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# every good open-source library does not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# require this additional step from the user\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# using our logistic regression code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "# manually append the coefficient term,\n",
    "# every good open-source library does not\n",
    "# require this additional step from the user\n",
    "data = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "# using our logistic regression code\n",
    "weights_batch, log_likelihood_batch = logistic_regression(\n",
    "    data = data,\n",
    "    label = np.array(y),\n",
    "    step_size = 5e-1, \n",
    "    batch_size = X.shape[0],  # batch gradient descent\n",
    "    max_iter = 200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare both logistic regression's auc score\n",
    "logreg = LogisticRegression(C = 1e9)\n",
    "logreg.fit(X, y)\n",
    "pred_prob = logreg.predict_proba(X)[:, 1]\n",
    "\n",
    "proba = predict_probability(data, weights_batch)\n",
    "\n",
    "# check that the auc score is similar\n",
    "auc1 = metrics.roc_auc_score(y, pred_prob)\n",
    "auc2 = metrics.roc_auc_score(y, proba)\n",
    "print('auc', auc1, auc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_sgd, log_likelihood_sgd = logistic_regression(\n",
    "    data = data,\n",
    "    label = y,\n",
    "    step_size = 5e-1, \n",
    "    batch_size = 30,  # stochastic gradient descent\n",
    "    max_iter = 200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_minibatch, log_likelihood_minibatch = logistic_regression(\n",
    "    data = data,\n",
    "    label = y,\n",
    "    step_size = 5e-1, \n",
    "    batch_size = 100,  # mini-batch gradient descent\n",
    "    max_iter = 200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 7))\n",
    "plt.plot(log_likelihood_sgd, label = 'stochastic gradient descent')\n",
    "plt.plot(log_likelihood_batch, label = 'batch gradient descent')\n",
    "plt.plot(log_likelihood_minibatch, label = 'mini-batch gradient descent')\n",
    "plt.legend(loc = 'best')\n",
    "plt.xlabel('# of iterations')\n",
    "plt.ylabel('Average log likelihood')\n",
    "plt.title('Convergence Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the convergence plot above, we can see that the it's a good idea to use mini-batch gradient descent since it strikes a good balance between batch gradient, which convergences steadily but can be computationly too expensive when the dataset is too large, and stochastic gradient, which is faster to train, but the result can be too noisy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros and Cons of Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll end this notebook listing out some pros and cons of this method.\n",
    "\n",
    "**Pros:**\n",
    "\n",
    "- Highly interpretable (if you remember how).\n",
    "- Model training and prediction are fast. Thus can be desirable in large-scale applications when we're dealing with millions of parameters.\n",
    "- Almost no parameter tuning is required (excluding regularization).\n",
    "- Outputs well-calibrated predicted probabilities.\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "- Presumes a linear relationship between the features\n",
    "- Performance is (generally) not competitive with the best supervised learning methods.\n",
    "- Can't automatically learn feature interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Notebook: Logistic Regression](http://nbviewer.jupyter.org/github/justmarkham/DAT8/blob/master/notebooks/12_logistic_regression.ipynb)\n",
    "- [Coursersa: Washington Classification](https://www.coursera.org/learn/ml-classification)\n",
    "- [Blog: Maximum likelihood and gradient descent demonstration](https://zlatankr.github.io/posts/2017/03/06/mle-gradient-descent)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {
    "height": "277px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
