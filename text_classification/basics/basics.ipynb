{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "@import url('http://fonts.googleapis.com/css?family=Source+Code+Pro');\n",
       "@import url('http://fonts.googleapis.com/css?family=Vollkorn');\n",
       "@import url('http://fonts.googleapis.com/css?family=Arimo');\n",
       "@import url('http://fonts.googleapis.com/css?family=Fira_sans');\n",
       "    \n",
       "    div.cell {\n",
       "        width: 1000px;\n",
       "        margin-left: 0% !important;\n",
       "        margin-right: auto;\n",
       "    }\n",
       "    div.text_cell code {\n",
       "        background: transparent;\n",
       "        color: #000000;\n",
       "        font-weight: 600;\n",
       "        font-size: 12pt;\n",
       "        font-style: bold;\n",
       "        font-family:  'Source Code Pro', Consolas, monocco, monospace;\n",
       "    }\n",
       "    h1 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "\t}\n",
       "\t\n",
       "    div.input_area {\n",
       "        background: #F6F6F9;\n",
       "        border: 1px solid #586e75;\n",
       "    }\n",
       "\n",
       "    .text_cell_render h1 {\n",
       "        font-weight: 200;\n",
       "        font-size: 30pt;\n",
       "        line-height: 100%;\n",
       "        color:#c76c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 1em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    } \n",
       "    h2 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "        text-align: left;\n",
       "    }\n",
       "    .text_cell_render h2 {\n",
       "        font-weight: 200;\n",
       "        font-size: 16pt;\n",
       "        font-style: italic;\n",
       "        line-height: 100%;\n",
       "        color:#c76c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 1.5em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    } \n",
       "    h3 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h3 {\n",
       "        font-weight: 200;\n",
       "        font-size: 14pt;\n",
       "        line-height: 100%;\n",
       "        color:#d77c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 2em;\n",
       "        display: block;\n",
       "        white-space: wrap;\n",
       "        text-align: left;\n",
       "    }\n",
       "    h4 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h4 {\n",
       "        font-weight: 100;\n",
       "        font-size: 14pt;\n",
       "        color:#d77c0c;\n",
       "        margin-bottom: 0.5em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "        white-space: nowrap;\n",
       "    }\n",
       "    h5 {\n",
       "        font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "    }\n",
       "    .text_cell_render h5 {\n",
       "        font-weight: 200;\n",
       "        font-style: normal;\n",
       "        color: #1d3b84;\n",
       "        font-size: 16pt;\n",
       "        margin-bottom: 0em;\n",
       "        margin-top: 0.5em;\n",
       "        display: block;\n",
       "        white-space: nowrap;\n",
       "    }\n",
       "    div.text_cell_render{\n",
       "        font-family: 'Fira sans', verdana,arial,sans-serif;\n",
       "        line-height: 125%;\n",
       "        font-size: 115%;\n",
       "        text-align:justify;\n",
       "        text-justify:inter-word;\n",
       "    }\n",
       "    div.output_wrapper{\n",
       "        margin-top:0.2em;\n",
       "        margin-bottom:0.2em;\n",
       "    }\n",
       "\n",
       "    code{\n",
       "      font-size: 70%;\n",
       "    }\n",
       "    .rendered_html code{\n",
       "    background-color: transparent;\n",
       "    }\n",
       "    ul{\n",
       "        margin: 2em;\n",
       "    }\n",
       "    ul li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.5em; \n",
       "    }\n",
       "    ul li li{\n",
       "        padding-left: 0.2em; \n",
       "        margin-bottom: 0.2em; \n",
       "        margin-top: 0.2em; \n",
       "    }\n",
       "    ol{\n",
       "        margin: 2em;\n",
       "    }\n",
       "    ol li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.5em; \n",
       "    }\n",
       "    ul li{\n",
       "        padding-left: 0.5em; \n",
       "        margin-bottom: 0.5em; \n",
       "        margin-top: 0.2em; \n",
       "    }\n",
       "    a:link{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    a:visited{\n",
       "       font-weight: bold;\n",
       "       color: #1d3b84;\n",
       "    }\n",
       "    a:hover{\n",
       "       font-weight: bold;\n",
       "       color: #1d3b84;\n",
       "    }\n",
       "    a:focus{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    a:active{\n",
       "       font-weight: bold;\n",
       "       color:#447adb;\n",
       "    }\n",
       "    .rendered_html :link {\n",
       "       text-decoration: underline; \n",
       "    }\n",
       "    .rendered_html :hover {\n",
       "       text-decoration: none; \n",
       "    }\n",
       "    .rendered_html :visited {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .rendered_html :focus {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .rendered_html :active {\n",
       "      text-decoration: none;\n",
       "    }\n",
       "    .warning{\n",
       "        color: rgb( 240, 20, 20 )\n",
       "    } \n",
       "    hr {\n",
       "      color: #f3f3f3;\n",
       "      background-color: #f3f3f3;\n",
       "      height: 1px;\n",
       "    }\n",
       "    blockquote{\n",
       "      display:block;\n",
       "      background: #fcfcfc;\n",
       "      border-left: 5px solid #c76c0c;\n",
       "      font-family: 'Open sans',verdana,arial,sans-serif;\n",
       "      width:680px;\n",
       "      padding: 10px 10px 10px 10px;\n",
       "      text-align:justify;\n",
       "      text-justify:inter-word;\n",
       "      }\n",
       "      blockquote p {\n",
       "        margin-bottom: 0;\n",
       "        line-height: 125%;\n",
       "        font-size: 100%;\n",
       "      }\n",
       "</style>\n",
       "<script>\n",
       "    MathJax.Hub.Config({\n",
       "                        TeX: {\n",
       "                           extensions: [\"AMSmath.js\"]\n",
       "                           },\n",
       "                tex2jax: {\n",
       "                    inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n",
       "                    displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ]\n",
       "                },\n",
       "                displayAlign: 'center', // Change this to 'center' to center equations.\n",
       "                \"HTML-CSS\": {\n",
       "                    scale:100,\n",
       "                        availableFonts: [],\n",
       "                        preferredFont:null,\n",
       "                        webFont: \"TeX\",\n",
       "                    styles: {'.MathJax_Display': {\"margin\": 4}}\n",
       "                }\n",
       "        });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code for loading the format for the notebook\n",
    "import os\n",
    "\n",
    "# path : store the current path to convert back to it later\n",
    "path = os.getcwd()\n",
    "os.chdir( os.path.join('..', '..', 'notebook_format') )\n",
    "from formats import load_style\n",
    "load_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ethen 2017-06-22 13:39:10 \n",
      "\n",
      "CPython 3.5.2\n",
      "IPython 5.4.1\n",
      "\n",
      "numpy 1.12.1\n",
      "pandas 0.20.2\n",
      "matplotlib 2.0.2\n",
      "sklearn 0.18.1\n"
     ]
    }
   ],
   "source": [
    "os.chdir(path)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. magic for inline plot\n",
    "# 2. magic to print version\n",
    "# 3. magic so that the notebook will reload external python modules\n",
    "%matplotlib inline\n",
    "%load_ext watermark\n",
    "%load_ext autoreload \n",
    "%autoreload 2\n",
    "\n",
    "%watermark -a 'Ethen' -d -t -v -p numpy,pandas,matplotlib,sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Machine Learning with scikit-learn\n",
    "\n",
    "## Agenda\n",
    "\n",
    "1. Model building in scikit-learn (quick refresher).\n",
    "2. Representing text as numerical data.\n",
    "3. Reading a text-based dataset into pandas and vectorizing.\n",
    "4. Building and evaluating a model (naive-bayes).\n",
    "5. Building and evaluating another model (logisitic regression).\n",
    "6. Examining a model for further insight.\n",
    "7. Tuning the vectorizer (commonly used parameters).\n",
    "\n",
    "## Part 1: Model building in scikit-learn (refresher)\n",
    "\n",
    "If you're already familiar with model-building in different packages, here's a quick refresher on how to train a simple classification model with scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the iris dataset as an example\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# store the feature matrix (X) and response vector (y)\n",
    "# by convention X is capitialized simply because it's a two-dimension matrix\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"Features\"** are also known as predictors, inputs, or attributes. The **\"response\"** is also known as the target, label, or output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "# check the shapes of X and y\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"Observations\"** are also known as samples, instances, or records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2\n",
       "2                4.7               3.2                1.3               0.2\n",
       "3                4.6               3.1                1.5               0.2\n",
       "4                5.0               3.6                1.4               0.2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the first 5 rows of the feature matrix (including the feature names)\n",
    "pd.DataFrame(X, columns = iris.feature_names).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "# examine the response vector\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to **build a model**, the features must be **numeric**, and every observation must have the **same features in the same order**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the class\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# instantiate the model (with the default parameters)\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# fit the model with data (occurs in-place)\n",
    "# learning the relationship between X and y\n",
    "knn.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to **make a prediction**, the new observation must have the **same features in the same order as the training observations**, both in number and meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict the response for a new observation\n",
    "knn.predict([[ 3, 5, 4, 2 ]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Representing text as numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# example text for model training (SMS messages)\n",
    "simple_train = ['call you tonight', 'Call me a cab', 'please call me... PLEASE!']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the [scikit-learn documentation](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction):\n",
    "\n",
    "> Text Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect **numerical feature vectors with a fixed size** rather than the **raw text documents with variable length**.\n",
    "\n",
    "Thus, when working with text, we will use [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to \"convert text into a matrix of token counts\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import and instantiate CountVectorizer (with the default parameters)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x6 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 9 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# learn the 'vocabulary' of the training data and transform \n",
    "# it into a 'document-term matrix'. Unlike models, for any \n",
    "# feature extraction or data-preprocessing toolkit, you use transform \n",
    "# instead of prediction, since it's taking some data and \"transforming\" it,\n",
    "# fit_transform is just a shortcut for calling .fit and .transform separately\n",
    "simple_train_dtm = vect.fit_transform(simple_train)\n",
    "simple_train_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cab', 'call', 'me', 'please', 'tonight', 'you']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the fitted vocabulary\n",
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, with the default parameters:\n",
    "\n",
    "- Single character like `a` will be removed.\n",
    "- Punctuations have been removed.\n",
    "- Words have been converted to lower cases.\n",
    "- The vocabulary has no duplicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 1, 1],\n",
       "       [1, 1, 1, 0, 0, 0],\n",
       "       [0, 1, 1, 2, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert sparse matrix to a dense matrix \n",
    "simple_train_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    0     1   0       0        1    1\n",
       "1    1     1   1       0        0    0\n",
       "2    0     1   1       2        0    0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the vocabulary and document-term matrix together.\n",
    "# 3 * 6 matrix, because there're three documents and six tokens that were learned\n",
    "# each number represents the counts for each token in each document\n",
    "pd.DataFrame(simple_train_dtm.toarray(), columns = vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the [scikit-learn documentation](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction):\n",
    "\n",
    "> In this scheme, features and samples are defined as follows:\n",
    "\n",
    "> - Each individual token occurrence frequency (normalized or not) is treated as a **feature**.\n",
    "> - The vector of all the token frequencies for a given document is considered a multivariate **sample**.\n",
    "\n",
    "> A **corpus of documents** can thus be represented by a matrix with **one row per document** and **one column per token** (e.g. word) occurring in the corpus.\n",
    "\n",
    "> We call **vectorization** the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the **Bag of Words** or \"Bag of n-grams\" representation. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document.\n",
    "\n",
    "### Side Note On Sparse Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 1)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 1)\t1\n",
      "  (2, 3)\t2\n",
      "  (2, 2)\t1\n",
      "  (2, 1)\t1\n"
     ]
    }
   ],
   "source": [
    "# check the type of the document-term matrix\n",
    "print(type(simple_train_dtm))\n",
    "\n",
    "# examine the sparse matrix contents\n",
    "# represented coordinates, and the values at that coordinates\n",
    "print(simple_train_dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the [scikit-learn documentation](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction):\n",
    "\n",
    "> As most documents will typically use a very small subset of the words used in the corpus, the resulting matrix will have **many feature values that are zeros** (typically more than 99% of them).\n",
    "\n",
    "> For instance, a collection of 10,000 short text documents (such as emails) will use a vocabulary with a size in the order of 100,000 unique words in total while each document will use 100 to 1000 unique words individually.\n",
    "\n",
    "> In order to be able to **store such a matrix in memory** but also to **speed up operations**, implementations will typically use a **sparse representation** such as the implementations available in the `scipy.sparse` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# example text for model testing\n",
    "simple_test = [\"please don't call me\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to **make a prediction**, the new observation must have the **same features as the training observations**, both in number and meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform testing data into a document-term matrix,\n",
    "# using the existing vocabulary from the training data\n",
    "simple_test_dtm = vect.transform(simple_test)\n",
    "simple_test_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    0     1   1       1        0    0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the vocabulary and document-term matrix together\n",
    "pd.DataFrame( simple_test_dtm.toarray(), columns = vect.get_feature_names() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary:**\n",
    "\n",
    "- `vect.fit(train)` **learns the vocabulary** of the training data.\n",
    "- `vect.transform(train)` uses the **fitted vocabulary** to build a document-term matrix from the training data. Or just `vect.fit_transform(train)` to combine the two steps into one.\n",
    "- `vect.transform(test)` uses the **fitted vocabulary** to build a document-term matrix from the testing data. Note that it **ignores tokens** it hasn't seen before, this is reasonable due to the fact that the word does not exist in the training data, thus the model doesn't know anything about the relationship between the word and the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Reading a text-based dataset into pandas and vectorizing\n",
    "\n",
    "This is a text data that has been labeled as spam and ham (non-spam), our goal is to see if we can correctly classify the labels using text messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimension: (5572, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "5  spam  FreeMsg Hey there darling it's been 3 week's n...\n",
       "6   ham  Even my brother is not like to speak with me. ...\n",
       "7   ham  As per your request 'Melle Melle (Oru Minnamin...\n",
       "8  spam  WINNER!! As a valued network customer you have...\n",
       "9  spam  Had your mobile 11 months or more? U R entitle..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the data's shape and first 10 rows\n",
    "sms = pd.read_table('sms.tsv', header = None, names = ['label', 'message'])\n",
    "print('dimension:', sms.shape)\n",
    "sms.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# alternative: read file into pandas from a URL\n",
    "# url = 'https://raw.githubusercontent.com/justmarkham/pycon-2016-tutorial/master/data/sms.tsv'\n",
    "# sms = pd.read_table( url, header = None, names = ['label', 'message'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the class distribution\n",
    "sms['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  label_num\n",
       "0   ham  Go until jurong point, crazy.. Available only ...          0\n",
       "1   ham                      Ok lar... Joking wif u oni...          0\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...          1\n",
       "3   ham  U dun say so early hor... U c already then say...          0\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...          0\n",
       "5  spam  FreeMsg Hey there darling it's been 3 week's n...          1\n",
       "6   ham  Even my brother is not like to speak with me. ...          0\n",
       "7   ham  As per your request 'Melle Melle (Oru Minnamin...          0\n",
       "8  spam  WINNER!! As a valued network customer you have...          1\n",
       "9  spam  Had your mobile 11 months or more? U R entitle...          1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert label to a numerical variable\n",
    "sms['label_num'] = sms['label'].map({'ham': 0, 'spam': 1})\n",
    "sms.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572,)\n",
      "(5572,)\n"
     ]
    }
   ],
   "source": [
    "# how to define X and y (from the SMS data) for use with COUNTVECTORIZER\n",
    "# COUNTVECTORIZER accepts one-dimension data\n",
    "X = sms['message']\n",
    "y = sms['label_num']\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4179,)\n",
      "(1393,)\n"
     ]
    }
   ],
   "source": [
    "# split X and y into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instantiate the vectorizer\n",
    "vect = CountVectorizer()\n",
    "\n",
    "# learn training data vocabulary, then use it to create a document-term matrix\n",
    "# equivalently: combine fit and transform into a single step\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "\n",
    "# transform testing data (using fitted vocabulary) into a document-term matrix\n",
    "X_test_dtm = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Building and evaluating a model\n",
    "\n",
    "Algorithms are all treated as black box.\n",
    "\n",
    "We will use [multinomial Naive Bayes](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html), Naive Bayes class algorithms are extremely fast and it's usually the go-to method for doing classification on text data.:\n",
    "\n",
    "> The multinomial Naive Bayes classifier is suitable for classification with **discrete features** (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 3.81 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import and instantiate a Multinomial Naive Bayes model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "nb = MultinomialNB()\n",
    "\n",
    "# train the model using X_train_dtm (timing it with an IPython \"magic command\")\n",
    "%time \n",
    "nb.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.988513998564\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.98664310005369615"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make class predictions for X_test_dtm\n",
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "\n",
    "# calculate accuracy of class predictions\n",
    "from sklearn import metrics\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))\n",
    "\n",
    "# print the confusion matrix\n",
    "# metrics.confusion_matrix(y_test, y_pred_class)\n",
    "\n",
    "# calculate predicted probabilities for X_test_dtm\n",
    "# extract the probability that each observation belongs to class 1,\n",
    "# though the downside of naive bayes is that the probabilities are poorly calibrated\n",
    "y_pred_prob = nb.predict_proba(X_test_dtm)[:, 1]\n",
    "\n",
    "# calculate AUC\n",
    "metrics.roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Building and evaluating another model\n",
    "\n",
    "We will compare multinomial Naive Bayes with [logistic regression](http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression):\n",
    "\n",
    "> Logistic regression, despite its name, is a **linear model for classification** rather than regression. Logistic regression is also known in the literature as logit regression, maximum-entropy classification (MaxEnt) or the log-linear classifier. In this model, the probabilities describing the possible outcomes of a single trial are modeled using a logistic function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 5.01 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import and instantiate a logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# train the model using X_train_dtm\n",
    "# it accepts both sparse and dense arrays\n",
    "%time \n",
    "logreg.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.987796123475\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.99368176123143015"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make class predictions for X_test_dtm\n",
    "y_pred_class = logreg.predict(X_test_dtm)\n",
    "\n",
    "# calculate accuracy\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))\n",
    "\n",
    "# calculate predicted probabilities for X_test_dtm (well calibrated)\n",
    "# it's a good model if you care about predicting the classified probability\n",
    "y_pred_prob = logreg.predict_proba(X_test_dtm)[:, 1]\n",
    "\n",
    "# calculate auc\n",
    "metrics.roc_auc_score(y_test, y_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Examining a model for further insight\n",
    "\n",
    "After building the model, it's a good practice to look at examples of where your model got it wrong, and think about what features you can add to \"maybe\" improve the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false positives\n",
      "2340    Cheers for the message Zogtorius. Ive been st...\n",
      "Name: message, dtype: object\n",
      "\n",
      "false negatives\n",
      "1777                    Call FREEPHONE 0800 542 0578 now!\n",
      "763     Urgent Ur £500 guaranteed award is still uncla...\n",
      "3132    LookAtMe!: Thanks for your purchase of a video...\n",
      "1045    We know someone who you know that fancies you....\n",
      "684     Hi I'm sue. I am 20 years old and work as a la...\n",
      "4073    Loans for any purpose even if you have Bad Cre...\n",
      "1875    Would you like to see my XXX pics they are so ...\n",
      "4298    thesmszone.com lets you send free anonymous an...\n",
      "4394    RECPT 1/3. You have ordered a Ringtone. Your o...\n",
      "4949    Hi this is Amy, we will be sending you a free ...\n",
      "761     Romantic Paris. 2 nights, 2 flights from £79 B...\n",
      "3991    (Bank of Granite issues Strong-Buy) EXPLOSIVE ...\n",
      "2821    INTERFLORA - It's not too late to order Inter...\n",
      "2863    Adult 18 Content Your video will be with you s...\n",
      "2247    Hi ya babe x u 4goten bout me?' scammers getti...\n",
      "4514    Money i have won wining number 946 wot do i do...\n",
      "Name: message, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"LookAtMe!: Thanks for your purchase of a video clip from LookAtMe!, you've been charged 35p. Think you can do better? Why not send a video in a MMSto 32323.\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print message text for the false positives (ham incorrectly classified as spam)\n",
    "print('false positives')\n",
    "print(X_test[(y_pred_class == 1) & (y_test == 0)])\n",
    "# equivalent way to performing the operation\n",
    "# print(X_test[y_pred_class > y_test])\n",
    "\n",
    "\n",
    "# print message text for the false negatives (spam incorrectly classified as ham)\n",
    "print('\\nfalse negatives')\n",
    "print(X_test[y_pred_class < y_test])\n",
    "\n",
    "# example false negative\n",
    "X_test[3132]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at these incorrectly classified messages, you might think does the message's length have something to do with it being a spam/ham. \n",
    "\n",
    "Next, we will examine the our trained Naive Bayes model to calculate the approximate **\"spamminess\" of each token** to see which word appears more often in spam messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.   0.   0. ...,   1.   1.   1.]\n",
      " [  5.  23.   2. ...,   0.   0.   0.]]\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes counts the number of times each token appears in each class, where\n",
    "# rows represent classes, columns represent tokens\n",
    "# and the trailing _ is just the convention that's used to denote attributes that\n",
    "# are learned during the model fitting process\n",
    "print(nb.feature_count_)\n",
    "\n",
    "# extract the number of times each token appears across all HAM and SPAM messages\n",
    "ham_token_count = nb.feature_count_[0, :]\n",
    "spam_token_count = nb.feature_count_[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7456\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>00</th>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000</th>\n",
       "      <td>0.0</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>008704050406</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0121</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01223585236</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ham  spam\n",
       "token                  \n",
       "00            0.0   5.0\n",
       "000           0.0  23.0\n",
       "008704050406  0.0   2.0\n",
       "0121          0.0   1.0\n",
       "01223585236   0.0   1.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# store the vocabulary of X_train\n",
    "X_train_tokens = vect.get_feature_names()\n",
    "print(len(X_train_tokens))\n",
    "\n",
    "# examine the first or last 50 tokens if you wish\n",
    "# print(X_train_tokens[0:50])\n",
    "# print(X_train_tokens[-50:])\n",
    "\n",
    "# create a DataFrame of tokens with their separate ham and spam counts\n",
    "tokens = pd.DataFrame({\n",
    "    'token': X_train_tokens, \n",
    "    'ham': ham_token_count, \n",
    "    'spam': spam_token_count\n",
    "}).set_index('token')\n",
    "tokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>very</th>\n",
       "      <td>64.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nasty</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>villa</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beloved</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>textoperator</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ham  spam\n",
       "token                   \n",
       "very          64.0   2.0\n",
       "nasty          1.0   1.0\n",
       "villa          0.0   1.0\n",
       "beloved        1.0   0.0\n",
       "textoperator   0.0   2.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine 5 random DataFrame rows\n",
    "tokens.sample(5, random_state = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3617.,   562.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes counts the number of observations in each class\n",
    "nb.class_count_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can calculate the relative \"spamminess\" of each token, we need to avoid **dividing by zero** and account for the **class imbalance**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "      <th>spam_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>very</th>\n",
       "      <td>0.017971</td>\n",
       "      <td>0.005338</td>\n",
       "      <td>0.297044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nasty</th>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.003559</td>\n",
       "      <td>6.435943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>villa</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.003559</td>\n",
       "      <td>12.871886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beloved</th>\n",
       "      <td>0.000553</td>\n",
       "      <td>0.001779</td>\n",
       "      <td>3.217972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>textoperator</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.005338</td>\n",
       "      <td>19.307829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ham      spam  spam_ratio\n",
       "token                                       \n",
       "very          0.017971  0.005338    0.297044\n",
       "nasty         0.000553  0.003559    6.435943\n",
       "villa         0.000276  0.003559   12.871886\n",
       "beloved       0.000553  0.001779    3.217972\n",
       "textoperator  0.000276  0.005338   19.307829"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add 1 to ham and spam counts to avoid dividing by 0\n",
    "# convert the ham and spam counts into ratio frequencies\n",
    "tokens['ham']  = (tokens['ham']  + 1) / nb.class_count_[0]\n",
    "tokens['spam'] = (tokens['spam'] + 1) / nb.class_count_[1]\n",
    "\n",
    "# calculate the conceptual ratio of spam-to-ham for each token\n",
    "tokens['spam_ratio'] = tokens['spam'] / tokens['ham']\n",
    "tokens.sample(5, random_state = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "      <th>spam_ratio</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>claim</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.158363</td>\n",
       "      <td>572.798932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prize</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.135231</td>\n",
       "      <td>489.131673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150p</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.087189</td>\n",
       "      <td>315.361210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tone</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.085409</td>\n",
       "      <td>308.925267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guaranteed</th>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.076512</td>\n",
       "      <td>276.745552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ham      spam  spam_ratio\n",
       "token                                     \n",
       "claim       0.000276  0.158363  572.798932\n",
       "prize       0.000276  0.135231  489.131673\n",
       "150p        0.000276  0.087189  315.361210\n",
       "tone        0.000276  0.085409  308.925267\n",
       "guaranteed  0.000276  0.076512  276.745552"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the DataFrame sorted by spam_ratio\n",
    "# only interpret them as relative ratio instead of actual ratio \n",
    "# since we added the one to prevent zero division\n",
    "tokens.sort_values('spam_ratio', ascending = False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e.g. From the looks of it, the word claim appears a lot more often in spam messages than ham ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83.667259786476862"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look up the spam_ratio for a given token\n",
    "tokens.loc['dating', 'spam_ratio']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Tuning the vectorizer\n",
    "\n",
    "Thus far, we have been using the default parameters of [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show default parameters for CountVectorizer\n",
    "vect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the vectorizer is worth tuning, just like a model is worth tuning! Here are a few parameters that you might want to tune:\n",
    "\n",
    "- **stop_words:** string {'english'}, list, or None (default)\n",
    "    - If 'english', a built-in stop word list for English is used.\n",
    "    - If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens.\n",
    "    - If None, no stop words will be used.\n",
    "    \n",
    "Removing common and uncommon words is extremely useful in text analytics. The rationale behind it is that common words such as 'the', 'a', or 'and' appear so commonly in the English language that they tell us almost nothing about how similar or dissimilar two documents might be, or in a sense, they carry less semantic weight. On the other hand, there may be words that only appear one of twice in the entire corpus and we simply don't have enough data about these words to learn a meaningful output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remove English stop words, these are usually commonly appeared words that\n",
    "# probably don't provide a lot of usual messages. \n",
    "vect = CountVectorizer(stop_words = 'english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **ngram_range:** tuple (min_n, max_n), default=(1, 1)\n",
    "    - The lower and upper boundary of the range of n-values for different n-grams to be extracted.\n",
    "    - All values of n such that min_n <= n <= max_n will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# include 1-grams and 2-grams\n",
    "# it might be the case that word pairs has relationships with the output,\n",
    "# e.g. with 2-grams \"not happy\" won't be split into \"not\" and \"happy\" and it might be predictive\n",
    "# however, it still might introduce more noise than signals\n",
    "vect = CountVectorizer(ngram_range = (1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **max_df:** float in range [0.0, 1.0] or int, default=1.0\n",
    "    - When building the vocabulary, ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words).\n",
    "    - If float, the parameter represents a proportion of documents.\n",
    "    - If integer, the parameter represents an absolute count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ignore terms if that appear too common, here when it\n",
    "# appears in more than 50% of the documents\n",
    "vect = CountVectorizer(max_df = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **min_df:** float in range [0.0, 1.0] or int, default=1\n",
    "    - When building the vocabulary, ignore terms that have a document frequency strictly lower than the given threshold. (This value is also called \"cut-off\" in the literature.)\n",
    "    - If float, the parameter represents a proportion of documents.\n",
    "    - If integer, the parameter represents an absolute count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# only keep terms that appear in at least 2 documents\n",
    "vect = CountVectorizer(min_df = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Guidelines for tuning CountVectorizer:**\n",
    "\n",
    "- Use your knowledge of the **problem** and the **text**, and your understanding of the **tuning parameters**, to help you decide what parameters to tune and how to tune them.\n",
    "- **Experiment**, and let the data tell you the best approach!\n",
    "\n",
    "In the cell below, we're simply putting the basic workflow of text classificatio's script into one cell for future reference's convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.988513998564\n",
      "auc 0.993681761231\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# using the example spam dataset\n",
    "# read it in, extract the input and output columns\n",
    "sms = pd.read_table('sms.tsv', header = None, names = ['label', 'message'])\n",
    "sms['label_num'] = sms['label'].map({'ham': 0, 'spam': 1})\n",
    "X = sms['message']\n",
    "y = sms['label_num']\n",
    "\n",
    "# split X and y into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 1)\n",
    "\n",
    "# convert both sets' text column to document-term matrix\n",
    "vect = CountVectorizer()\n",
    "X_train_dtm = vect.fit_transform(X_train)\n",
    "X_test_dtm  = vect.transform(X_test)\n",
    "\n",
    "# train the mutinomial naive bayes model, \n",
    "# predict on the test set and output the accuracy score\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred_class)\n",
    "print('accuracy:', accuracy)\n",
    "\n",
    "# train the logistic regression model, \n",
    "# predict on the test set and output the auc score\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train_dtm, y_train)\n",
    "y_pred_prob = logreg.predict_proba(X_test_dtm)[:, 1]\n",
    "auc = metrics.roc_auc_score(y_test, y_pred_prob)\n",
    "print('auc', auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sparsity ratio:  0.998228130998\n"
     ]
    }
   ],
   "source": [
    "def sparsity_ratio(X):\n",
    "    \"\"\"\n",
    "    The rule of thumb is if the sparsity ratio is greater than 90% \n",
    "    then you can probably benefit from sparse formats, that are the \n",
    "    default representation of document-term matrix for scikit learn\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : scipy sparse matrix\n",
    "        document-term matrix\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    sparsity : float\n",
    "        the ratio of elements in the matrix that are zeros\n",
    "    \"\"\"\n",
    "    sparsity = 1 - X.nnz / np.prod(X.shape)\n",
    "    return sparsity\n",
    "\n",
    "print('input sparsity ratio: ', sparsity_ratio(X_train_dtm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# change default figure and font size\n",
    "plt.rcParams['figure.figsize'] = 8, 6 \n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "def visualize_coefficients(coefficients, feature_names, n_top_features = 10):\n",
    "    # obtain the top `n_top_features` coefficients of logsitic regresion\n",
    "    # here top refers to the largest positive and negative coefficients\n",
    "    \n",
    "    # convert the coefficient to a 1d-array and obtain the \n",
    "    # positive and negative coefs with larger absolute values\n",
    "    # and stack them together into one array\n",
    "    coefs = coefficients.ravel()\n",
    "    sorted_coefs = np.argsort(coefs)\n",
    "    positive_coefs = sorted_coefs[-n_top_features:]\n",
    "    negative_coefs = sorted_coefs[:n_top_features]\n",
    "    top_coefs = np.hstack([negative_coefs, positive_coefs])\n",
    "\n",
    "    fig = plt.figure(figsize = (15, 5))\n",
    "    colors = ['#A60628' if c < 0 else '#348ABD' for c in coefs[top_coefs]]\n",
    "    x_ticks = np.arange(2 * n_top_features)\n",
    "    plt.bar(x_ticks, coefs[top_coefs], color = colors)\n",
    "    feature_names = np.array(feature_names)\n",
    "    plt.xticks(0.5 + x_ticks, feature_names[top_coefs], rotation = 90)\n",
    "    plt.title('top {} positive/negative coefficient'.format(n_top_features))\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3oAAAFrCAYAAACHRvtvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nC/Mbf+j9FbzDmyAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112ba4cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "feature_names = vect.get_feature_names()\n",
    "\n",
    "coef_plot = visualize_coefficients( \n",
    "    coefficients = logreg.coef_,\n",
    "    feature_names = feature_names, \n",
    "    n_top_features = 10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "- [Youtube: PyCon 2016 Machine Learning with Text in scikit-learn](https://www.youtube.com/watch?v=znfy3T9OiAQ)\n",
    "- [Github: PyCon 2016 Machine Learning with Text in scikit-learn](https://github.com/justmarkham/pycon-2016-tutorial)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "322px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
